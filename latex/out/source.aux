\relax 
\abx@aux@refcontext{none/global//global/global}
\abx@aux@cite{0}{chen2018neural}
\abx@aux@segm{0}{0}{chen2018neural}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:intro}{{1}{2}{}{}{}}
\abx@aux@cite{0}{He_2016_CVPR}
\abx@aux@segm{0}{0}{He_2016_CVPR}
\abx@aux@cite{0}{simonyan2014very}
\abx@aux@segm{0}{0}{simonyan2014very}
\abx@aux@cite{0}{ioffe2015batch}
\abx@aux@segm{0}{0}{ioffe2015batch}
\abx@aux@cite{0}{srivastava2015highway}
\abx@aux@segm{0}{0}{srivastava2015highway}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Residual Networks}{3}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:resnet}{{2}{3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.1}Residual Learning}{3}{}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017attention}
\abx@aux@segm{0}{0}{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A visual explenation of residual learninig. Information flows through the shortcut connections as if they were identity mappings. The network learns the residual instead of a direct mapping. Residual neural networks use this property to achieve very large depth. }}{4}{}\protected@file@percent }
\newlabel{fig:res_expl}{{2.1}{4}{}{}{}}
\abx@aux@cite{0}{rico1992discrete}
\abx@aux@segm{0}{0}{rico1992discrete}
\abx@aux@cite{0}{weinan2018mean}
\abx@aux@segm{0}{0}{weinan2018mean}
\abx@aux@cite{0}{chen2018neural}
\abx@aux@segm{0}{0}{chen2018neural}
\abx@aux@cite{0}{lu2018beyond}
\abx@aux@segm{0}{0}{lu2018beyond}
\abx@aux@cite{0}{weinan2018mean}
\abx@aux@segm{0}{0}{weinan2018mean}
\abx@aux@cite{0}{ruthotto2020deep}
\abx@aux@segm{0}{0}{ruthotto2020deep}
\abx@aux@cite{0}{kidger2022neural}
\abx@aux@segm{0}{0}{kidger2022neural}
\abx@aux@cite{0}{massaroli2020dissecting}
\abx@aux@segm{0}{0}{massaroli2020dissecting}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Neural ODEs}{6}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.1}A continuous time network}{6}{}\protected@file@percent }
\newlabel{resnet}{{3.1}{6}{}{}{}}
\newlabel{diffeq}{{3.2}{6}{}{}{}}
\abx@aux@cite{0}{chen2019ordinary}
\abx@aux@segm{0}{0}{chen2019ordinary}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.2}Inference}{7}{}\protected@file@percent }
\newlabel{ivp}{{3.3}{7}{}{}{}}
\newlabel{forwaredEu}{{3.4}{7}{}{}{}}
\newlabel{res2eul}{{3.5}{7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces ResNets can be considered as discretizations of an ODE solution. Under this assumption Resnet defines a continuous time vector field.}}{8}{}\protected@file@percent }
\newlabel{fig:resnet_to_node}{{3.1}{8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.3}Training}{8}{}\protected@file@percent }
\abx@aux@cite{0}{matsubara2021symplectic}
\abx@aux@segm{0}{0}{matsubara2021symplectic}
\abx@aux@cite{0}{pontryagin2018mathematical}
\abx@aux@segm{0}{0}{pontryagin2018mathematical}
\abx@aux@cite{0}{chiu2023exploiting}
\abx@aux@segm{0}{0}{chiu2023exploiting}
\abx@aux@cite{0}{chen2018neural}
\abx@aux@segm{0}{0}{chen2018neural}
\abx@aux@cite{0}{kidger2022neural}
\abx@aux@segm{0}{0}{kidger2022neural}
\newlabel{eq:ode_loss}{{3.6}{9}{}{}{}}
\newlabel{adjoint}{{3.8}{9}{}{}{}}
\newlabel{dldtheta}{{3.9}{9}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.4}Applications, benefits and limitations}{9}{}\protected@file@percent }
\abx@aux@cite{0}{poli2020hypersolvers}
\abx@aux@segm{0}{0}{poli2020hypersolvers}
\abx@aux@cite{0}{kidger2021efficient}
\abx@aux@segm{0}{0}{kidger2021efficient}
\abx@aux@cite{0}{zhuang2021mali}
\abx@aux@segm{0}{0}{zhuang2021mali}
\abx@aux@cite{0}{gander201550}
\abx@aux@segm{0}{0}{gander201550}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Numerical Solvers}{11}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq:sol:ivp}{{4.1}{11}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Introduction}{11}{}\protected@file@percent }
\newlabel{eq:sol:lipsitz}{{4.2}{11}{}{}{}}
\abx@aux@cite{0}{suli2010numerical}
\abx@aux@segm{0}{0}{suli2010numerical}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.2}Forward Euler}{12}{}\protected@file@percent }
\abx@aux@cite{0}{butcher2016numerical}
\abx@aux@segm{0}{0}{butcher2016numerical}
\newlabel{eq:sol:euler}{{4.0.2}{13}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Implicit Euler}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.3}General explicit one-step method}{14}{}\protected@file@percent }
\newlabel{eq:sol:trunc_err}{{4.5}{14}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.4}Runge-Kutta methods}{15}{}\protected@file@percent }
\newlabel{eq:sol:rungekutta}{{4.6}{15}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Explicit Runge-Kutta methods}{15}{}\protected@file@percent }
\newlabel{eq:tayl1}{{4.7}{16}{}{}{}}
\newlabel{eq:tayl2}{{4.8}{16}{}{}{}}
\abx@aux@cite{0}{vaswani2017attention}
\abx@aux@segm{0}{0}{vaswani2017attention}
\abx@aux@cite{0}{nievergelt1964parallel}
\abx@aux@segm{0}{0}{nievergelt1964parallel}
\abx@aux@cite{0}{maday2002parareal}
\abx@aux@segm{0}{0}{maday2002parareal}
\abx@aux@cite{0}{emmett2012toward}
\abx@aux@segm{0}{0}{emmett2012toward}
\abx@aux@cite{0}{massaroli2021differentiable}
\abx@aux@segm{0}{0}{massaroli2021differentiable}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Parallelism in Time}{18}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Polynomial approximation}{18}{}\protected@file@percent }
\newlabel{dydt}{{5.1}{18}{}{}{}}
\newlabel{dPhidt}{{5.2}{18}{}{}{}}
\newlabel{dphi_fphi}{{5.3}{18}{}{}{}}
\abx@aux@cite{0}{trefethen1996finite}
\abx@aux@segm{0}{0}{trefethen1996finite}
\newlabel{eq:error_wrt_B}{{5.4}{19}{}{}{}}
\newlabel{eq:cheb_rec}{{5.7}{19}{}{}{}}
\newlabel{Dcheb}{{5.8}{19}{}{}{}}
\newlabel{approx_error}{{5.9}{20}{}{}{}}
\newlabel{approx_error_sum}{{5.10}{20}{}{}{}}
\newlabel{eq:error_zero}{{5.11}{20}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{A zero order optimisation algorithm}{20}{}\protected@file@percent }
\newlabel{eq:B_inside_f}{{5.13}{20}{}{}{}}
\newlabel{eq:grad_err}{{5.15}{20}{}{}{}}
\newlabel{eq:cheb_mat}{{5.0.1}{21}{}{}{}}
\newlabel{DPHI}{{5.0.1}{21}{}{}{}}
\newlabel{update}{{5.17}{21}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces  Regression of the approximate solutions to the true solution. As the coefficients go to the optimum and the error function approaches the minimum the trajectories are getting closer to the true solution.}}{22}{}\protected@file@percent }
\newlabel{fig:reg_sol}{{5.1}{22}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Higher order minimization}{22}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Polynomial approximation numerical integration}}{23}{}\protected@file@percent }
\newlabel{alg:intapprox}{{14}{23}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}Realisation details}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Imposing initial conditions}{23}{}\protected@file@percent }
\abx@aux@cite{0}{gill2019practical}
\abx@aux@segm{0}{0}{gill2019practical}
\newlabel{eq:pan:B01}{{5.22}{24}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Hessian}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}Comparison with classical and other parellel-in-time methods}{24}{}\protected@file@percent }
\abx@aux@cite{0}{emmett2012toward}
\abx@aux@segm{0}{0}{emmett2012toward}
\abx@aux@cite{0}{maday2002parareal}
\abx@aux@segm{0}{0}{maday2002parareal}
\abx@aux@cite{0}{wright2006numerical}
\abx@aux@segm{0}{0}{wright2006numerical}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.4}Shortcomings and improvements}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.5}Experiments}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Spiral}{26}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Metrics for the solution of an IVP with different methods. For PAN params indicates the number of approximation coefficients per dimention, in this case there are 2 dimensions. NFE for PAN is show as NFE for least squares plus (+) NFE for Newton.}}{27}{}\protected@file@percent }
\newlabel{spiral-metrics}{{5.1}{27}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Lorenz Attractor}{28}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Accuracy and NFE for Lorenz system. For PAN the interval has been split in subintervals of length $0.3$ and the algorithm has been applied sequencially on each interval.}}{29}{}\protected@file@percent }
\newlabel{lorenz_metric}{{5.2}{29}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {.1}The backpropagation equations}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.1}Vanilla Neural Networks}{32}{}\protected@file@percent }
\newlabel{dCdy}{{29}{33}{}{}{}}
\newlabel{dCdW}{{30}{33}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Backpropagation}}{34}{}\protected@file@percent }
\newlabel{alg:backprop}{{2}{34}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.2}Residual Networks}{34}{}\protected@file@percent }
\newlabel{resnet_dCdy}{{32}{34}{}{}{}}
\newlabel{resnet_dCdW}{{34}{34}{}{}{}}
\abx@aux@cite{0}{chen2018neural}
\abx@aux@segm{0}{0}{chen2018neural}
\abx@aux@cite{0}{kidger2022neural}
\abx@aux@segm{0}{0}{kidger2022neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {.1.3}Neural ODEs}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {.2}An alternative derivation for $\frac  {dL}{d \pmb  {\theta }}$}{35}{}\protected@file@percent }
\newlabel{adjoint_proof}{{.2}{35}{}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Adjoint}}{36}{}\protected@file@percent }
\newlabel{alg:adjoint}{{3}{36}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {.3}Vector-Jacobian products}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {.4}Optimisation}{37}{}\protected@file@percent }
\newlabel{min_a}{{39}{37}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.4.1}The Newton-Raphson algorithm with Hessian modification}{38}{}\protected@file@percent }
\newlabel{newtonalgo}{{.4.1}{38}{}{}{}}
\newlabel{taylor}{{41}{38}{}{}{}}
\newlabel{grad_m}{{42}{38}{}{}{}}
\newlabel{newton_dir}{{43}{38}{}{}{}}
\newlabel{pos_def}{{46}{38}{}{}{}}
\abx@aux@cite{0}{cheng1998modified}
\abx@aux@segm{0}{0}{cheng1998modified}
\abx@aux@cite{0}{wright2006numerical}
\abx@aux@segm{0}{0}{wright2006numerical}
\abx@aux@cite{0}{gill2019practical}
\abx@aux@segm{0}{0}{gill2019practical}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Modified Cholesky}}{40}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Line search}}{41}{}\protected@file@percent }
\newlabel{alg:linesearh}{{5}{41}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {.4.2}Gradient Descent Methods}{41}{}\protected@file@percent }
\abx@aux@cite{0}{nakkiran2019deep}
\abx@aux@segm{0}{0}{nakkiran2019deep}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces zoom}}{42}{}\protected@file@percent }
\newlabel{alg:zoom}{{6}{42}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Stochastic Gradient Descent}{42}{}\protected@file@percent }
\abx@aux@cite{0}{kiwiel2001convergence}
\abx@aux@segm{0}{0}{kiwiel2001convergence}
\abx@aux@cite{0}{rumelhart1986learning}
\abx@aux@segm{0}{0}{rumelhart1986learning}
\@writefile{toc}{\contentsline {subsubsection}{ Momentum }{43}{}\protected@file@percent }
\abx@aux@cite{0}{kashyap2022survey}
\abx@aux@segm{0}{0}{kashyap2022survey}
\abx@aux@cite{0}{duchi2011adaptive}
\abx@aux@segm{0}{0}{duchi2011adaptive}
\@writefile{toc}{\contentsline {subsubsection}{Nesterov Momentum}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adagrad}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{RMSProp}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Adam}{45}{}\protected@file@percent }
\abx@aux@read@bbl@mdfivesum{DD19196A1684AAA1771EFB5D8D1BF559}
\abx@aux@read@bblrerun
\abx@aux@defaultrefcontext{0}{chen2018neural}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{He_2016_CVPR}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{simonyan2014very}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ioffe2015batch}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{srivastava2015highway}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{vaswani2017attention}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rico1992discrete}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{weinan2018mean}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{lu2018beyond}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ruthotto2020deep}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kidger2022neural}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{massaroli2020dissecting}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{chen2019ordinary}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{pontryagin2018mathematical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{chiu2023exploiting}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{poli2020hypersolvers}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kidger2021efficient}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{zhuang2021mali}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{suli2010numerical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{butcher2016numerical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nievergelt1964parallel}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{maday2002parareal}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{emmett2012toward}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{massaroli2021differentiable}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{trefethen1996finite}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{gill2019practical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{wright2006numerical}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{cheng1998modified}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{nakkiran2019deep}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kiwiel2001convergence}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rumelhart1986learning}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kashyap2022survey}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{duchi2011adaptive}{none/global//global/global}
\gdef \@abspage@last{46}
