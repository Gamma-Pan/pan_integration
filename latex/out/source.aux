\relax 
\abx@aux@refcontext{none/global//global/global}
\abx@aux@cite{0}{chen2018neural}
\abx@aux@segm{0}{0}{chen2018neural}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{2}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:intro}{{1}{2}{}{}{}}
\abx@aux@cite{0}{He_2016_CVPR}
\abx@aux@segm{0}{0}{He_2016_CVPR}
\abx@aux@cite{0}{simonyan2014very}
\abx@aux@segm{0}{0}{simonyan2014very}
\abx@aux@cite{0}{ioffe2015batch}
\abx@aux@segm{0}{0}{ioffe2015batch}
\abx@aux@cite{0}{srivastava2015highway}
\abx@aux@segm{0}{0}{srivastava2015highway}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Residual Networks}{3}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:resnet}{{2}{3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.1}Residual Learning}{3}{}\protected@file@percent }
\abx@aux@cite{0}{vaswani2017attention}
\abx@aux@segm{0}{0}{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A visual explenation of residual learninig. Information flows through the shortcut connections as if they were identity mappings. The network learns the residual instead of a direct mapping. Residual neural networks use this property to achieve very large depth. }}{4}{}\protected@file@percent }
\newlabel{fig:res_expl}{{2.1}{4}{}{}{}}
\abx@aux@cite{0}{rico1992discrete}
\abx@aux@segm{0}{0}{rico1992discrete}
\abx@aux@cite{0}{weinan2018mean}
\abx@aux@segm{0}{0}{weinan2018mean}
\abx@aux@cite{0}{chen2018neural}
\abx@aux@segm{0}{0}{chen2018neural}
\abx@aux@cite{0}{lu2018beyond}
\abx@aux@segm{0}{0}{lu2018beyond}
\abx@aux@cite{0}{weinan2018mean}
\abx@aux@segm{0}{0}{weinan2018mean}
\abx@aux@cite{0}{ruthotto2020deep}
\abx@aux@segm{0}{0}{ruthotto2020deep}
\abx@aux@cite{0}{kidger2022neural}
\abx@aux@segm{0}{0}{kidger2022neural}
\abx@aux@cite{0}{massaroli2020dissecting}
\abx@aux@segm{0}{0}{massaroli2020dissecting}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Neural ODEs}{6}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.1}A continuous time network}{6}{}\protected@file@percent }
\newlabel{resnet}{{3.1}{6}{}{}{}}
\newlabel{diffeq}{{3.2}{6}{}{}{}}
\abx@aux@cite{0}{chen2019ordinary}
\abx@aux@segm{0}{0}{chen2019ordinary}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.2}Inference}{7}{}\protected@file@percent }
\newlabel{ivp}{{3.3}{7}{}{}{}}
\newlabel{forwaredEu}{{3.4}{7}{}{}{}}
\newlabel{res2eul}{{3.5}{7}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces ResNets can be considered as discretizations of an ODE solution. Under this assumption Resnet defines a continuous time vector field.}}{8}{}\protected@file@percent }
\newlabel{fig:resnet_to_node}{{3.1}{8}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.3}Training}{8}{}\protected@file@percent }
\abx@aux@cite{0}{matsubara2021symplectic}
\abx@aux@segm{0}{0}{matsubara2021symplectic}
\abx@aux@cite{0}{pontryagin2018mathematical}
\abx@aux@segm{0}{0}{pontryagin2018mathematical}
\abx@aux@cite{0}{chiu2023exploiting}
\abx@aux@segm{0}{0}{chiu2023exploiting}
\abx@aux@cite{0}{chen2018neural}
\abx@aux@segm{0}{0}{chen2018neural}
\abx@aux@cite{0}{kidger2022neural}
\abx@aux@segm{0}{0}{kidger2022neural}
\newlabel{eq:ode_loss}{{3.6}{9}{}{}{}}
\newlabel{adjoint}{{3.8}{9}{}{}{}}
\newlabel{dldtheta}{{3.9}{9}{}{}{}}
\abx@aux@cite{0}{poli2020hypersolvers}
\abx@aux@segm{0}{0}{poli2020hypersolvers}
\abx@aux@cite{0}{kidger2021efficient}
\abx@aux@segm{0}{0}{kidger2021efficient}
\abx@aux@cite{0}{zhuang2021mali}
\abx@aux@segm{0}{0}{zhuang2021mali}
\abx@aux@cite{0}{gander201550}
\abx@aux@segm{0}{0}{gander201550}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.4}Applications, benefits and limitations}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Numerical Solvers}{11}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq:sol:ivp}{{4.1}{11}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Introduction}{11}{}\protected@file@percent }
\newlabel{eq:sol:lipsitz}{{4.2}{11}{}{}{}}
\abx@aux@cite{0}{suli2010numerical}
\abx@aux@segm{0}{0}{suli2010numerical}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.2}Forward Euler}{12}{}\protected@file@percent }
\abx@aux@cite{0}{butcher2016numerical}
\abx@aux@segm{0}{0}{butcher2016numerical}
\newlabel{eq:euler_itr}{{4.5}{13}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Implicit Euler}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.3}General explicit one-step method}{14}{}\protected@file@percent }
\newlabel{eq:sol:trunc_err}{{4.6}{14}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.4}Runge-Kutta methods}{15}{}\protected@file@percent }
\newlabel{eq:sol:rungekutta}{{4.7}{15}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Explicit Runge-Kutta methods}{15}{}\protected@file@percent }
\newlabel{eq:tayl1}{{4.8}{16}{}{}{}}
\newlabel{eq:tayl2}{{4.9}{16}{}{}{}}
\abx@aux@cite{0}{vaswani2017attention}
\abx@aux@segm{0}{0}{vaswani2017attention}
\abx@aux@cite{0}{nievergelt1964parallel}
\abx@aux@segm{0}{0}{nievergelt1964parallel}
\abx@aux@cite{0}{maday2002parareal}
\abx@aux@segm{0}{0}{maday2002parareal}
\abx@aux@cite{0}{emmett2012toward}
\abx@aux@segm{0}{0}{emmett2012toward}
\abx@aux@cite{0}{massaroli2021differentiable}
\abx@aux@segm{0}{0}{massaroli2021differentiable}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Parallelism in Time}{18}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Polynomial approximation}{18}{}\protected@file@percent }
\newlabel{dydt}{{5.1}{18}{}{}{}}
\newlabel{dPhidt}{{5.2}{18}{}{}{}}
\newlabel{dphi_fphi}{{5.3}{18}{}{}{}}
\abx@aux@cite{0}{trefethen1996finite}
\abx@aux@segm{0}{0}{trefethen1996finite}
\newlabel{eq:error_wrt_B}{{5.4}{19}{}{}{}}
\newlabel{eq:cheb_rec}{{5.7}{19}{}{}{}}
\newlabel{Dcheb}{{5.8}{19}{}{}{}}
\newlabel{approx_error}{{5.9}{20}{}{}{}}
\newlabel{approx_error_sum}{{5.10}{20}{}{}{}}
\newlabel{eq:error_zero}{{5.11}{20}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{A zero order optimisation algorithm}{20}{}\protected@file@percent }
\newlabel{eq:B_inside_f}{{5.13}{20}{}{}{}}
\newlabel{eq:grad_err}{{5.15}{20}{}{}{}}
\newlabel{eq:cheb_mat}{{5.0.1}{21}{}{}{}}
\newlabel{DPHI}{{5.0.1}{21}{}{}{}}
\newlabel{update}{{5.17}{21}{}{}{}}
\newlabel{eq:biter}{{5.18}{21}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces  Regression of the approximate solutions to the true solution. As the coefficients go to the optimum and the error function approaches the minimum the trajectories are getting closer to the true solution.}}{22}{}\protected@file@percent }
\newlabel{fig:reg_sol}{{5.1}{22}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{Higher order minimization}{22}{}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Polynomial approximation numerical integration}}{23}{}\protected@file@percent }
\newlabel{alg:intapprox_2}{{1}{23}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}Realisation details}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Imposing initial conditions}{23}{}\protected@file@percent }
\abx@aux@cite{0}{gill2019practical}
\abx@aux@segm{0}{0}{gill2019practical}
\newlabel{eq:pan:B01}{{5.22}{24}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{The Hessian}{24}{}\protected@file@percent }
\abx@aux@cite{0}{suli2010numerical}
\abx@aux@segm{0}{0}{suli2010numerical}
\abx@aux@cite{0}{emmett2012toward}
\abx@aux@segm{0}{0}{emmett2012toward}
\abx@aux@cite{0}{maday2002parareal}
\abx@aux@segm{0}{0}{maday2002parareal}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}Comparison with classical and other parellel-in-time meth