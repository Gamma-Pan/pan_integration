\documentclass[11pt]{report}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{cancel}
\usepackage{algorithm}
\usepackage[noEnd=false,indLines=false]{algpseudocodex}
\usepackage[left=3.5cm, right=3.5cm]{geometry}
\usepackage{tikz}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}[section]

\usetikzlibrary{shapes,positioning,backgrounds, calc}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{external}
\tikzexternalize
\usepackage[style=ieee]{biblatex} %Imports biblatex package
\addbibresource{references.bib} %Import the bibliography file

\title{Polynomial approximation numerical integration\\ \large{A parallel approach to Initial Value Problems through error minimisation} \\ v0.99}

\author{Yiorgos Panagiotopoulos}
\date{June 2024}

\begin{document}

    \maketitle

    \abstract
    We present a short introduction to neural ordinary differential equations and how they emerge as continuous time
    generalizations of residual neural networks.
    One of the main bottlenecks of neural ODEs has been their speed both on training and inference.
    The numerical solver used for the forward and backward pass of the network can greatly impact its performance both
    in terms of speed and accuracy.
    We develop a new numerical ODE solver based on refining a polynomial approximation of the solution and explore its
    impact on this framework.
    At the end we showcase how our method compares to some commonly used numerical solvers.


    \chapter{Introduction} \label{ch:intro}
    The machine learning scene in 2024 is dominated by gargantuan models with billions of parameters.
    Even though they have demonstrated impressive results the compute and energy requirements of such models are a
    concern.
    It's the author's opinion that we ought to transition to analog or hybrid models of computation to reduce energy
    usage and increase speed.
    Even though digital computers are not going away any time soon studying alternative learning frameworks could ease
    the transition.

    Neural differential equations re-emerged in the recent years after~\cite{chen2018neural}.
    It's considered that they have great potential especially in:
    \begin{itemize}
        \item modelling physical phenomena governed by differential equations
        \item irregularly sampled time series
        \item generative models including continuous normalising flows and stochastic differential equations
    \end{itemize}
    More importantly, in out opinion, they highlight a connection between deep learning and dynamical systems.
    Even thought we still operate in the discrete domain to use them in our current systems they offer an opportunity
    to close the gap between continuous time models and machine learning.

    We provide a quick overview of Neural ODES and propose a novel numerical integration method to tackle some of
    them limitations.


    \chapter{Residual Networks} \label{ch:resnet}
    In order to understand the intuition behind neural ordinary differential equations one should have a basic knowledge
    of residual networks.
    Residual networks, also known as ResNets, were introduced by He and others in their seminal
    paper~\cite{He_2016_CVPR}, to address the problem of \textit{degradation} in very deep architectures.
    It had become apparent by state of the art models of that time like~\cite{simonyan2014very} that depth plays a very
    important role in vision tasks, including classification.
    Stacking layers (depth) allows for the integration of low/mid/high level features in the learning process.
    The immediate obstacle in deep learning is the notorious vanishing gradient problem, but this has been largely
    solved through batch/group normalisation~\cite{ioffe2015batch}.

    It had been noticed though~\cite{srivastava2015highway}, that while stacking more layers, accuracy is initially saturated, unsurprisingly, but then it degrades rapidly.
    This degradation phenomenon is not caused by overfitting as the training error increases with the number of layers.
    ResNets solved this problem by utilising a residual learning framework.

    \subsection{Residual Learning}
    One can prove, by construction, that a deep architecture can be as accurate as a corresponding shallower one.
    This is achieved by adding identity layers in-between the layers of the shallower model.
    In more detail, consider a network that converges with some accuracy.
    Adding more layers to it would achieve the exact same accuracy if the extra layers learned to map their inputs straight to their outputs effectively making them identity mappings.
    The existence of this artificially deep network suggests that deep networks shouldn't produce higher error that the shallower ones.
    But the empirical evidence show that optimisers can't find this solution, at least in sensible time limits.

    \begin{figure}[H]
        \begin{center}
            \begin{tikzpicture}[
                every path/.style={ultra thick},
                scale=3,
                block/.style={
                    rectangle,
                    rounded corners,
                    thick,
                    minimum size = 1.5cm,
                    inner sep = 0pt,
                    node distance = 3.5cm,
                    draw=black!80
                },
                shallow/.style={
                    block,
                },
                deep/.style={
                    block,
                    dashed,
                    thick
                },
                darkgreen/.style={
                    green!80!black!90
                },
                >= stealth,
            ]

                \begin{scope}[local bounding box = net1]
                    \node[shallow] (A) at (0,0) {};
                    \node[below right=0cm of A.north west] {$f_{k-1}(x)$};

                    \node[shallow] (B) [right= of A] {};
                    \node[below right=0cm of B.north west] {$f_{k}(x)$};

                    \node[shallow] (C) [right= of B] {};
                    \node[below right=0cm of C.north west] {$f_{k+1}(x)$};

                    \draw[->] (A.west)+(-0.1cm,0) -- (A.west);
                    \draw[->] (A.east) -- (B.west);
                    \draw[->] (B.east) -- (C.west);
                    \draw[dashed] (C.east) -- +(0.1cm,0);

                    \node[above] at (current bounding box.north) {A section of the starting neural net.};

                \end{scope}

                \begin{scope}[local bounding box = scope1, yshift = -0.8cm]
                    \node[shallow] (A) at (0,0) {};
                    \node[below right=0cm of A.north west] {$f_{k-1}(x)$};

                    \node[shallow] (B) [right= of A] {};
                    \node[below right=0cm of B.north west] {$f_{k}(x)$};

                    \node[shallow] (C) [right= of B] {};
                    \node[below right=0cm of C.north west] {$f_{k+1}(x)$};

                    \node[deep] (AB) at ($(A)!0.5!(B)$) {};
                    \node[below right=0cm of AB.north west] {$\bar{f}_{k-1}(x)$};

                    \node[deep] (BC) at ($(B)!0.5!(C)$) {};
                    \node[below right=0cm of BC.north west] {$\bar{f}_{k}(x)$};

                    \draw[->] (A.west)+(-0.1cm,0) -- (A.west);
                    \draw[->] (A.east) -- (AB.west);
                    \draw[->] (AB.east) -- (B.west);
                    \draw[->] (B.east) -- (BC.west);
                    \draw[->] (BC.east) -- (C.west);
                    \draw[dashed] (C.east) -- +(0.1cm,0);

                    \node[above] at (scope1.north) {Increase its depth by adding more layers.};
                \end{scope}

                \begin{scope}[local bounding box = scope2,yshift=-1.8cm]
                    \node[shallow] (A) at (0,0) {};
                    \node[below right=0cm of A.north west] {$f_{k-1}(x)$};

                    \node[shallow] (B) [right= of A] {};
                    \node[below right=0cm of B.north west] {$f_{k}(x)$};

                    \node[shallow] (C) [right= of B] {};
                    \node[below right=0cm of C.north west] {$f_{k+1}(x)$};

                    \node[deep,darkgreen] (AB) at ($(A)!0.5!(B)$) {};
                    \node[below right=0cm of AB.north west] {$\bar f_{k-1}(x)$};
                    \node[above left=0cm of AB.south east, darkgreen] {$\mathbf I$};

                    \node[deep,darkgreen] (BC) at ($(B)!0.5!(C)$) {};
                    \node[below right=0cm of BC.north west] {$\bar f_{k}(x)$};
                    \node[above left=0cm of BC.south east, darkgreen] {$ \mathbf I$};

                    \draw[->] (A.west)+(-0.1cm,0) -- (A.west);
                    \draw (A.east) -- (AB.west);
                    \draw[darkgreen] (AB.west) -- (AB.east);
                    \draw (AB.east) -- (B.west);
                    \draw[->] (AB.east) -- (B.west);
                    \draw (B.east) -- (BC.west);
                    \draw[darkgreen] (BC.west) -- (BC.east);
                    \draw[->] (BC.east) -- (C.west);
                    \draw[dashed] (C.east) -- +(0.1cm,0);

                    \node[above,align=left] at (scope2.north) {If the extra layers learned to be identity mappings the\\
                    deeper network would function identically to the original.};
                \end{scope}

                \begin{scope}[local bounding box = scope3,yshift=-3.1cm]
                    ;
                    \node[shallow] (A) at (0,0) {};
                    \node[below right=0cm of A.north west] {$f_{k-1}(x)$};

                    \node[shallow] (B) [right= of A] {};
                    \node[below right=0cm of B.north west] {$f_{k}(x)$};

                    \node[shallow] (C) [right= of B] {};
                    \node[below right=0cm of C.north west] {$f_{k+1}(x)$};

                    \node[deep, yshift=1cm] (AB) at ($(A)!0.5!(B)$) {};
                    \node[below right=0cm of AB.north west] {$\bar f_{k-1}(x)$};
                    \node[above left=0cm of AB.south east] {};

                    \node[deep, yshift=1cm] (BC) at ($(B)!0.5!(C)$) {};
                    \node[below right=0cm of BC.north west] {$\bar f_{k}(x)$};
                    \node[above left=0cm of BC.south east] {};

                    \draw[->] (A.west)+(-0.1cm,0) -- (A.west);
                    \draw[->, rounded corners] (A.east) +(0.1cm,0) -- ($(AB.west) - (0.1cm,0)$)  -- (AB.west);
                    \draw[->, rounded corners] (AB.east) -- ++(0.1cm,0) -- ($(B.west) + (-0.15cm,0)$);
                    \draw[->, rounded corners] (B.east) +(0.1cm,0) -- ($(BC.west) - (0.1cm,0)$)  -- (BC.west);
                    \draw[->, rounded corners] (BC.east) -- ++(0.1cm,0) -- ($(C.west) + (-0.15cm,0)$);
                    \draw[dashed] (C.east) -- +(0.1cm,0);

                    \draw[->,darkgreen, rounded corners] (B.west) +(-0.3cm,0) -- ++(0.05cm, -0.37cm) -- ++(0.42cm, 0cm) -- ($(B.east) + (0.25cm,0)$);
                    \draw[->,darkgreen] (A.east) -- (B.west);
                    \draw[->,darkgreen] (B.east) -- (C.west);

                    \node[above,align=left] at (scope3.north) {Shortcut connections precondition the deep network \\ to identity mappings and train the residual };
                \end{scope}
            \end{tikzpicture}
        \end{center}
        \caption{A visual explenation of residual learninig. Information flows through the shortcut connections as
        if they were identity mappings. The network learns the residual instead of a direct mapping.
        Residual neural networks use this property to achieve very large depth. }
        \label{fig:res_expl}
    \end{figure}

    In order to incorporate this observation into the network, so called \textit{shortcut connections} are introduced to the layers.
    Basically, the input to a block of stacked layers is added back to it's output.
    This way the network instead of learning the direct mapping $\mathcal{H}(x) = \mathcal{F}(x)$, it learns the residual mapping $\mathcal{H}(x) = \mathcal{R}(x)+x$.
    The optimiser can then push the residual mappings to $0$ leading to identity connections.
    Even though identity mappings are unlikely to be optimum, experiments showed that the residual connections have generally small responses, meaning identities are good pre-conditioners for deep architectures.

    Since their conception Residual Networks have revolutionised deep learning allowing for much deeper architectures than previously used.
    Furthermore, the idea of residual learning has found vast appeal in many other architectures.
    Transformers for example, employ residual connections, allowing many such layers to be stacked creating large expressive models~\cite{vaswani2017attention}.


    \chapter{Neural ODEs}
    There exist examples in the literature of combining neural networks and dynamical systems, even since the 1990s~\cite{rico1992discrete}.
    Interest began to increase after the publication of ResNet~\cite{weinan2018mean}.
    But the field of Neural Differential Equations really became prominent with~\cite{chen2018neural}.

    \subsection{A continuous time network}
    Let's revisit Residual Networks and how they work.
    The input-output relationship of the $k$-th discrete ResNet block is given by:
    \begin{equation}
        \pmb{y}_{k+1} = \pmb{y}_{k} + f(\pmb{y}_{k}; \pmb{\theta}_k) \label{resnet}
    \end{equation}
    where $\pmb{y}_{k+1} \in \mathbb{R}^{N_y}$ is the output of the block, $\pmb{y}^{k} \in \mathbb{R}^{N_y}$ is its input and $f$ is a fully connected layer with parameters $\pmb{\theta}_k \in \mathbb{R}^{N_\theta}$.
    Notice that we refer to ResNet blocks, not layers, since residual connections are generally not between the input and output of a single linear layer but a stack of them; from now on we will use the terms interchangeably.

    The update in the \textit{hidden state} $\pmb{y}$ in~\eqref{resnet} resembles the formula of the forward Euler iteration for solving ordinary differential equations.
    This observation led some researchers~\cite{lu2018beyond,weinan2018mean,ruthotto2020deep} to make a connection between neural network architectures and differential equations.
    Suppose that we could keep increasing the depth of the ResNet in~\eqref{resnet} while the time of the forward pass remained bounded, meaning each layer would take less and less time.
    In the limit we get a differential equation governed by the dynamics defined by a neural network $f$:
    \begin{equation}
        \pmb{y}^{(1)}(t) = f(\pmb{y}(t); \pmb{\theta}) \label{diffeq}
    \end{equation}
    Instead of a discrete sequence of hidden states, $\pmb{y}(t)$ is a continuous flow $\pmb{y} : [0,T] \to \mathbb{R}^{N_y}$ defined be the vector field parameterised by $\pmb{\theta}$.

    One important issue we haven't addressed is that the network's weights, $\pmb\theta_k$, in~\eqref{resnet} are
    different for each $k$, but at~\eqref{diffeq} we consider them to be constant in time.
    For notationâ€™s sake we could define $f$ as $f(\pmb{y}(t), t, \pmb{\theta})$ meaning at certain ``depths'' different sections of the weights vector $\pmb{\theta}$ are used, maybe in a piecewise constant fashion~\cite{kidger2022neural}.
    Alternatively, the weights could be time dependant which complicates the model but leads to \textit{Galerkin} Neural ODEs, a more suitable continuous time equivalent of ResNet~\cite{massaroli2020dissecting}.
    For our purposes we consider the weights constant for the rest of this paper.

    \subsection{Inference}
    Under this new machine learning framework depth is replaced by time.
    Inference is performed by solving an initial value problem from initial time $t=0$ to terminal time $t=T$, i.e.:
    \begin{equation}
        \pmb{y}(T) = \pmb{y}(0) + \int_{0}^{T} f(\pmb{y}(\tau); \pmb{\theta}) \, d\tau \label{ivp}
    \end{equation}

    Obviously, Eq.~\eqref{ivp} describes the solution of a continuous time differential equation, while, as we know, our
    computers work in discrete time \textit{(for now)}.
    In order to overcome this obstacle we employ a numerical method for solving \textit{Initial Value Problems}, .
    If we were to use forward Euler, an explicit method, we would get a recurrence relation of the form
    \begin{equation}
        \pmb{y}(t_k + h) = \pmb{y}(t_k) + h \; \pmb{y}^{(1)}(t_k)
        \label{forwaredEu}
    \end{equation}
    By considering that $\pmb{y}^{(1)}(t) = f(\pmb{y}(t);\pmb{\theta})$ Eq. \eqref{forwaredEu} can be written as:
    \begin{equation}
        \pmb{y}(t_k + h) = \pmb{y}(t_k) + h f(\pmb{y}(t_k); \pmb{\theta} )
        \label{res2eul}
    \end{equation}
    which coincides with the ResNet formula.
    Many other neural networks of deep architectures can be interpreted as solving a neural ODE using different methods~\cite{chen2019ordinary}.
    The field of numerical differential equation solvers is quite vast meaning one could choose one that fits his specifications.
    A notable case is that of adaptive solvers like Runge-Kutta-Fehlberg, that use varying step sizes.
    This way to obtain  $\pmb{y}(T)$ the solver may require different number of $f$ evaluations depending on the input and the complexity of the vector field at that input.
    For simple dynamics the solver can ``decide'' to use large step sizes meaning fewer function evaluations.
    Otherwise when the vector field is more complex, a smaller step size will be used to reduce error thus leading to more function evaluations.
    We could interpret this behaviour as a network of ``variable depth''.

    It's important to clarify here that there are effectively two networks at play.
    Firstly, there is $f(\cdot)$, inherited from a block or layer of the original formulation of the problem.
    It is a neural network in the classical sense comprised of matrix vector multiplications and non-linearities.
    Secondly, there is the outlining model, the neural differential equation, which receives input $\pmb{y}_0$ and produced
    output $\pmb{y}(T)$; $f(\cdot)$ is to neural ODE what a layer is to ResNet.
    Internally, depending on the solver, $f(\cdot)$ will be evaluated for many values of $\pmb{y}(t)$.
    As disguised above, in contrast to ResNet $f(\cdot)$ contains all the learnable parameters of the model,
    which in out case are independent of depth.
    In conclusion the Neural ODE consists of: a neural network $f(\cdot)$ -that defines a \textit{learnable}
    vector field- parameterised by $\pmb{\theta}$,  some input $y_0$ and a numerical solver that applies $f(\cdot)$ until it reaches $\pmb{y}(T)$.

    \begin{figure}[H]
        \begin{center}
            \input{resnet_to_node_plot.pgf}
        \end{center}
        \caption{ResNets can be considered as discretizations of an ODE solution. Under this assumption Resnet
        defines a continuous time vector field.}
        \label{fig:resnet_to_node}
    \end{figure}

    \subsection{Training}
    Optimising any model through gradient descent, in a supervised learning setting, requires acquiring the gradient of
    some loss function $\mathcal{L}(\pmb{\theta})$ with respect to (wrt.) the model's parameters $\pmb{\theta}$.
    In reality this loss function is defined upon the joint distribution of the inputs and outputs but since we don't
    have access to such quantities we calculate the loss using a numerical approximation based on a subset of all
    possible inputs (along with their corresponding outputs marginal probability) called the training set.
    Usually, the loss for each sample in the dataset is calculated using a cost, or distance, function between the
    output of the network and the desired or ideal output $\pmb{y}^*$.
    Sometimes, the loss function may be dependent on the state on multiple times.
    But for simplicity's sake we will examine the case where it only depends on the state at time $t=T$,
    which as it is apparent, depends indirectly on parameters $\theta$.
    \begin{equation}
        L(\pmb{y}(T), \pmb{y}^*) = L \left( \pmb{y}(0) +\int_{0}^{T} f(\pmb{y}(\tau), \tau, \pmb{\theta}) \,d\tau , \pmb{y}^* \right)
        \label{eq:ode_loss}
    \end{equation}
    One could naively solve the initial value problem using any numerical ODE solver and try to backpropagate the gradients through the steps of the solver.
    This approach may be possible with the help of modern automatic differentiation software but its highly memory inefficient~\cite{matsubara2021symplectic}.
    In order to overcome this problem we are going to follow a different approach.
    To this end let's first define the gradient of $L$ wrt the hidden state at some time $t$, i.e.:
    \begin{equation}
        \pmb a(t) = \frac{\partial L}{\partial \pmb{y}(t) }
    \end{equation}
    The above quantity is called the \textit{adjoint state}.
    It can be proven that for its derivative~\cite{pontryagin2018mathematical}, the following equation holds:
    \begin{equation}
        \pmb{a}^{(1)}(t)
        =
        - \pmb{a}(t)^T
        \frac
        {\partial f( \pmb{y}(t), t, \pmb{\theta} )}
        {\partial \pmb{y}(t) }
        \label{adjoint}
    \end{equation}

    Notice that we can easily compute $\pmb{a}(T)$ as we know the form of the loss function eg.
    mean square error loss.
    Having this initial condition $\pmb{a}(T)$ we can solve the differential equation~\eqref{adjoint} backwards in time to find $\pmb{a}(t)$ for any $t$.
    Solving the equation, values of $\pmb{y}(t)$ may be required.
    We can get them by solving the initial differential equation~\eqref{ivp} starting from $\pmb{y}(t)$ and again moving from time $T$ to time $0$.

    Our goal is to calculate $\nabla_{\pmb{\theta}}L$ which can be done by solving another differential equation or equivalently by evaluating the following integral:
    \begin{equation}
        \nabla_{\pmb{\theta}}L
        = \int_T^0
        \pmb{a}(t)^T
        \frac
        {\partial f(\pmb{y}(\tau), \pmb{\theta})}
        {\partial \theta}
        \, d\tau
        \label{dldtheta}
    \end{equation}
    All these results stem from optimal control theory and more specifically consist specialised cases of Pontryagin's Maximum Principle.
    A discussion on how to prove~\eqref{adjoint} and~\eqref{dldtheta} in a simpler way is presented on Appendix~\ref{adjoint_proof}.

    \subsection{Applications, benefits and limitations}
    There are examples in the literature where researchers have used Neural ODEs to simulate physical problems as the continuous time dynamics fit such problems well.
    Recently the authors of~\cite{chiu2023exploiting} showed Neural CDEs, a variant of neural ODEs, lend themselves well to video modelling tasks, with performance comparable to classical neural nets.
    Other applications include modelling for irregularly sampled time series and continuous normalising flows ~\cite{chen2018neural,kidger2022neural}.
    More generally Neural ODEs could be seen as a drop in replacement for ResNets.

    Despite the scientific interest in the last years continuous time models have mostly remained in the lab.
    One major cause for this is the speed of inference and training.
    Neural ODEs typically require more function evaluations than classical models.
    Furthermore, one subtle limitation is that as the vector fields becomes more and more complex to fit the data the harder and more expensive it gets to solve them numerically.
    For those reasons Neural ODEs tend to become computationally intractable for large datasets.
    Researches have tried to combat these problems using methods like hypersolvers~\cite{poli2020hypersolvers} and algebraically
    reversible solvers~\cite{kidger2021efficient,zhuang2021mali}~.
    Another way to speed up continuous time networks is to speed up the numerical solver through parallelism ~\cite{gander201550}.


    \chapter{Numerical Solvers}
    Ordinary differential equations are heavily featured in many scientific and engineering disciplines.
    We usually wish to solve problems where the dynamics of some system $f$ along with some initial state $y_0$ are
    known, at some initial time $t_0$ and we are tasked to find the value of the state vector at some time $t_1$.
    This problem can be formulated as an initial value problem or IVP .
    \begin{equation}
        \label{eq:sol:ivp}
        y^{(1)}(t) = f(t, y), \quad y(0) = y_0
    \end{equation}
    The exact solutions to these ODEs is usually hard to calculate or intractable.
    To this end, many numerical methods have been developed over the years.

    \subsection{Introduction}
    In this chapter we will make a short reference to some basic theoretical prerequisites of numerical ODE solver.
    Then we will showcase some important methods.
    For simplicity we will focus on scalars but all the theorems are easily extensible to vectors.
    Moreover, we can consider higher dimensional IVPs as a (coupled) collection of scalar ODEs, so the same principles as
    those presented here apply.

    In general even if $f$ is continuous it is not guaranteed that the there exists a unique solution to IVP
    ~\eqref{eq:sol:ivp}.
    Fortunately, under some mild conditions on $f$ the existence and uniqueness of a solution is ensured through the
    following theorem.

    \begin{theorem}[Picard-Lindel\"of theorem]
        Suppose that $f(\cdot , \cdot )$ is a continuous function of its arguments in a region $U$ of the $(t, y)$ plane
        which contains the rectangle
        \begin{equation*}
            \pmb{R}  = \left\{ (t,y) : t_0 \leq t \leq T , \lvert y - y_0 \rvert \leq Y \right\} ,
        \end{equation*}
        where $T > t_0$ and $Y > 0$ are constants.
        Suppose also, that there exists a positive constant $L$ such that
        \begin{equation}
            \label{eq:sol:lipsitz}
            \lvert f(t, y_{\alpha}) - f(t, y_\beta) \rvert \leq L \lvert y_m - y_\beta \rvert
        \end{equation}
        holds whenever $(t, y_\alpha)$ and $(t, y_\beta)$ lie in the rectangle $\pmb{R}$.
        Finally, letting
        \begin{equation*}
            M = \max\left\{\lvert f(t, y) \rvert : (t, y) \in \pmb{R} \right\} ,
        \end{equation*}
        suppose that $M(T - t_0) \leq Y$.
        Then there exists a unique continusously differentiable function $t \mapsto y(t)$, defined on the closed
        interval $[t_0, T_M]$, which satisfies ~\eqref{eq:sol:ivp}.
    \end{theorem}

    The condition ~\eqref{eq:sol:lipsitz} is called \textbf{Lipschitz condition} and $L$ is called the
    \textbf{Lipschitz constant} for f.
    For most applications we are interested in, we can assume that $f$ is Lipschitz.
    There also exists a an extension of Picard's theorem to multivariate functions $f: \mathbb{R}^m \to \mathbb{R}^m$.
    The theorem for multiple dimensions is analogous to the scalar case except that the absolute function above is
    replaced with the Euclidean norm on vectors.

    An important property in the study of differential equations and numerical solvers is that of stability.
    \begin{definition}
        A solution $y = v(t)$ to ~\eqref{ivp} is said to be \textbf{stable} on the interval $[t_0, T ]$ if for every
        $\epsilon >0$ there exists $\delta > 0$ such that for all $z$ satisfying $\lvert  v(t_0) - z \rvert < \delta$
        the solution $y = w(t)$ to the differential equation $y^{(1)}(t) = f(t, y) $ satisfying the initial condition
        $w(t_0) = z$ is defined for all $t \in [t_0, T]$ and satisfies $ \lvert v(t) - w(t) \rvert \leq \epsilon$ for
        all $t$ in $[t_0, T]$.
        A solution which is stable on $[t_0, \infty)$ (i.e. stable on $[t_0, Y]$ for all $Y$ and with $\delta$
        independent of $T$) is said to be \textbf{stable in the Lyapunov sense}.
        Moreover, if
        \begin{equation*}
            \lim_{x\to\infty} \lvert v(x) - w(x) \rvert = 0,
        \end{equation*}
        then the solution $y = v(t)$ is called \textbf{asymptotically stable}.
    \end{definition}

    One can prove~\cite{suli2010numerical} that if $f$ at~\eqref{eq:sol:ivp} is Lipschitz continuous the solution to the
    IVP is stable on $[t_0, T]$.

    \subsection{Forward Euler}
    The Euler method, also known as forward Euler, is one of first developed and hence one of the most basic numerical
    ODE solvers.
    It works by partitioning the interval on which we wish to solve the initial value problem to a mesh of $M$
    discrete points ${t_n: k=0,1,\dots M-1}$ with distances $h_k$ between them, without loss of generality less assume
    the points are equidistant and $h = t_{k+1} - t_k$.
    Supposing we know the value of the function $y$ at some time $t_0$, Euler's method allows us to iteratively
    calculate an approximation $y_n$ for the value of $y(t_n)$ at the mesh points.
    It can be derived directly by the forward difference approximation of the derivative.
    \begin{align}
        y^{(1)}(t_n) &= \lim_{h \to 0} \frac{y(t_n+h) - y(t_n)}{h} \\
        &\approx \frac{y(t_n+h) - y(t_n)}{ h} \quad \text{for small $h$}
    \end{align}
    Rearranging terms in the above equation and substituting the known function for the derivative we arrive at the
    Euler iteration.
    \begin{equation}
        \label{eq:euler_itr}
        y_{n+1} = y_n + h f(t_n, y_n))
    \end{equation}

    The local truncation error is the error of the approximation compared to the true solution for one step of the
    algorithm supposing $y(t_n) = y_n$
    We can intuitively understand that as the \textit{step size} at ~\eqref{eq:euler_itr} gets smaller the lower the
    truncation error becomes.
    This can be shown formally considering the Taylor expansion of $y$ around $t_n$
    \begin{align*}
        y(t_n+h) &= y(t_n) + h y^{(1)}(t_n) + \frac{1}{2} h^2 y^{(2)}(t_n) + \mathcal{O}(h^3) \\
        y(t_n+h) &= y_{n+1}  + \frac{1}{2} h^2 y^{(2)}(t_n) + \mathcal{O}(h^3) \\
        y(t_n+h) - y_{n+1} &= \frac{1}{2} h^2 y^{(2)}(t_n) + \mathcal{O}(h^3) \\
        \text{local error} &= \mathcal{O}(h^2)
    \end{align*}
    So the error scales inversely with the square of the step size, as we will see later higher order solver sacrifice
    some of the speed of Euler's method, which require only one evaluation of $f$ per iteration, to achieve error that
    scales inversely with higher powers of the step size.
    The above analysis refers to the error of one step, as the algorithm continues the error from previous iterations
    accumulates.
    Under some loose conditions, namely $f$ is Lipschitz continuous on $y$ and $y$ has a bounded second derivative it
    can be proven that the global truncation error is roughly proportional to $h$ ~\cite{butcher2016numerical}.
    The exact upper limit of the global truncation error has a more complex but not of significant importance for our
    case.
    This property, the global truncation error being $O(h)$, makes forward Euler a \textit{first order} method.

    \subsubsection{Implicit Euler}
    An alternative derivation of the Euler method is found by integrating the ODE between two consecutive mesh points
    $t_n$ and $t_{n+1}$
    \begin{equation*}
        y(t_{n+1}) = y(t_n) + \int_{t_n}^{t_{n+1}} f(\tau, f(y( \tau)) ) t\tau
    \end{equation*}
    and applying a numerical integration technique known as the rectangle rule
    \begin{align*}
        y(t_{n+1}) &= y(t_n) + h f(t_n, f(y( t_n)) ) \\
        y_{n+1} & = y_n + hf(t_n, y_n)
    \end{align*}
    Different numerical integration methods, called quadratures, give rise to different numerical solvers, for example
    the trapezium rule results in the following formula
    \begin{equation*}
        y_{n+1} =  y_n + \frac{1}{2} h \left[ f(t_n, y_n) + f(t_{n+1}, y_{n+1})\right]
    \end{equation*}
    Notice that in the above equation $y_{n+1}$ appears on both left and right hand sides.
    This is called an explicit Euler method and requires the solution of this implicit equation for each $n$.

    \subsection{General explicit one-step method}
    A general explicit one-step method expands the class of Euler solver and it can be written in the form:
    \begin{equation*}
        y_{n+1} = y_n + h \Phi(t_n, y_n ; h),\quad n=0,1,\dots, M-1
    \end{equation*}
    Where $\Phi$ is a continuous function on its variables, in the case of forward Euler $\Phi(t_n, y_n;h)= f(t_n,y_n).$

    The global error for methods of the form is given by:
    \begin{equation*}
        e_n = y(t_n) - y_n
    \end{equation*}
    The truncation error $T_n$ is given by:
    \begin{equation}
        \label{eq:sol:trunc_err}
        T_n = \frac{ y(t_{n+1}) - y_{n+1} }{ h } = \frac{ y(t_{n+1}) - y(t_n) }{ h } - \Phi(t_n, y(t_n);h)
    \end{equation}
    The next theorem provides a bound on the global error in terms of the truncation
    error.
    It can be shown that if $\Phi$ is Lipschitz continuous in its second argument the explicit one-step method's global
    error is bounded in terms of the local truncation error.
    This suggests that if as truncation error goes to zero as $h \to 0$ then the global error also converges to zero
    (as long as $|e_0| \to 0$ when $h \to 0$).
    This observation motivates the following definition.

    \begin{definition}
        A general explicit one-step numerical method is said to be \textbf{consistent} with the IVP~\eqref{eq:sol:ivp}
        if the truncation error defined in Eq. ~\ref{eq:sol:trunc_err} is such that for any $\epsilon$
        there exists a positive $h(\epsilon)$ for which $T_n < \epsilon$ for $0 < h< h{\epsilon}$ and any pair of points
        $(t_n, y(t_n)), (t_{n+1}, y(t_{n+1}) ) $ on any solution curve in \pmb{R}.
    \end{definition}

    Besides consistent, we wish our solver to be accurate, to quantify the accuracy of a solver we define its order of
    accuracy.

    \begin{definition}
        A general explicit one-step method is said to have \textbf{order of accuracy} p, if p is the largest positive
        integer such that, for any sufficiently smooth solution curve $(t, y(t))$ in $\pmb{R}$ of the
        IVP~\eqref{eq:sol:ivp}, there exists constants $k$ and $h_0$ such that
        \begin{equation*}
            |T_n| \leq Kh^p \quad \text{for} \; 0<h\leq h_0
        \end{equation*}
        for any pair of points $(t_n, y(t_n)), (t_{n+1}, y(t_{n+1}) ) $ on the solution curve.
    \end{definition}

    Having introduced the concepts of consistency and the order of accuracy we can move to more involved and more
    accurate methods called Runge-Kutta methods.

    \subsection{Runge-Kutta methods}
    Runge-Kutta methods, introduced by Runge with significant contributions by Heun and Kutta, aim to improve Euler's
    method accuracy by introducing intermediate evaluations of $f$ between $t_n$ and $t_{n+1}$.

    In their most general formulation of a Runge-Kutta method is given by:
    \begin{equation}
        \label{eq:sol:rungekutta}
        y_{n+1} =
        y_{n} + \sum_{i=1}^s b_i k_i,
    \end{equation}
    where
    \begin{equation*}
        k_i = f\left(t_n + c_ih, y_n + h\sum_{j=1}^s a_{ij} k_j \right), \quad i=1,\dots,s
    \end{equation*}
    The above formula is quite terse, and actually includes both explicit and implicit Runge-Kutta methods, we will
    focus on explicit methods.

    \subsubsection{Explicit Runge-Kutta methods}
    It is helpful to start from a specific example, namely the second order Runge Kutta method and generalize to higher
    orders.
    We aim to solve the same initial value as above again on some grid of points ${t_n, k=1, \dots,M-1}$.
    For a single time step we aim to find $y_{k+1} = y(t_n+h)$ knowing $y_n$ and $f$.
    Ideally we would solve the ODE by integrating
    \begin{equation*}
        y(t_n+h) = y_n + \int_{t_n}^{t_n + h} f( \tau, y(\tau) ) d\tau
    \end{equation*}
    But calculating this integral is often hard or even impossible, so we approximate it using some quadrature rule
    which in general can be written as:
    \begin{equation*}
        y(t_n + h) \approx y_n + h\sum_{i=1}^N \omega_i f(t_n + c_i h, y(t_n + c_i h))
    \end{equation*}
    The specific quadrature used generates and a different order Runge Kutta method.
    If we were for example to use $N=4$ and the 3 point Simpson's quadrature rule we would derive the fourth order
    Runge-Kutta method commonly referred to as ``RK4''.
    But our goal here is to derive a general second order method so we use $N=2$, and we also choose $c_1 = 0$.
    \begin{equation*}
        y(t_n+h) \approx y(t_n) + hb_1 f(t_n, y_n)) + hb_2 f(t_n + c_2 h, y(t_n+ c_2 h) )
    \end{equation*}
    The first term we can easily calculate since we know all the relative quantities and we denote
    $k_1 = h\:f(t_n, y_n)$.
    The second term contains $y(t_n + c_2 h)$ which is unknown, but we can calculate its Euler approximation and
    introduce $ k_2 = h\:f(t_n + \alpha h, y_n + \beta k_1) $ where $a=c_2, b = c_2 - t_n$.
    Finally, we have the following (note that a third order method would also have $k_3$ term that would include $k_1$ and $k_2$, a
    fourth $k_4$ etc)
    \begin{equation*}
        y(t_n + h) = b_1 k_1 + b_2 k_2
    \end{equation*}
    To find out the unknown variables $b_1, b_2, \alpha, \beta$ we replace the left hand side with its Taylor expansion
    around $t_n$:
    \begin{equation}
        y(t_n) + h y^{(1)}(t_n) + \frac{h^{2}}{2!}y^{(1)}(t_n) + \mathcal{O}\left(h^{3}\right)
        =
        y(t_n) + b_{1}k_{1} + b_{2}k_{2}.
        \label{eq:tayl1}
    \end{equation}
    In the above equation $y^{(1)}(t_n)$ is obviously $f(t_n, y(t_n))$ and
    $y^{(2)}(t_n) = \left. \frac{d}{dt} f(t, y(t)) \right|_{t=t_n}$ since $f$ has both direct and indirect dependence of t
    it is expanded as $\frac{d}{dt} f(t, y(t)) = f_{t} + ff_{y}$  from the multivariate chain rule, where $f_x$ is
    $\frac{\partial f}{\partial x}(t, y(t))$, we omit $f$'s arguments omitted for convenience.
    Substituting $k_1$, $k_2$ in ~\eqref{eq:tayl1} we get:
    \begin{equation}
        hf + \frac{h^{2}}{2}(f_{t} + ff_{y}) + \mathcal{O}\left(h^{3}\right)
        =
        b_{1}hf + b_{2}hf(t + \alpha h, y + \beta k_{1}).
        \label{eq:tayl2}
    \end{equation}
    We now apply the 2-dimensional Taylor expansion on the right-hand side of ~\eqref{eq:tayl2}:
    \begin{align*}
        hf + \frac{h^{2}}{2}(f_{t} + ff_{x}) + \mathcal{O}\left(h^{3}\right)  &=
        b_{1}hf + b_{2}(hf +\alpha h^{2}f_{t} + \beta h^{2} ff_{x}) + \mathcal{O}\left(h^{3}\right)
        \\
        hf + \frac{h^{2}}{2}(f_{t} + ff_{x}) + \mathcal{O}\left(h^{3}\right)  &=
        (b_{1} + b_{2})hf + h^2 (\alpha b_1 f_{t} + \beta b_2 ff_{x}) + \mathcal{O}\left(h^{3}\right)
    \end{align*}
    In order for the Runge-Kutta method to agree with the Taylor expansion of the true value of $y$ at $t_n + h$ up to
    $\mathcal{O}(h^3)$ accuracy we need to choose:
    \begin{align*}
        b_1 + b_2 = 1, \\
        \alpha b_1 = 1/2, \\
        \beta b_2 = 1/2
    \end{align*}
    Choosing different parameters that satisfy these constraints results to different second order Runge-Kutta methods.

    Generally explicit methods of order N are of the form:
    \begin{equation*}
        y_{n+1} = y_n + h \sum_{i=1}^{N} b_i k_i
    \end{equation*}
    where
    \begin{align}
        k_1 &= f(t_n, y_n), \\
        k_2 &= f(t_n + c_2 h, y_n + (a_{21} k_1) h), \\
        k_3 &= f(t_n + c_3 h, y_n + (a_{31} k_1 + a_{32 k_2})h), \\
        &\vdots \\
        k_N &= f(t_n + c_s h, y_n + (a_{s1}k_1 + a_{s2}k_2 + \dots + a_{s, s-1}k_{s-1})h)
    \end{align}

    Similarly to the second order case to specify a $N$-th order method we need the coefficient $a_{ij}$ called the
    Runge-Kutta matrix, the weights $b_i$ and the nodes $c_i$.
    It is also true in the general case that for a Runge-Kutta method to be consistent it is a necessary and sufficient
    condition for $b_i$ to hold that.
    \begin{equation*}
        \sum_{i=1}^{s} b_i = 1
    \end{equation*}

    A convenient representation for the equations at ~\eqref{eq:sol:rungekutta} is the Butcher tableau, it contains
    the coefficients matrix, the weights and the nodes in a grid like structure.
    For an explicit method the Butcher tableau is written as:
    \begin{equation*}
        \begin{array}
        {c|cccccc}
            0 \\
            c_2 & a_{21} \\
            c_3 & a_{31} & a_{32} \\
            \vdots & \vdots & & \ddots \\
            c_s & a_{s_1} & a_{s2} & \dots & a_{s,s-1} \\
            \hline \\
            & b_1 & b_2 & \dots & b_{s-1} & b_{s}
        \end{array}
    \end{equation*}

    As we mentioned above the ``RK4'' method is the most well known explicit Rugne-Kutta method, its Butcher tableau
    looks like this:
    \begin{equation*}
        \begin{array}
        {c|cccc}
            0 \\
            1/2 & 1/2 \\
            1/2 & 0 & 1/2 \\
            1 & 0 & 0 & 1 \\
            \hline \\
            & 1/6 & 1/3 & 1/3 & 1/6
        \end{array}
    \end{equation*}
    We started this section with the simplest numerical ODE method forward Euler, it can also be expressed as a first
    order explicit Runge Kutta method whose Butcher tableau is:
    \begin{equation*}
        \begin{array}
        {c|c}
            0 \\
            \hline \\
            & 1
        \end{array}
    \end{equation*}


    \chapter{Parallelism in Time}
    It is apparent that Moore's law is crumbling.
    In the last decades focus has shifted to parallel computation to increase performance rather than increasing single
    threaded frequencies~\cite{theis2017end}.
    In the field of deep learning, massive parallelisation of training and inference workloads have led to major
    breakthroughs ~\cite{vaswani2017attention}.
    Differential equations solvers have intrinsic serial characteristics.
    Each iteration of a time stepping algorithm depends on the previous steps.
    Nevertheless, after the seminal work presented in~\cite{nievergelt1964parallel}, several methods have been proposed
    for parallel in time, numerical integration.
    Recent advances include the parareal algorithm~\cite{maday2002parareal} and PFASST~\cite{emmett2012toward} algorithm.
    Massaroli et al. \cite{massaroli2021differentiable} used principles of multiple shooting layers,
    a subcategory of parallel in time methods, and root finding algorithms, to accelerate neural ODES.
    In the next subsection we introduce a new parallel in time algorithm which is based on the use of a polynomial
    approximation of the solution of the IVP.

    \subsection{Parareal}

    \subsection{Polynomial approximation}
    Let us consider the following initial value problem that we wish to solve in some time interval
    $t \in [0 , T]$ with $\pmb{y} (t_0)= \pmb{y}_0$.
    We will examine the scalar case first where $y : \mathbb{R} \to \mathbb{R}$:
    \begin{equation}
        y^{(1)}(t) = f(y(t)) \label{dydt}
    \end{equation}
    We define the $N$-th order polynomial approximation  $\hat{y}(t)=\sum_{n=0}^{N-1} b_n \phi_n(t) $  where $ \phi_n(t) $ are basis functions
    for a vector space of polynomials and $b_n$ are some coefficients we wish to find.
    We can also express $\hat{y}$ as the dot product $\hat{y}(t)=  \pmb{b}^t \pmb{\phi}(t)$ where
    $\pmb{\phi} : [0,T] \to \mathbb{R}^N$ and $\pmb{b} \in \mathbb{R}^N$.
    By substituting $\hat{y}$ in~\eqref{dydt} we get:
    \begin{align}
        \pmb{b}^T \pmb{\phi}^{(1)}(t)   &\approx f( \pmb{b}^T \pmb{\phi}(t) ) \label{dPhidt}
        \\
        \pmb{b}^T \pmb{\dot{\phi}}(t)  &\approx f( \pmb{b}^T \pmb{\phi}(t)) \label{dphi_fphi}
    \end{align}
    We define an approximation error (in the mean square error sense) that we wish to minimise in the interval of interest.
    \begin{equation}
        \label{eq:error_wrt_B}
        e(\pmb{b}) = \int_0^T
        \left(
        \pmb{b}^T  \, \pmb{\dot{\phi}}(\tau) - f( \pmb{b}^T \pmb{\phi}(\tau)   )
        \right)^2
        d\tau
    \end{equation}

    Depending on the choice of basis functions, the derivative of $\pmb{\phi}(t)$ with respect to time can be trivial
    to obtain analytically.
    We opt to use Chebyshev polynomials as the basis functions of our approximation which are known for their
    properties in approximation theory ~\cite{trefethen1996finite}.
    They form an orthonormal basis for functions in $[-1,1]$ but we can use them in our desired interval through a
    simple change of variables while being mindful of the differential in $\frac{d}{dt} \pmb{\phi}(t) $.
    More specifically, through the linear transformation $\hat{t} = 2*\frac{ t -  \alpha} { \beta - \alpha} -1$ we can
    move the integration interval from $[a,b]$ to $[-1,1]$.
    By substituting $t = \hat{t}$  in ~\eqref{dPhidt} we get:
    \begin{align*}
%        \label{dydt_hat}
         \pmb{b}^T \pmb{\phi}^{(1)}(\hat{t}(t)) &= f( \pmb{b}^T \pmb{\phi}(\hat{t}(t)) ) \\
        \hat{t}^{(1)}(t)  \;  \pmb{b}^T \pmb{\phi}^{(1)}(\hat{t}(t))  &= f( \pmb{b}^T \pmb{\phi}(\hat{t}(t)) ) \\
        \frac{2}{b-a} \pmb{b}^T \pmb{\dot\phi}(\hat{t}) &= f( \pmb{b}^T \pmb{\phi}(\hat{t}) )
    \end{align*}
    For the sake of convenience for the rest of the section we omit the hat from $t$ and the constant in front of
    $\pmb{\dot\phi}(t)$ but keep in mind that the limits of integration are $[-1,1]$
    \\

    Chebyshev polynomials of the first kind $T_n(t)$ can be defined by:
    \begin{equation}
        T_n(\cos \theta) = \cos(n \theta)
    \end{equation}
    While Chebyshev polynomials of the second kind can be defined by:
    \begin{equation}
        U_n(\cos \theta) \sin \theta = \sin( (n+1)\theta )
    \end{equation}
    Alternatively, they can be obtained through the recurrence relationships:
    \begin{equation}
        \label{eq:cheb_rec}
        \begin{aligned}[c]
            T_0(t) &= 1 \\
            T_1(t) &= t \\
            T_n(t) &= 2tT_{n-1}(t) - T_{n-1}(t)
        \end{aligned}
        \qquad \quad
        \begin{aligned}[c]
            U_0(t) &= 1 \\
            U_1(t) &= 2t\\
            U_n(t) &= 2tU_{n-1}(t) - U_{n-1}(t)\\
        \end{aligned}
    \end{equation}
    It can also be shown that for the derivative of $T_n$ it holds that:
    \begin{equation}
        T_n^{(1)} = n U_{n-1}
        \label{Dcheb}
    \end{equation}

    Generalising to a vector valued $\pmb{y} : [t_0=-1,t_1=1] \to \mathbb{R}^L$ the collection of coefficients becomes a matrix $B\in \mathbb{R}^{N \times L}$.
    Another way of thinking about this is stacking $L$ coefficient vectors, one for each dimension next to each other, forming the coefficient matrix $B$.
    The error function is then reformulated as
    \begin{align}
        e(B) &= \int_{t_0}^{t_1} \lVert
        B^T \pmb{\dot\phi}(\tau)   -
        f (B^T  \pmb\phi(\tau)  )
        \rVert^2
        \; d\tau
        \label{approx_error}
    \end{align}
    Since we operate in discrete time we define a grid of $M$ points $\pmb{t} = [t_0 \; t_1 \; \dots \; t_{M-1}]^T$ and
    replace the integral with a Riemann sum.
    The points are usually equidistant but other sets of points between $t_0$ and $t_1$ could be used.
    For our calculations we use the Chebyshev node of the second kind $t_m = \cos\left( \frac{m\pi}{M-1} \right), \ m=0,\dots,M-1$.
    \begin{align}
        e(B) &= \sum_m \lVert
        B^T \pmb{\dot\phi}(t_m)  -
        B^T f (  \pmb\phi(t_m)  )
        \rVert^2
        \Delta t_m
        \label{approx_error_sum}
    \end{align}
    We therefore seek to find coefficients $B^*$ that minimise the above error.
    To find the minimum we set the derivative wrt $B$ equal to zero and solve for $B$
    \begin{equation}
        \label{eq:error_zero}
        \nabla_B e(B) = 0
    \end{equation}
    Depending on the form of $f$ it can be hard -or even impossible- to find a closed form solution to this problem.
    We therefore resort to an iterative process for finding $B$, starting from some $B_0$ we form the solution as
    $B_{k+1} = B_{k} + \eta \Delta B_{k}$, $\eta$ being a positive scalar learning rate.
    Depending on the order of the optimisation method used $\Delta B_{k}$ takes different forms, most
    commonly (under the gradient descent schema) $\Delta B_{k}$ is defined as $\nabla_{B_{k}} e$.

    \subsubsection{A zero order optimisation algorithm}
    Let's substitute $B$ at $(k+1)$-th iteration in ~\ref{eq:error_zero}:
    \begin{align}
        \nabla_{B_{k+1}} \sum_m \lVert
        B_{k+1}^T \pmb{\dot\phi}(t_m)  -
        f ( (B_{k+1}^T \pmb\phi(t_m)  )
        \rVert^2_2
        \Delta t_m = 0 \\
        \nabla_{B_{k+1}}\sum_m \lVert
        B_{k+1}^T \pmb{\dot\phi}(t_m)  -
        f \big( (B_{k} + \eta \Delta B_k)^T \pmb\phi(t_m)  \big)
        \rVert^2_2
        \Delta t_m = 0 \label{eq:B_inside_f}
    \end{align}
    We substitute the function $g(B, t) = f(B^T \pmb{\phi}(t) )$ inside the sum with its zero-th order
    approximation around $B_k$, meaning the constant (on $B$) function $f( B_k^T \pmb{\phi}(t) )$.
    Exchanging the nabla and applying some matrix calculus we get:
    \begin{align}
        \sum_m \;
        \left(
        B_{k+1}^T \pmb{\dot\phi}(t_m)  -
        f \big( (B_{k}^T \pmb\phi(t_m)  \big)
        \right)
        \pmb{\dot\phi}^T(t_m)
        \Delta t_m = 0 \\
        \sum_m
        B_{k+1}^T \pmb{\dot\phi}(t_m) \pmb{\dot\phi}^T(t_m) \Delta t_m-
        f \big( (B_{k}^T \pmb\phi(t_m)  \big) \pmb{\dot\phi}^T(t_m) \Delta t_m
        = 0
        \label{eq:grad_err}
    \end{align}
    We notice in the above relationship that the sums of outer products can be rewritten as matrix multiplications, to
    that goal lets define the following quantities.
    \begin{equation*}
        \Phi =  \pmb{\phi}(\pmb{t}^t)  =
        \begin{bmatrix}
            T_0(t_0)     & T_0(t_1)     & \dots & T_{0}(t_{M-1})   \\
            T_1(t_0)     & T_1(t_1)     & \dots & T_1(t_{M-1})     \\
            & & \vdots \\
            T_{N-1}(t_0) & T_{N-1}(t_1) & \dots & T_{N-1}(t_{M-1}) \\
        \end{bmatrix}\label{eq:cheb_mat}
    \end{equation*}
    From ~\eqref{Dcheb} we can define the time derivatives of $\pmb\phi$ as:
    \begin{equation*}
        \label{DPHI}
        \dot{\Phi} = \pmb{\dot\phi}(\pmb{t}^t) =
        \begin{bmatrix}
            0                        & 0                        & \dots & 0                            \\
            1\cdot U_0(t_0)          & 1\cdot U_0(t_1)          & \dots & 1\cdot U_0(t_{M-1})          \\
            & & \vdots \\
            (N-1) \cdot U_{N-2}(t_0) & (N-1) \cdot U_{N-2}(t_1) & \dots & (N-1) \cdot U_{N-2}(t_{M-1})
        \end{bmatrix}
    \end{equation*}
    Using these matrices we can rewrite Eq.~\ref{eq:grad_err} in terms of matrix products as follows:
    \begin{align}
        0 &=
        B_{k+1}^T \dot\Phi (\dot\Phi^T \odot \Delta \pmb{t} \cdot \pmb{1}^T  ) -
        f \big( B_{k}^T  \Phi \big) ({\dot\Phi}^T \odot \Delta \pmb{t} \cdot \pmb{1}^T )
        \\
        B_{k+1}^T &=
        f \big( B_{k}^T \Phi \big) ( {\dot\Phi}^T \odot \Delta \pmb{t} \cdot \pmb{1}^T)
        ( \dot\Phi (\dot\Phi^T \odot \Delta \pmb{t} \cdot \pmb{1}^T ) )^{-1} \label{update}
    \end{align}
    Eq.~\ref{update} outlines the iteration scheme for this zero order optimisation algorithm.
    Omitting the partitions' lengths terms $\Delta \pmb{t}$ temporarily for clarity:
    \begin{equation}
        \label{eq:biter}
        B_{k+1}^T =
        f \big( B_{k}^T \Phi \big) {\dot\Phi}^T
        ( \dot\Phi \dot\Phi^T )^{-1}
    \end{equation}
    It becomes apparent that the inverted matrix is the same for all iterations meaning it only needs to be inverted
    once at the beginning.
    Furthermore, $f$ is applied to each row of its input matrix in parallel, which provides a parallel speed-up to
    the algorithm.
    \begin{figure}[H]
        \begin{center}
            \input{sol_regression.pgf}
        \end{center}
        \caption{ Regression of the approximate solutions to the true solution. As the coefficients go to the
        optimum and the error function approaches the minimum the trajectories are getting closer to the true solution.}
        \label{fig:reg_sol}
    \end{figure}

    \subsubsection{Higher order minimization}
    It's apparent that the accuracy of this iterative solution depends on the residual of the zero order
    approximation on Eq.~\eqref{eq:grad_err}.
    The zero order algorithm converges rather fast but can't achieve high accuracy.
    Experimental results show that it oscillates around the optimum.

    To combat that, we can use the solution matrix $\bar B$ as the starting value of a higher order method to
    increase accuracy.
    A simple choice is a fist degree optimisation method such as gradient descent or one of each variants.

    Another choice is the Newton-Raphson algorithm with Hessian modification, a second order method.
    A version of the algorithm ~\ref{alg:linesearh} is showcased in Appendix~\ref{newtonalgo}, where line-search is used for the learning
    rate or step size of the update.

    Combining polynomial approximation with the Newton algorithm we arrive at algorithm ~\ref{alg:intapprox_2}.

    \begin{algorithm}
        \caption{Polynomial approximation numerical integration}
        \begin{algorithmic}
            \State choose $\epsilon_{ls}$, $\epsilon_{n}$, $B_0$, $K_max$
            \State $\pmb{t} \gets [t_0 \; t_1 \; t_2 \; \dots \; t_{M-1} ]^T$
            \State $k \gets 1$
            \State calculate $\Phi, \dot\Phi$
            \Repeat
                \State calculate $e(B_k]$
                \If { $e(B^k)< \epsilon_{n}$ }
                    \State \textbf{return} $B_k$
                \EndIf
                \State $ \pmb{d}^{[k]} \gets -\left[ \nabla^2 e(B^{[k]}) \right]^{-1} \nabla e(B^{[k]})$
                \State $ \alpha^{[k]} \gets \text{lineseach}(e, B^{[k]}, \pmb{d}^{[k]}) $
                \State $ B^{[k+1]} \gets B_k + \alpha \pmb{d}^{[k]}$
                \State $k \gets k+1$
            \Until{$k>k_max$}
        \end{algorithmic}
        \label{alg:intapprox_2}
    \end{algorithm}

    \subsection{Realisation details}

    \subsubsection{Imposing initial conditions}
    Since the value of $y(t)$ and $f(y(t))$ are known at $t=0$ we can calculate a part of $B$ analytically which
    allows to:
    \begin{itemize}
        \item impose our initial conditions
        \item to reduce the computational cost.
    \end{itemize}
    Let's denote with $\pmb{b}_n$ the $n$-th column of $B^T$, $B_{2:}$ the last $N-2$ columns of $B^T$ and $\pmb\phi_{2:}$ the last
    $M-2$ elements of $\pmb\phi$.
    Starting with initial state:
    \begin{align}
        \pmb{y}(t_0) = B^T \pmb\phi(t_0)
        =
        \begin{bmatrix}
            \pmb{b}_0 & \pmb{b}_1
        \end{bmatrix}
        \begin{bmatrix}
            \phi_0(t_0) \\ \phi_1(t_0)
        \end{bmatrix}
        + {B}_{2:} \pmb{\phi_{2:}}(t_0)
    \end{align}
    We can write a similar equation for $f(\pmb{y}_0)$.
    \begin{align}
        f(\pmb{y}(t_0)) = B^T \pmb{\dot\phi}(t_0)
        =
        \begin{bmatrix}
            \pmb{b}_0 & \pmb{b}_1
        \end{bmatrix}
        \begin{bmatrix}
            \dot\phi_0(t_0) \\ \dot\phi_1(t_0)
        \end{bmatrix}
        + {B}_{2:} \pmb{\dot\phi_{2:}}(t_0)
    \end{align}
    We can group this two equations in a system:
    \begin{align}
        \begin{bmatrix}
            \pmb{y}_0 & \pmb{f}_0
        \end{bmatrix} =
        \begin{bmatrix}
            \pmb{b}_0 & \pmb{b}_1
        \end{bmatrix}
        \begin{bmatrix}
            \phi_0(t_0) & \dot{\phi}_0(t_0) \\
            \phi_1(t_0) & \dot{\phi}_1(t_0)
        \end{bmatrix} +
        B_{2:}
        \begin{bmatrix}
            \pmb\phi_{2:}(t_0) \\ \pmb{\dot{\phi}_{2:}}(t_0)
        \end{bmatrix}
    \end{align}
    Solving for $[\pmb{b}_0 \;\; \pmb{b}_1]$ we have:
    \begin{equation}
        \label{eq:pan:B01}
        \begin{bmatrix}
            \pmb{b}_0 & \pmb{b}_1
        \end{bmatrix}
        =
        \left(
        \begin{bmatrix}
            \pmb{y}_0 & \pmb{f}_0
        \end{bmatrix}
        -
        B_{2:}
        \begin{bmatrix}
            \pmb\phi_{2:}(t_0) \\ \pmb{\dot{\phi}_{2:}}(t_0)
        \end{bmatrix}
        \right)
        U^{-1}
    \end{equation}
    where:
    \begin{equation*}
        U =
        \begin{bmatrix}
            \phi_0(t_0) & \dot{\phi}_0(t_0) \\
            \phi_1(t_0) & \dot{\phi}_1(t_0)
        \end{bmatrix}
    \end{equation*}

    The lower triangular structure of $\phi(t)$ ensures that this $2\times 2 $ matrix is always invertible.
    This can be show easily since $\phi_0(t_0) = T_0(-1)$ is always $1$,  $\phi_1(t_0) = T_1(-1) = -1$, the derivative of
    $T_0$ is obviously $0$ and $\dot\phi_1(t_0)$ is something non-zero.

    Equation ~\eqref{eq:pan:B01} allows us to express the first two rows of the coefficients matrix $B$ as a linear combination
    of the rest $N-2$ rows.
    This means we don't need to ``learn'' them though optimisation, we instead minimize the error subject to the
    remaining rows.

    The initialisation of $B$ is in the users choice, it could be random matrix.
    Alternatively, one could use a coarse ODE solver, likely a forward Euler iteration with relatively large step size,
    to get a rough estimate $\pmb{y}_c(t)$.
    Then by solving the least squares problem $\pmb{y}_c(t) - B^T_0 \Phi $ get an initialisation that's closer to the
    optimum, $B^*$, for faster convergence.
    One has to figure out a good balance between the number of steps of the coarse solver -which are innately serial-
    and how far the initial $B$ is from the optimum, which will increase the number of iterations.

    \subsubsection{The Hessian}

    The primary concern computationally is the number of function evaluations or NFE of $f$,
    the neural networks.
    The number of coefficients in $B$ is orders of magnitude smaller that the number of parameters in the network,
    hence storing the full Hessian shouldn't be a limiting factor.

    Instead of directly inverting the Hessian to obtain the next $B$, we use a modified Cholesky decomposition
    (specifically of $LDU$ form) as described in ~\cite{gill2019practical}.
    We use this technique in order to project the Hessian to a space of positive definite matrices in case it is not,
    which is a necessary condition for the Newton algorithm to converge.
    This way we can increase the definiteness of the Hessian while also solving the linear system
    $\pmb{d}^{[k]} \gets -\left[ \nabla^2 e(B^{[k]}) \right]^{-1} \nabla e(B^{[k]})$ as an LDU system (two triangular
    and one diagonal systems).
    To prevent confusion around the dimensionality of the Hessian $\nabla_{B} e$,
    the columns of $B$ are stacked as a vector so that the hessian is a rectangular matrix instead of a $4D$ tensor.

    \subsection{Comparison with classical and other parellel-in-time methods}
    First of all, the parallelisation potential of such a method comes from the fact that $f$ can be evaluated for all
    input points $\hat y(t)$ simultaneously at a single iteration.
    More specifically, during the fist, zero-th order part of the algorithm, a vector of points is passed into $f$ to
    update the coeffecients as can be seen in Eq.~\eqref{eq:Biter}.
    In reality, $f$ is applied on each element of this vector but since the calculations are independent all elements are
    evaluated in parallel.
    Similarly, during the Newton iteration the Hessian can, up to some degree, be computed in parallel by exploiting
    vectorization.
    This is in contrast to classical ODE solvers where the solution, or the trajectory in state space, is calculated from
    initial time to terminal time, step by step; in our case the whole trajectory (a continuous time approximation
    of it rather) is known from the start.
    This is what allows to calculate the values -and the derivatives- along the entire trajectory in a parallel manner.

    Obviously, more than one iteration is needed to obtain a solution to some specified accuracy.
    But if there are sufficiently many computing nodes available, each iteration should be at least comparable to a
    single step of many common ODE solvers.
    The critical point of the proposed method is that it takes less parallel/batch evaluations of $f$ to converge to a solution
    than steps of a time stepping solver.

    For the second part of the algotithm the Jacobian and Hessian matrices of $e$ also have to be calculated.
    Because of how automatic differentiation works the Jacobian is a byproduct of calculating the Hessian.
    Calculating the full Hessian is actually not of trivial cost and we will address this later.
    Additionally, the line search algorithm -for finding step sizes along the path of the Newton direction requires
    some evaluations of $e$ and its Jacobian.
    Decreasing the number of function evaluations is crucial for further speeding up this algorithm.
    To this goal the cheaper recursive least squares solution is found first and the Newton algorithm is used for
    refining the solution to accuracy.

    It's important to note that traditional ODE numerical solver also take more that 1 (serial) function evaluations at
    each step.
    For example RK45, a widely used algorithm from the family of Runge-Kutta methods, involves six sequential function
    evaluations at each step~\cite{suli2010numerical}.

    Comparing popular parallel in time ODE solving methods, like PFASST and parareal, to our approach showcases
    similarities but also differences.
    One major difference is simplicity which allows easier implementation.
    Polynomial Approximation Numerical integration only builds upon simple polynomial approximation and a numerical
    optimiser.
    Other methods, like PFASST ~\cite{emmett2012toward}, rely on more complicated \textit{spectral deferred correction}
    methods, a well established but more involved numerical method.
    Moreover, other parallel in time techniques, like parareal, split the initial value problem into $N$ smaller boundary
    value problems using a combination of coarse and fine solvers ~\cite{maday2002parareal}.

    One advantage of the method presented here is that it evaluates $f$ on multiple inputs simultaneously,
    to minimise some error function.
    This \textit{synchronised} parallelisation scheme resembles how multiples input samples are processed in batches
    during training of a neural network.
    This means that one can exploit GPUs' architecture for mass parallelisation using existing frameworks.

    \subsection{Shortcomings and improvements}
    As mentioned above the critical factor for the performance of neural ODEs is the number of function evaluations that
    have to be performed sequentially.
    While our experiments show that polynomial approximation integration can reduce NFE there are hidden costs involved
    with the calculating of the full Hessian and during line search.

    Firstly, because the parameters $B$ should be significantly less than the parameters of the network; calculating the
    full Hessian doesn't present problems as far as memory is concerned.
    But since Newton optimisation requires the inverse of the Hessian it is not possible to perform a fast
    Hessian-vector operation exploiting automatic differentiation properties.
    We instead calculate the full Hessian first, row by row, and then invert it.

    Secondly, the line search algorithm involves computing the objective function and it's derivative at multiple
    states ($\pmb{y}(t)$).
    While we hope that it shouldn't check more that two or three candidate values for the learning rate we can
    expect at least 4 more function evaluations that will be performed serialy ~\cite{wright2006numerical}.

    These limitations of Newton optimisation with line search call for more numerical methods to be tested like
    Quasi-Newton methods such as BFGS and Conjugate Gradients~\cite{wright2006numerical}.

%    \subsection{hyper solver + parallel in time(???)}
%    Shifting out focus to the more specific problem of training neural ODEs.

    \subsection{Experiments}
    We compare the performance of our method to popular time stepping methods in terms of accuracy and absolute error.
    Most implementations of variable step length methods like RK45 allow for absolute (atol) and relative (rtol)
    tolerances to be specified which keep the local error estimates bellow $atol + rtol |y|$.
    For our case, we define tolerances for both steps of the algorithm, least squares and Newton iteration in the
    following ways.
    For the first part, the algorithm stops when there is no sufficient change in the parameters:
    $\lVert B^{[k]} - B^{[k-1]} \rVert_2 < $ tol\_ls.
    While for the Newton part the iteration stops when the gradient is sufficiently close to zero:
    $ \lVert \nabla_e \rVert < $ tol\_newton.
    Adjusting these two \textit{hyperparameters} as well as the number of approximation coefficients and the number of
    points where the error is calculated; we can change the convergence properties of the algorithm.

    \subsubsection{Spiral}
    First, we will examine a toy example of a nonlinear spiral in $\mathbb{R}^{2}$.
    We assume that the true solution is provided by the RK45 method with step size three orders of magnitude smaller than the
    interval.
    We assume this approximation to be good enough for out purpose and compare the norm of the error at terminal time
    with other alongside our approach.

    \begin{equation*}
        \pmb{y}^{(1)}  =
        \tanh(
        \begin{bmatrix}
            a & 1 \\ -1 & -a
        \end{bmatrix}
        \pmb{y}
        )
    \end{equation*}

    We solve the ODE in the interval $[0,10]$ for $\pmb{y}(0) = [2,\ 1]^T$.
    We use the same tolerances for the time stepping methods and for ours (PAN) even thought there is not direct
    equivalence.
    This shouldn't matter as we are ultimately interested only in the comparison between accuracy and number of function
    evaluations (NFE).
    For the other solvers the relative error is also set to a small number so that the absolute error is unaffected.

    \renewcommand{\arraystretch}{1.5}
    \renewcommand{\tabcolsep}{10.25pt}
    \begin{table}[!h]
        \begin{center}
            \caption{Metrics for the solution of an IVP with different methods. For PAN params indicates the number of
            approximation coefficients per dimention, in this case there are 2 dimensions.
            NFE for PAN is show as NFE for least squares plus (+) NFE for Newton.}
            \label{spiral-metrics}
            \begin{tabular}{|c|c|c|c|}
                \hline
                Method(params) & tolerance & error(t) & NFE \\
                \hline
                RK45 & 1.0e-06 & 4.14e-07 & 212
                \\ \hline
                Radau & 1.0e-06 & 6.97e-08 & 599
                \\ \hline
                BDF & 1.0e-06 & 4.56e-06 & 250
                \\ \hline
                LSODA & 1.0e-06 & 4.34e-06 & 171
                \\ \hline
                PAN(50) & 1e-06/1e-06 & 1.86e-10 & 37 + 5
                \\ \hline
                PAN(100) & 1e-06/1e-06 & 4.17e-14 & 36 + 5
                \\ \hline \hline
                RK45 & 1.0e-09 & 6.28e-10 & 716
                \\ \hline
                Radau & 1.0e-09 & 1.05e-10 & 2994
                \\ \hline
                BDF & 1.0e-09 & 1.66e-08 & 656
                \\ \hline
                LSODA & 1.0e-09 & 4.48e-10 & 361
                \\ \hline
                PAN(50) & 1e-09/1e-09 & 1.86e-10 & 42 + 5
                \\ \hline
                PAN(100) & 1e-09/1e-09 & 2.25e-15 & 55 + 5
                \\ \hline
            \end{tabular}
        \end{center}
    \end{table}

    We remind that for PAN, during optimisation $f$ is evaluated on multiple points, but since this calculations can be
    performed in parallel we assume the wall clock time is similar to one evaluation plus parallelisation overhead.
    As we can see in Table ~\ref{spiral-metrics} for this simple problem our method achieves better accuracy while
    taking significantly less number of function evaluation.

    \subsubsection{Lorenz Attractor}
    The Lorenz system is a system of ordinary differential equations known for its chaotic behaviour.
    \begin{align*}
        x^{(1)}&= \sigma( y - x ) \\
        y^{(1)}&=  x( \rho - z) - y \\
        z^{(1)}&=  xy - \beta z\\
    \end{align*}

    We again solve the equation using a variety of solvers and compare them with our method.
    Though, this time we use ``step'' for our method, meaning we apply the algorithm in smaller sub-intervals setting
    the endpoint of each sub-solution as the initial conditions for the next interval.
    The problem parameters are set to: $\rho = 28, \ \sigma=10,\ \beta = \frac{8}{3}$, the interval is $[0, 15]$ and
    $\pmb{y}(0) = [2, 1 ,1]^T $.

    \begin{table}[!h]
        \begin{center}
            \caption{Accuracy and NFE for Lorenz system. For PAN the interval has been split in subintervals of length
                $0.3$ and the algorithm has been applied sequencially on each interval.}
            \label{lorenz_metric}
            \renewcommand{\arraystretch}{1.5}
            \renewcommand{\tabcolsep}{10.25pt}
            \begin{tabular}{|c|c|c|c|}
                \hline
                Method(params) & tolerance & error(t) & NFE \\
                \hline
                RK45 & 1.0e-06 & 5.75e-02 & 5174
                \\ \hline
                Radau & 1.0e-06 & 2.17e-02 & 16476
                \\ \hline
                BDF & 1.0e-06 & 2.09e-01 & 4794
                \\ \hline
                LSODA & 1.0e-06 & 4.86e-02 & 2803
                \\ \hline
                PAN(50) & 1e-06/1e-06 & 1.60e-07 & 1180 + 250
                \\ \hline
                PAN(100) & 1e-06/1e-06 & 1.60e-07 & 1231 + 250
                \\ \hline\hline
                RK45 & 1.0e-09 & 6.58e-05 & 19532
                \\ \hline
                Radau & 1.0e-09 & 4.19e-06 & 89999
                \\ \hline
                BDF & 1.0e-09 & 5.65e-04 & 15184
                \\ \hline
                LSODA & 1.0e-09 & 5.45e-06 & 4735
                \\ \hline
                PAN(50) & 1e-09/1e-09 & 1.61e-07 & 1399 + 262
                \\ \hline
                PAN(100) & 1e-09/1e-09 & 1.61e-07 & 1450 + 390
                \\ \hline
            \end{tabular}
        \end{center}
    \end{table}

    In Table~\ref{lorenz_metric} we see that the proposed method achieves better accuracy for less amount of function
    evaluations on this system of ODEs.
    However, we must address at this point that accuracy seems to become saturated and decreasing tolerances or increasing number
    of coefficients doesn't improve results.
    It becomes apparent that finding the optimal hyperparameters is crucial for the most efficient use of the algorithm.

    \newpage
    \printbibliography

    \appendix


    \chapter{The backpropagation equations}
    In order to contrast and highlight the differences between classical neural networks, residual neural networks and
    neural ODEs we showcase how the gradients of the loss wrt the learnable parameters is calculated in each architecture.
    This quantity is necessary in order to minimise the loss of the network using any -gradient descent based- optimisation algorithm.

    \subsection{Vanilla Neural Networks}
    Classical artificial neural networks are comprised of linear transformations followed by non-linear activation functions.
    The equation that describes the input-output relationship of the $n$-th layer is:
    \begin{equation}
        \bm{y}_{n} = \phi( W_{n} \bm{y}_{n-1} )
    \end{equation}
    Where $\phi$ is a non-linear function, $W_n$ is the layer's weights, $\bm{y}_{n-1}$ the output of the previous layer, we omit the biases vector to simplify the equations but the logic is the same.
    Unfolding this equation for the whole network of depth $N$ we get:
    \begin{equation}
        \bm{y}_{out} = \phi ( W_{N} \cdot \phi(W_{N-1} \cdot  \phi(W_{N-2}( \dots ))))
    \end{equation}
    The output of the network is passed in a (scalar) cost function $C(\bm{y}_{out},\bm{y}^*)$.
    In order to apply gradient descent based optimisation algorithms it is necessary to calculate the gradient wrt.
    all the weights:
    \begin{equation}
        \frac{ \partial C}{\partial W_{n} }, \quad \text{for} \ n=1,2,\dots,l
    \end{equation}
    Focusing on a single layer, lets use $\textbf{z}_{n}$ to denote $W_{n} \textbf{y}_{n-1}$ or whats goes in the non-linear function at each layer.
    By applying the chain rule we get:
    \begin{align}
        \frac{ \partial C}{\partial \textbf{y}_{n-1} }
        =
        \left[ \frac{ \partial \textbf{y}_{n}}{\partial \textbf{y}_{n-1}  } \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        &=
        \left[ \frac{ \partial}{\partial \textbf{y}_{n-1} } \phi(W_n \textbf{y}_{n-1}) \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        \\
        &=
        \left[ W_n^T \phi^{(1)}( \pmb{z}_n ) \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        \\
        &=
        \left[ \phi^{(1)}( \pmb{z}_n ) \right]^T
        W_n \;
        \frac{ \partial C}{\partial \textbf{y}_{n} }
    \end{align}
    The non-linearity $\phi$ is applied on each element of the input vector independently, so its partial derivative wrt.
    the input its a diagonal matrix and its transpose is the same as itself
    \begin{align}
        \frac{ \partial C}{\partial \textbf{y}_{n-1} }
        =
        \phi^{(1)}( \pmb{z}_n )
        W_n \;
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        \label{dCdy}
    \end{align}
    Eq.~\eqref{dCdy} allows to calculate all such derivatives recursively starting from the last layer where $\frac{\partial C}{\partial \textbf{y}_{out}}$ has a closed form solution.
    We can now calculate the desired gradients with respect to the weights.
    Below $W_n^i$ is the $i$-th column of the weights matrix of the $n$-th layer.
    \begin{align*}
        \frac{ \partial C}{\partial W_n^{[i]} }
        =
        \left[ \frac{ \partial \pmb{y}_{n}}{\partial W_n^i } \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        &=
        \left[ \frac{ \partial }{\partial W_n^{[i]} }  \phi( W_{n} \bm{y}_{n-1} )  \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        \\
        &=
        \left[ \frac{ \partial }{\partial W_n^{[i]} }  \phi(  W_{n}^{[1]} \bm{y}_{n-1}^{[1]} + \dots +W_{n}^{[i]} \bm{y}_{n-1}^{[i]} + \dots )  \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        \\
        &=
        \left[ \left[ \pmb{y}_{n-1}^{[i]} \right]^T \phi^{(1)}(\pmb{z}_n )  \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        \\
        &=
        \phi^{(1)}(\pmb{z}_n ) \; \pmb{y}_{n-1}^{[i]}
        \frac{ \partial C}{\partial \pmb{y}_{n} }
    \end{align*}
    Stacking all the columns together we get:
    \begin{equation}
        \label{dCdW}
        \frac{ \partial C}{\partial W_n}
        =
        \phi^{(1)}(\pmb{z}_n )
        \frac{ \partial C}{\partial \pmb{y}_{n} }  \,
        \pmb{y}_{n-1}
    \end{equation}
    Using~\eqref{dCdy} and~\eqref{dCdW} together we can find all the necessary gradients for backpropagation; as expressed in algorithm~\ref{alg:backprop}
    \begin{algorithm}
        \caption{Backpropagation}
        \label{alg:backprop}
        \begin{algorithmic}
            \State Do the forward pass, save in memory $W_n, \textbf{z}_n, \textbf{y}_n$
            \State Calculate $\frac{\partial C}{\partial \textbf{y}_{out}}$ which is trivial
            \For{$n=N-1, \dots, 1$}
                \State Using Eq.~\eqref{dCdy} and  $\frac{\partial C}{\partial \textbf{y}_{n+1}} $ find $\frac{\partial C}{\partial \textbf{y}_n}$
                \State  Using Eq.~\eqref{dCdW} and $\frac{\partial C}{\partial \textbf{y}_n}$ find $\frac{\partial C}{\partial W_n}$
            \EndFor
        \end{algorithmic}
    \end{algorithm}

    The backpropagation algorithm is quite fast in modern hardware as it consists of mostly matrix vector multiplications.
    Although, it is memory intensive since it requires saving all the weights and activations from the forward pass and accessing them during the backward pass.

    \subsection{Residual Networks}
    Residual neural networks operate quire similarly to classical ones in terms of training.
    \begin{equation}
        \bm{y}_{n} = \phi( W_{n} \bm{y}_{n-1} ) + \bm{y}_{n-1}
    \end{equation}
    \begin{align}
        \label{resnet_dCdy}
        \frac{ \partial C}{\partial \textbf{y}_{n-1} }
        =
        \left[ \frac{ \partial \textbf{y}_{n}}{\partial \textbf{y}_{n-1}  } \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        &=
        \left\{
        \frac{ \partial }{\partial \textbf{y}_{n-1}}
        \left[
            \phi( W_n \textbf{y}_{n-1} ) + \textbf{y}_{n-1}
            \right]
        \right\}^T
        \frac{ \partial C}{ \partial \textbf{y}_{n} }
        \\
        &=
        \left[
            W_n^T \phi^{(1)}( \pmb{z}_n ) + I
            \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
    \end{align}
    Eq.~\eqref{resnet_dCdy} is again a recursive relation that can be used to find all intermediate ``states '' starting from the output layer.
    The gradients wrt.
    weights becomes:
    \begin{align}
        \frac{ \partial C}{\partial W_n^{[i]} }
        =
        \left[ \frac{ \partial \pmb{y}_{n}}{\partial W_n^i } \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        &=
        \left\{
        \frac{ \partial }{\partial W_n^{[i]} }
        \left[
            \phi( W_{n} \bm{y}_{n-1} )  + \textbf{y}_{n-1}
            \right]
        \right\}^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        \label{resnet_dCdW}
    \end{align}
    Since the output of layer $n-1$ doesn't depend on the weights of layer $n$,~\eqref{resnet_dCdW} becomes identical to~\eqref{dCdW} and the same backpropagation algorithm is used to train the network.
    \begin{tikzpicture}
        \node [rectangle, draw, text width=0.8\textwidth, inner sep=10pt] (content) at (0,0) {
            Backpropagation can be seen as a specific form of Automatic Differentiation, a very powerful framework to acquire gradients numerically.
            The governing principle is the same, propagating the gradients through functions with closed form derivative, utilising the chain rule.
        };
    \end{tikzpicture}

    \subsection{Neural ODEs}
    Calculating $\nabla_{\theta}L$ in neural ODEs can be achieved by differentiating through the solver steps but as we will demonstrate there is a much more efficient way, using tools from optimal control theory, namely adjoint sensitivities.
    The governing equations for neural ODEs are Eqs.~\eqref{ivp},~\eqref{adjoint},~\eqref{dldtheta} we repeat them for convenience:

    \begin{equation*}
        \pmb{y}(T) =  \pmb{y}(0) +\int_{0}^{T} f(\pmb{y}(\tau), \tau, \pmb{\theta}) \,d\tau
        , \quad
        \pmb{y}(0) = \pmb{y}_0
    \end{equation*}

    \begin{equation*}
         \pmb{a}^{(1)}(t)
        =
        - \pmb{a}(t)^T
        \frac
        {\partial f( \pmb{y}(t), t, \pmb{\theta} )}
        {\partial \pmb{y} }
        , \quad
        \pmb{a}(T) = \frac{\partial L}{\partial \pmb{y}(T)}
    \end{equation*}

    \begin{equation*}
        \nabla_{\pmb{\theta}} L =
        - \int_T^0
        \pmb{a}(t)^T
        \frac
        {\partial f(\pmb{y}(\tau), \pmb{\theta})}
        {\partial \pmb{\theta}}
        \, d\tau
    \end{equation*}
    Calculating the last integral numerically requires the values of $\pmb{a}(t)$ at all the points in time the solver chooses.
    In the case of a fixed step size solver with step $h$ those would be $t_m = T - mh, \; m=0,1,\dots$.
    In the case of an adaptive step size $t_m$ could be any point between $T$ and $0$.
    In both cases we can calculate those quantities by solving the second ODE starting from time $t=T$ -where $a(T)$ has a closed form solution- and moving backwards in time.
    Next we need the value of $\pmb{y}$ at the same $t_m$s. Again we can calculate them starting from $\pmb{y}(T)$ and solving the original IVP again backwards in time.
    Notice that we don't need to save any intermediate state or activations from the forward pass since we can re-calculate them moving backwards.
    In this sense, neural ODEs are reversible (up to numerical tolerances).

    Algorithm~\ref{alg:adjoint} reformulates the integral equation for the gradient of the loss wrt.
    weights as a differential equation: $ \pmb{a}^{(1)}_\theta(t) = -\pmb{a}(t)^T \frac{\partial f}{\partial \pmb{\theta}}$.
    We know that $\pmb{a}_\theta(T) = 0$ and we search for $\pmb{a}_\theta(0) = \nabla_\theta L$.

    \begin{algorithm}
        \caption{Adjoint}
        \label{alg:adjoint}
        \begin{algorithmic}
            \State Choose ODE solver hyperparameters ($h$, $\dots$)
            \State Do the forward pass, find $\pmb{y}(T)$
            \State $\pmb{a}(T) \gets \frac{\partial L}{\partial \pmb{y}(T)}$ (closed form)
            \State $\pmb{a}_{\theta}(T) \gets 0$
            \State $t \gets T$
            \While{$t > 0$}
                \State $\pmb{y}(t - \Delta t) \gets \text{IntStep}(\pmb{y}(t), f) $
                \State $\pmb{a}(t - \Delta t) \gets \text{IntStep}(
                \pmb{a}(t), -\pmb{a}(t) \frac{\partial f }{\partial \pmb{y}}
                )$
                \State $\pmb{a}_{\theta}(t - \Delta t) \gets \text{IntStep}(
                \pmb{a}_{\theta}(t), -\pmb{a}(t) \frac{\partial f }{\partial \pmb{\theta}}
                )$
                \State $t \gets t - \Delta t$
            \EndWhile
            \State \textbf{return} $\pmb{a}_\theta(0) = \nabla_\theta L$
        \end{algorithmic}
    \end{algorithm}


    \chapter{An alternative derivation for $\frac{dL}{d \pmb{\theta}}$}
    \label{adjoint_proof}
    There are already many ways in the literature for finding this quantity without delving into control theory and reverse sensitivities.
    Intuitively in can be thought as a continuous analogous to classical backpropagation
    \begin{align}
        \text{classical} &\to
        \frac{d L}{ d \pmb{\theta}} =
        \sum_k
        \frac{\partial L}{\partial \pmb{y}_{k}}
        \frac{\partial f(\pmb{y}_k, \pmb\theta)}{\partial \pmb{\theta}}
        \\
        \text{adjoint sensitivities} &\to
        \frac{d L}{ d \pmb{\theta}} =
        \int_T^0
        \frac{\partial L}{\partial \pmb{y}(\tau)}
        \frac{\partial f(\pmb{y}(\tau), \pmb{\theta})}{\partial \pmb{\theta}} \, d\tau
    \end{align}
    In the original neural ODEs paper~\cite{chen2018neural} the authors prove Eq.~\eqref{adjoint} and use an augmented
    state to prove Eq.~\eqref{dldtheta} while others like~\cite{kidger2022neural} provide alternative proofs.
    We present another, in our opinion much simpler, derivation for Eq.~\eqref{dldtheta}.
    Consider the more general case where the loss is dependant on intermediate state on a point $s \in (0,T)$.
    This is equivalent to the usual case where the loss depends only on the output state with $\mathcal{L}(s) = L(\pmb{y}(s))=0$ for $s\neq T$.
    Moreover, the state $\pmb{y}$ is a also dependant on the weights even though it's not explicitly written.
    \begin{align*}
        \frac{ d L(\bm{y}(s, \bm{\theta})) }{ d \bm{\theta} }
        &= \int_0^s \frac{d}{dt} \left( \frac{ d L(\bm{y}(s, \bm{\theta})}{ d \bm{\theta}} \right) dt \\
        &= \int_0^s \frac{d}{dt} \left( \frac{ d L(\bm{y}(t, \bm{\theta})}{ d \bm{y}(t)} \frac{d \bm{y}(t)}{d\bm{\theta}} \right) dt \\
        &= \int_0^s
        \frac{d}{dt} \frac{ d L(\bm{y}(t, \bm{\theta})}{ d \bm{y}(t)} \cdot \frac{d \bm{y}(t)}{d\bm{\theta}}
        +
        \frac{ d L(\bm{y}(t, \bm{\theta})}{ d \bm{y}(t)} \cdot \frac{d}{dt} \frac{d \bm{y}(t)}{d\bm{\theta}}
        dt \\
        &= \int_0^s
        \dot{\bm{a}}(t) \frac{d \bm{y}(t)}{d\bm{\theta}}
        +
        \bm{a}(t) \cdot \frac{d}{d\bm{\theta}} \frac{d \bm{y}(t)}{dt}
        dt \\
        &= \int_0^s
        \dot{\bm{a}}(t) \frac{d \bm{y}(t)}{d\bm{\theta}}
        +
        \bm{a}(t) \cdot \frac{d\bm{f}}{d\theta}
        dt \\
        &= \int_0^s
        \dot{\bm{a}}(t) \frac{d \bm{y}(t)}{d\bm{\theta}}
        +
        \bm{a}(t) \left( \frac{\partial \bm{f}}{\partial \bm{\theta}}
        +
        \frac{\partial \bm{f}}{\partial \bm{y}(t)} \frac{\partial \bm{y}(t)}{\partial \bm{\theta}}\right)
        dt \\
        &= \int_0^s
        \frac{d \bm{y}(t)}{d\bm{\theta}}
        \left( \dot{\bm{a}}(t) + a\frac{\partial \bm{f}}{\partial \bm{y}(t)} \right)
        +
        \bm{a}(t) \frac{\partial \bm{f}}{\partial \bm{\theta}}
        dt \\
    \end{align*}
    From Eq.~\eqref{adjoint} the sum in the parenthesis is 0:
    \begin{align}
        \frac{ d L(\bm{y}(s, \bm{\theta})) }{ d \bm{\theta} }
        = \int_0^s
        \bm{a}(t) \frac{\partial \bm{f}}{\partial \bm{\theta}}
        dt
    \end{align}
    Setting $s=T$ we arrive at the desired formula.


    \chapter{Optimisation}
    In the field of numerical optimisation many methods have been developed for minimising a \textit{scalar} objective function $f$.
    One category of those, \textit{line search} methods, are iterative algorithms that seek to update the current iterate $x_k$ to a new value closer to the minimum.
    They work by choosing a direction $p_k$ -and step size $\alpha$- along which $f$ is decreased.
    Alternatively the problem can be restated as:
    \begin{equation}
        \min_{a>0} f(x_k + \alpha p_k) \label{min_a}
    \end{equation}
    While ideally we would solve~\eqref{min_a} exactly, in practice it is computationally expensive and practically unnecessary.
    Line search implementations usually generate some trial step sizes until they find one that satisfies certain termination conditions we will examine later.

    A \textit{descent direction} is a direction that causes $f$ to decrease along it given sufficiently small $\alpha > 0$: $f(x_k + \alpha p_k) < f(x_k)$.We can show that if $p_k$ is a descent direction then $p_k^T \nabla f_k < 0$.
    From the first order Taylor series expansion we have:
    \begin{equation*}
        f(x_k+ap_k) = f(x_k) + a p_k^T \nabla f_k + O(a^2)< f(x_k) \\
    \end{equation*}
    Ignoring quadratic term since $\alpha$ is small.
    \begin{align*}
        a p_k^T \nabla f_k &< 0 \\
        p_k^T \nabla f_k &< 0
    \end{align*}
    It can be proven though Zoutendijk's theorem that: as long as the search direction is a descent direction and the step size fulfils some conditions the algorithm is globally convergent.

    The defining characteristic of a line search method is the way in which we obtain the search direction $p_k$.
    An obvious choice is to use the direction of \textit{steepest descent}, mathematically obtained as the negative of the gradient at the current iterate.
    \begin{equation}
        x_{k+1} = x_k - \alpha \nabla f_k
    \end{equation}

    In machine learning literature steepest descent, or as they are more commonly refereed, \textit{gradient descent} methods are extremely common.
    Almost all neural network models are trained using some derivative of classic gradient descent.
    These methods are not categorised as line searches since they do not seek to find an appropriate step length $\alpha$ on each iteration.
    They instead define either a fixed or dynamically updated \textit{learning rate}.
    Usually they employ some notions of momentum and stochasticity to move along a direction dictated by an approximation of the local gradient.
    Even so they remain first order methods since they only utilise information about the first derivative.
    Some notable gradient descent methods as well as how the minimisation problem is formulated in the context of machine learning are discussed later.

    \subsection{The Newton-Raphson algorithm with Hessian modification}
    \label{newtonalgo}
    In more general minimisation problems a prevalent search direction for line search optimisers is the \textit{Newton direction}, methods using this direction are called Newton methods.
    This direction is derived from the second-order Taylor series approximation of the objective function, near the current iterate $x_k$.
    Consider a continuous, twice differentiable function $f: \mathbb{R}^n \to \mathbb{R}^n$, its second order Taylor series approximation is:
    \begin{equation}
        f(x_k + p) = f_k + p^t \nabla f_k + \frac{1}{2} p^t \nabla^2 f_k p + R(p) \label{taylor}
    \end{equation}
    with $R(p)$ being of order $O(\rVert p \rVert^3)$.
    For small values of $p$, the residual term diminishes and we have a pretty good quadratic model of $f$.
    Seeking to minimise this model we set the derivative wrt. $p$ equal to 0:
    \begin{align}
        \nabla f_k + \nabla^2 f_k p_t &= 0  \label{grad_m} \\
        p = -\left[ \nabla^2 f_k \right]^{-1} \nabla f_k \label{newton_dir}
    \end{align}
    Eq.~\eqref{newton_dir} gives the definition of the newton direction.
    As mentioned before, in order for the algorithm to be globally convergent the search direction has to be a descent direction.
    By multiplying~\eqref{grad_m} from the left with $p^t$ we get:
    \begin{align}
        p^t \nabla f_k + p_t \nabla^2 f_k p_t =& 0 \\
        p^t \nabla^2 f_k p_t =& -p^t \nabla f_k < 0 \\
        p^t \nabla^2 f_k p_t &> 0 \label{pos_def}
    \end{align}
    From~\eqref{pos_def} it is apparent that the Hessian matrix $H_k=\nabla^2 f_k$ has to be positive definite which is generally true if $x_k$ is near the minimum but not necessarily true away from it.
    If the Hessian is not positive definite there is no guarantee that~\eqref{newton_dir} gives a descent direction or even that the Hessian is non-singular.
    In order to address this issue a positive definite approximation of the Hessian is used.
    The approximation can be obtained in several ways, some involve adding a multiple of the identity or some correction matrix $\Delta H$, others modify the eigenvalues of the true Hessian directly.

    For example in~\cite{cheng1998modified} is shown that, if $H_k$ has spectral decomposition $H_k = Q \Lambda Q^T$ then the correction matrix of minimum Frobenius norm that ensures that the smallest eigenvalue of $H_k + \Delta H$ is larger or equal to $\delta$ is given by:
    \begin{equation*}
        \Delta H = Q \text{diag}(\tau_i) Q^T, \quad \text{with} \quad \tau_i =
        \left\{
        \begin{array}{ll}
            0,                  & \lambda_i \geq \delta, \\
            \delta - \lambda_i, & \lambda_i < \delta,
        \end{array}
        \right.
    \end{equation*}

    Another technique, the one used here, is to perform (or try to perform) a Cholesky decomposition, more specifically an LDL decomposition of the Hessian.
    Since the Hessian is not always positive definite the factorisation $H = LDL^T$ may not exist or if even if it does the algorithm used to compute it is numerically unstable.
    The core idea of modified Cholesky decomposition is to modify the values of the diagonal matrix D to be sufficiently positive while the factorisation is computed.

    More specifically,~\cite{wright2006numerical} provide an algorithm for computing the $LDL^T$ factorisation a positive definite approximation of a matrix.
    It accepts two additional parameters $\delta, \beta$ so that the following bounds are satisfied.
    \begin{equation}
        d_j \geq \delta, \quad \lvert m_{ij} \rvert \leq \beta, \quad i = j+1, j_2, \dots, n
    \end{equation}

    The algorithm calculates the elements of the diagonal $D$ and the unitriangular matrix $L$ column by column.
    It is a simpler version of the algorithm presented in Eq.~\cite{gill2019practical} that additionally introduces symmetric row-column interchanges resulting in lower $ \lVert H_k - \hat{H}_k \rVert$, $\hat{H}_k$ being the approximation.
    \begin{algorithm}
        \label{alg:mod_chol}
        \caption{Modified Cholesky}
        \begin{algorithmic}
            \For{ $i = j+1,\dots,n$ }
                \State $c_{jj} \gets a_{jj} - \sum_{s=1}^{j-1} d_s l_{js}$
                \State $\theta_j \gets \max_{j<i\leq n}(\lvert c_{ij} \rvert)$
                \State $d_j \gets  \max
                \left(
                \lvert c_{jj} \rvert,
                \left( \frac{\theta_j}{\beta} \right)^2,
                \delta \right)$
                \For{$i = j+1,\dots, n$}
                    \State $c_{ij} \gets a_{ij} - \sum_{s=1}^{j-1} d_s l_{is} l_{js}$
                    \State $l_{ij} \gets c_{ij} / d_j$
                \EndFor
            \EndFor
        \end{algorithmic}
    \end{algorithm}
    After addressing the definiteness of the the Hessian let's describe how an algorithm would decide on the step size.
    As noted earlier Zoutendijk's theorem requires the search direction to be a descent direction as well as the step size to fulfil a certain set of conditions.
    One can prove Zoutedijk's theorem for multiple sets of conditions, namely the Wolfe, strong Wolfe and Goldstein conditions.
    We will focus on the strong Wolfe conditions.
    Assume we have chose direction $p_k$ at step $k$ of the algorithm, our second objective is to find step size $a$ to proceed to the next step without minimising $\phi(\alpha) = f(x_k + \alpha p_k)$ explicitly.
    The first strong Wolfe condition states that our guess for $a$ should give \textit{sufficient decrease} in the objective function in the following way:
    \begin{equation}
        f(x_k + ap_k) \leq f(x_k) + c_1 a \nabla f_k ^T p_k
    \end{equation}
    for some $c_1 \in (0,1)$ usually chosen to be quite small ($\approx 10^{-4}$). This ensures the reduction in $f$ is proportional to the step size as well as the directional derivative $\nabla f ^T p_k$.
    The sufficient decrease condition is not enough to make reasonable progress since it's satisfied for values of $\alpha$ close to 0.
    To prevent this, the second strong Wolfe condition or \textit{curvature condition} states that $\alpha$ should also satisfy:
    \begin{equation}
        \lvert \nabla f(x_k + \alpha p_k )^T p_k \rvert \leq \lvert c_2 \nabla f_k^T p_k \rvert
    \end{equation}
    for some constant $c_2 \in (c_1, 1)$ usually set around $0.9$ for Newton direction.
    Notice that the left hand side if the derivative wrt. $\alpha$ of $\phi(a)$ at $a_k$ and the right hand size at $0$.
    Practically the curvature condition requires the slope of $\phi$ at the new point to be greater that at the start times $c_2$.
    Remember that $p_k$ is a descent direction so $\phi(0)$ is strictly negative.
    Larger slope would mean we get closer to a stationary point of zero gradient.
    The absolute ensures that the slope doesn't become too positive and we don't overshoot away from the stationary point.

    Utilising those conditions we construct the following line search algorithm parameterised by $c_1, c_2, \alpha_{max}$

    \begin{algorithm}
        \caption{Line search}
        \label{alg:linesearh}
        \begin{algorithmic}
            \State Set $a_0 \gets 0$, choose $a_1 \in (0, a_{max})$
            \State $i \gets 1$
            \Repeat
                \If {$\phi(a_i) > \phi(0) + c_1 a_i \phi^{(1)}(0)$ or [$\phi(a_i) \geq \phi(a_{i-1})$ and $i>1$]}
                    \State return \textbf{zoom}($a_{i-1}, a_i$)
                \EndIf
                \If {$\lvert \phi^{(1)}(a_i) \rvert \leq -c_2 \phi^{(1)}(0)$}
                    \State return $a_i$
                \EndIf
                \If {$\phi^{(1)}(a_i) \geq 0$}
                    \State return \textbf{zoom}($a_i, a_{i-1}$)
                \EndIf
                \State $a_{i+1} \gets \min({a_max, 2*a_i})$
                \State $i \gets i+1$
            \Until
        \end{algorithmic}
    \end{algorithm}

    The procedure uses the knowledge that the interval $(a_{i-1} , a_{i})$ contains step lengths satisfying the strong Wolfe
    conditions if one of the following three conditions is satisfied:
    \begin{enumerate}
        \item $a_i$ violates the sufficient decreasee condition
        \item $\phi(a_i) \geq \phi(a_{i-1}) $
        \item $\phi^{(1)}(a_i) \geq 0$
    \end{enumerate}
    The last step of the algorithm performs extrapolation to find the next trial value $a_{i+1}$.
    We can use some more involved extrapolation technique like the ones mentioned previously in the section, or
    we can simply set $a_{i+1}$ to some constant multiple of $a_i$.
    Whichever strategy we use, it is
    important that the successive steps increase quickly enough to reach the upper limit $a_{\max}$ in
    a finite number of iterations.

    We know specify the function zoom, which requires a little explanation.
    The order of its input is such that each call has the form: \textbf{zoom}($a_{lo}$, $a_{hi}$), where
    \begin{enumerate}
        \item the interval bounded by $a_{lo}$ and $a_{hi}$ contains step lengths that satisfy the strong Wolfe conditions
        \item $a_{lo}$ is, among all step lengths generated so far and satisfying the sufficient decrease conditions, the one giving the smallest value;
        \item $a_hi$ is chosen so that $\phi{a_{lo}}(a_{hi} - a_{lo}) < 0$
    \end{enumerate}
    Each iteration of zoom generates and iterate $a_j$ between $a_{lo}$ and $a_{hi}$, and then replaces one of these
    endpoints by $a_j$ in such a way that the above properties continue to hold.

    \begin{algorithm}
        \caption{zoom}
        \label{alg:zoom}
        \begin{algorithmic}
            \Repeat
                \State Interpolate to find a trial step length $a_j$ between $a_{lo}$ and $a_{hi}$
                \State Evaluate $\phi(a_j)$
                \If {$\phi(a_j) > \phi(0) + c_1 a_j \phi^{(1)}(0)$ or $\phi(a_j) \geq \phi(a_{lo}$]}
                    \State $a_{hi} \gets a_j$
                \Else
                    \State Evaluate $\phi^{(1)}(a_j)$
                    \If {$ |\phi^{(1)}(a_j)| \leq -c_2 \phi^{(1)}(0)$}
                        \State Set $a^* \gets a_j$ and \textbf{stop}
                    \EndIf
                    \If { $\phi^{(1)}(a_j)(a_{hi} - a_{lo}) \geq 0 $ }
                        \State $a_{hi} \gets a_{lo}$
                    \EndIf
                    \State $a_{lo} \gets a_{hi}$
                \EndIf
            \Until
        \end{algorithmic}
    \end{algorithm}

    If the new estimate $a_j$ happens to satisfy the strong Wolfe conditions, then zoom has served
    its purpose of identifying such a point, so it terminates with $a^* = a_j$.
    Otherwise, if $a_j$ satisfies the sufficient decrease condition and has a lower function value than $a_{lo}$ ,
    then we set $a_{lo} \gets a_j$ to maintain condition (2).
    If by doing so we end up violating condition (3), we set $a_{hi}$ to the old value of $a_{lo}$ ensuring condition
    (3) holds.

    Several implementations of the Newton algorithm with line search exist in scientific software packages.
    We have written an exact Newton routine in \textbf{PyTorch} utilising automatic differentiation and GPU parallelism.

    \subsection{Gradient Descent Methods}
    For the sake of completeness we will present some fist order optimisation methods.
    For really high dimensional problems, such as those we are concerned with in modern deep learning settings, second
    order methods introduce quadratically scaled (on the number of parameters) computation and memory cost.
    This cost is related to the computation and storage of the Hessian (or an approximation of it) which is of size
    ${num_{parameters}}^2$.
    The tradeoff between number of iterations and cost per iteration leans in favor of gradient descent optimizers as
    the number of parameters increases, especially considering the fact that they lend themselves well to automatic
    differentiation and distributing training.
    The loss function for example
    All these in combination with the higher implementation complexity have lead to the dominance of first order
    optimisation techniques in machine learning.

    \subsubsection{Stochastic Gradient Descent}
    Optimising a deep learning model in the strict sense means minimising the expected generalization error
    $\mathbb{E}_{(\pmb{x},\pmb{y}) \sim p_{\text{data}}} L(f(\pmb{x},\pmb\theta),\pmb{y})$ where $p_{data}$ is the joint
    distribution of inputs $\pmb{x}$ and outputs $\pmb{y}$, $\pmb\theta$ are the models parameters and $L$ some per
    sample cost function.
    Since the distribution of data is largely unknown we create an empirical estimate from a set of training examples
    ${\pmb{x}_i,\pmb{y}_i}_{i=1}^{N}$
    \begin{equation*}
        \tilde L(\pmb\theta ) = \frac{1}{N} \sum_{i=1}^N L( f(\pmb{x}_i; \pmb\theta), \pmb{y}_i )
    \end{equation*}
    Minimising such a loss function though traditional gradient descent would require to evaluate the cost for all
    samples in the training set on each iteration to obtain the loss.
    \begin{equation*}
        \pmb\theta^{k+1} = \pmb\theta^{k} + \eta \nabla_{\theta_k} L(\pmb\theta^{k})
    \end{equation*}
    To alleviate some computational cost on each iteration Stochastic Gradient Descent (SGD) is used which instead of calculating
    the loss for all samples in the training set it samples a subset of them and performs the weights updated based
    on that.
    This approach introduces some stochasticity to the gradient approximation which leads to noise increasing the
    convergence rate but can lower the overall computational cost.
    To keep the the algorithm to diverging due to noise a lower learning rate is required to maintain stability
    ~\cite{kiwiel2001convergence}.
    Furthermore, the stochastic nature of SGD can allow it to escape from local minima in the non convex
    high-dimensional loss space that a classic gradient descent scheme would be trapped in.

    To accelerate Stochastic Gradient Descent a number of methods have been proposed some of them in the recent years due
    to the popularity of deep neural networks.

    \subsubsection{ Momentum }
    Based on the idea of physical momentum in real world objects the momentum method was proposed in the context of machine
    learning in~\cite{rumelhart1986learning} borrowing the idea from previous work by Polyak~\cite{polyak1964some}.
    This algorithm considers previous values of the gradient in the update step in a exponentially decaying manner.
    The update along dimensions that keep moving in the same direction keeps getting bigger and for dimensions that
    change direction the update gets smaller.
    Even though it can be applied to both stochastic and classical gradient descent, SGD with a momentum term has been
    the gold standard for many years for non-convex optimisation problems, and presents very good results.
    It converges faster that classical GD due to accumulated velocity while also dampening oscillations in regions of high
    curvatures by combining gradients of opposite signs.
    SGD Momentum can be expressed with the following equation
    \begin{align*}
        \Delta \pmb\theta_{k+1} = \alpha \Delta \pmb\theta_k - \eta \nabla_{\theta_k} L (\pmb\theta_k) \\
        \pmb\theta_{k+1} = \pmb\theta_{k} + \Delta \pmb\theta_{k+1}
    \end{align*}
    where $\alpha$ is an exponential decay factor between $0$ and $1$ that determines the amount of contribution
    previous gradients have to the weights update.

    \subsubsection{Nesterov Momentum}
    A major disadvantage of classical momentum (CM) method is its tendency to overshoot as is approaches the minimum.
    Nesterov momentum seeks to solve this problem drawing from Nesterov's accelerated gradient method for convex functions.
    The equations for Nesterov's momemtum are:
    \begin{align*}
        \Delta \pmb\theta_{k+1} = \alpha \Delta \pmb\theta_k - \eta \nabla_{\theta_k} L(\pmb\theta_{k} + \alpha \Delta \pmb\theta_{k}) \\
        \pmb\theta_{k+1} = \pmb\theta_{k} + \Delta \pmb\theta_{k+1}
    \end{align*}
    This formula resembles CM but instead of calculating the gradient of loss at the current $\pmb\theta$ it is
    calculated after the previous velocity is added.
    This method is based on a corrector-predictor scheme where you first make a big step on the direction of the
    accumulated gradient and then make a correction based on the gradient on the point you land.
    This behaviour helps the method avoid oscillations, such as those that appear in CM around high-curvature regions,
    by pointing the gradients back to $\pmb\theta_k$ more effectively than classical momentum~\cite{kashyap2022survey}.

    \subsubsection{Adagrad}
    The Adagrad algorithm individually adapts the learning rates of all model parameters by scaling them inversely
    proportional to the square root of the the sum of the historical squared values of the gradient.
    \begin{align*}
        \pmb\theta_{k+1} \gets \pmb\theta_{k} - \frac{\eta}{  \sqrt{G_{k}} + \epsilon } \nabla_{\theta_k} L( \pmb\theta_k)
    \end{align*}
    Where $G$ is a matrix containing the squared sum of the past gradients with regards to all $\pmb\theta_k$ along its
    diagonal and $\epsilon$ is a small quantity to avoid dividing by zero.
    For weight parameter with the largest gradient has the largest decrease in its learning rate, while the parameter
    with the smallest gradient has the smallest decrease in its learning rate.
    This results in faster progress in the more gentlysloped regions of parameter space and works well when the
    gradient is sparse~\cite{duchi2011adaptive}.

    \subsubsection{RMSProp}
    The RMSProp algorithm is famous for being an unpublished but well known optimisation algorithm.
    It was introduced by Hinton in one of his courses.
    RMSprop modifies Adagrad so that instead of accumulating the square of the gradients for each parameter, which can
    cause the learning rate to diminish uses an exponentially decaying moving average of the squared gradients to
    discard the past history.
    \begin{align*}
        G_{k+1} = \beta G_{k} + (1 - \beta) ( \nabla_{\theta_k} L (\pmb\theta_k) )^2\\
        \pmb\theta_{k+1} = \pmb\theta_{k} - \frac{\eta}{  \sqrt{G_{k}} + \epsilon } \nabla_{\theta_k} L( \pmb\theta_k)
    \end{align*}

    \subsubsection{Adam}
    One of the most widely used optimisation algorithms used in deep learning today is Adam, which stands for
    adaptive moments~\cite{kingma2014adam}.
    It combines elements of momentum and RMSprop with some modifications.
    Firstly, we incorporate momentum by computing the first order moment of the gradient as exponentially decaying
    moving average.
    Secondly, we calculate the second order moment as the gradient, again as an exponentially decaying average.
    The parameter update rule is given as:
    \begin{align*}
        \pmb{v}_k = \alpha \pmb{v}_{k-1} + (1 - \alpha) \nabla_{\theta_k} L( \pmb\theta_{k} ) \\
        \pmb{s}_k = \beta \pmb{s}_{k-1} + (1 - \beta) \nabla_{\theta_k} L( \pmb\theta_{k} ) \\
        \pmb\theta_{k+1} = \pmb\theta_{k} - \eta \frac{\hat{\pmb{v}}_k}{  \sqrt{ \hat{\pmb{s}}_k + \epsilon }} \nabla_{\theta_k} L( \pmb\theta_k)
    \end{align*}
    In the above equations $\hat{\pmb{s}}_k$, $\hat{\pmb{v}}_k$ are the bias corrected estimates of the moments to account
    for their initialization.
    Since RMSProp lacks the correction factor, it has a high-bias in the early stages of training, while Adam is
    robust to the choice of hyperparameters.


\end{document}


