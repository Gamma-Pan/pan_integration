\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{cancel}
\usepackage{algorithm}
\usepackage[noEnd=false,indLines=false]{algpseudocodex}
\usepackage[left=3.5cm, right=3.5cm]{geometry}
\usepackage{tikz}
\usetikzlibrary{shapes,positioning,backgrounds, calc}
\usepackage{pgfplots}
\pgfplotsset{compat=newest }
\usepgfplotslibrary{external}
\tikzexternalize
\usepackage[style=ieee]{biblatex} %Imports biblatex package
\addbibresource{references.bib} %Import the bibliography file

\title{Polynomial approximation integration  \\ \large{Speeding up Neural Ordinary Differential Equations thought Parallelism} \\ v0.8}

\author{Yiorgos Panagiotopoulos}
\date{April 2024}

\begin{document}

    \maketitle

    \abstract
    We present a short introduction to neural ordinary differential equations and how they emerge as continuous time
    generalizations of residual neural networks.
    One of the main bottlenecks of neural ODEs has been their speed both on training and inference.
    The numerical solver used for the forward and backward pass of the network can greatly impact its performance both
    in terms of speed and accuracy.
    We develop a new numerical ODE solver based on refining a polynomial approximation of the solution and explore its
    impact on this framework.
    At the end we showcase how our method compares to some commonly used numerical solvers.

    \section{Introduction}
    The machine learning scene in 2024 is dominated by gargantuan models with billions of parameters.
    Even though they have demonstrated impressive results the compute and energy requirements of such models are a
    concern.
    It's the authors opinion that we ought to transition to analog or hybrid models of computation to reduce energy
    usage and increase speed.
    Even though digital computers are not going away any time soon studying alternative learning frameworks could ease
    the transition.

    Neural differential equations re-emerged in the recent years after~\cite{chen2018neural}.
    We believe they have great potential especially in modelling physical phenomena governed by differential equations,
    irregularly sampled time series and generative models like continuous normalising flows and stochastic differential
    equations.
    More importantly, in out opinion, they highlight a connection between deep learning and dynamical systems.
    Even thought we still operate in the discrete domain to use them in our current systems they offer an opportunity
    to close the gap between continuous time models and machine learning.

    We provide a quick overview of Neural ODES and propose a novel numerical integration method to tackle some of
    them limitations.


    \section{Residual Networks}
    In order to understand the intuition behind neural ordinary differential equations one should have a basic knowledge
    of residual networks.
    Residual networks, also known as ResNets, were introduced by He and others in their seminal
    paper~\cite{He_2016_CVPR}, to address the problem of \textit{degradation} in very deep architectures.
    It had become apparent by state of the art models of that time like~\cite{simonyan2014very} that depth plays a very
    important role in vision tasks, including classification.
    Stacking layers (depth) allows for the integration of low/mid/high level features in the learning process.
    The immediate obstacle in deep learning is the notorious vanishing gradient problem, but this has been largely
    solved through batch/group normalisation~\cite{ioffe2015batch}.

    It had been noticed though~\cite{srivastava2015highway}, that while stacking more layers, accuracy is initially saturated, unsurprisingly, but then it degrades rapidly.
    This degradation phenomenon is not caused by overfitting as the training error increases with the number of layers.
    ResNets solved this problem by utilising a residual learning framework.

    \subsection{Residual Learning}
    One can prove, by construction, that a deep architecture can be as accurate as a corresponding shallower one.
    This is achieved by adding identity layers in-between the layers of the shallower model.
    In more detail, consider a network that converges with some accuracy.
    Adding more layers to it would achieve the exact same accuracy if the extra layers learned to map their inputs straight to their outputs effectively making them identity mappings.
    The existence of this artificially deep network suggests that deep networks shouldn't produce higher error that the shallower ones.
    But the empirical evidence show that optimisers can't find this solution, at least in sensible time limits.

    \begin{figure}
        \begin{center}
            \begin{tikzpicture}[
                every path/.style={ultra thick},
                scale=3,
                block/.style={
                    rectangle,
                    rounded corners,
                    thick,
                    minimum size = 1.5cm,
                    inner sep = 0pt,
                    node distance = 3.5cm,
                    draw=black!80
                },
                shallow/.style={
                    block,
                },
                deep/.style={
                    block,
                    dashed,
                    thick
                },
                darkgreen/.style={
                    green!80!black!90
                },
                >= stealth,
            ]

                \begin{scope}[local bounding box = net1]
                    \node[shallow] (A) at (0,0) {};
                    \node[below right=0cm of A.north west] {$f_{k-1}(x)$};

                    \node[shallow] (B) [right= of A] {};
                    \node[below right=0cm of B.north west] {$f_{k}(x)$};

                    \node[shallow] (C) [right= of B] {};
                    \node[below right=0cm of C.north west] {$f_{k+1}(x)$};

                    \draw[->] (A.west)+(-0.1cm,0) -- (A.west);
                    \draw[->] (A.east) -- (B.west);
                    \draw[->] (B.east) -- (C.west);
                    \draw[dashed] (C.east) -- +(0.1cm,0);

                    \node[above] at (current bounding box.north) {A section of the starting neural net.};

                \end{scope}

                \begin{scope}[local bounding box = scope1, yshift = -0.8cm]
                    \node[shallow] (A) at (0,0) {};
                    \node[below right=0cm of A.north west] {$f_{k-1}(x)$};

                    \node[shallow] (B) [right= of A] {};
                    \node[below right=0cm of B.north west] {$f_{k}(x)$};

                    \node[shallow] (C) [right= of B] {};
                    \node[below right=0cm of C.north west] {$f_{k+1}(x)$};

                    \node[deep] (AB) at ($(A)!0.5!(B)$) {};
                    \node[below right=0cm of AB.north west] {$\bar{f}_{k-1}(x)$};

                    \node[deep] (BC) at ($(B)!0.5!(C)$) {};
                    \node[below right=0cm of BC.north west] {$\bar{f}_{k}(x)$};

                    \draw[->] (A.west)+(-0.1cm,0) -- (A.west);
                    \draw[->] (A.east) -- (AB.west);
                    \draw[->] (AB.east) -- (B.west);
                    \draw[->] (B.east) -- (BC.west);
                    \draw[->] (BC.east) -- (C.west);
                    \draw[dashed] (C.east) -- +(0.1cm,0);

                    \node[above] at (scope1.north) {Increase its depth by adding more layers.};
                \end{scope}

                \begin{scope}[local bounding box = scope2,yshift=-1.8cm]
                    \node[shallow] (A) at (0,0) {};
                    \node[below right=0cm of A.north west] {$f_{k-1}(x)$};

                    \node[shallow] (B) [right= of A] {};
                    \node[below right=0cm of B.north west] {$f_{k}(x)$};

                    \node[shallow] (C) [right= of B] {};
                    \node[below right=0cm of C.north west] {$f_{k+1}(x)$};

                    \node[deep,darkgreen] (AB) at ($(A)!0.5!(B)$) {};
                    \node[below right=0cm of AB.north west] {$\bar f_{k-1}(x)$};
                    \node[above left=0cm of AB.south east, darkgreen] {$\mathbf I$};

                    \node[deep,darkgreen] (BC) at ($(B)!0.5!(C)$) {};
                    \node[below right=0cm of BC.north west] {$\bar f_{k}(x)$};
                    \node[above left=0cm of BC.south east, darkgreen] {$ \mathbf I$};

                    \draw[->] (A.west)+(-0.1cm,0) -- (A.west);
                    \draw (A.east) -- (AB.west);
                    \draw[darkgreen] (AB.west) -- (AB.east);
                    \draw (AB.east) -- (B.west);
                    \draw[->] (AB.east) -- (B.west);
                    \draw (B.east) -- (BC.west);
                    \draw[darkgreen] (BC.west) -- (BC.east);
                    \draw[->] (BC.east) -- (C.west);
                    \draw[dashed] (C.east) -- +(0.1cm,0);

                    \node[above,align=left] at (scope2.north) {If the extra layers learned to be identity mappings the\\
                    deeper network would function identically to the original.};
                \end{scope}

                \begin{scope}[local bounding box = scope3,yshift=-3.1cm]
                    ;
                    \node[shallow] (A) at (0,0) {};
                    \node[below right=0cm of A.north west] {$f_{k-1}(x)$};

                    \node[shallow] (B) [right= of A] {};
                    \node[below right=0cm of B.north west] {$f_{k}(x)$};

                    \node[shallow] (C) [right= of B] {};
                    \node[below right=0cm of C.north west] {$f_{k+1}(x)$};

                    \node[deep, yshift=1cm] (AB) at ($(A)!0.5!(B)$) {};
                    \node[below right=0cm of AB.north west] {$\bar f_{k-1}(x)$};
                    \node[above left=0cm of AB.south east] {};

                    \node[deep, yshift=1cm] (BC) at ($(B)!0.5!(C)$) {};
                    \node[below right=0cm of BC.north west] {$\bar f_{k}(x)$};
                    \node[above left=0cm of BC.south east] {};

                    \draw[->] (A.west)+(-0.1cm,0) -- (A.west);
                    \draw[->, rounded corners] (A.east) +(0.1cm,0) -- ($(AB.west) - (0.1cm,0)$)  -- (AB.west);
                    \draw[->, rounded corners] (AB.east) -- ++(0.1cm,0) -- ($(B.west) + (-0.15cm,0)$);
                    \draw[->, rounded corners] (B.east) +(0.1cm,0) -- ($(BC.west) - (0.1cm,0)$)  -- (BC.west);
                    \draw[->, rounded corners] (BC.east) -- ++(0.1cm,0) -- ($(C.west) + (-0.15cm,0)$);
                    \draw[dashed] (C.east) -- +(0.1cm,0);

                    \draw[->,darkgreen, rounded corners] (B.west) +(-0.3cm,0) -- ++(0.05cm, -0.37cm) -- ++(0.42cm, 0cm) -- ($(B.east) + (0.25cm,0)$);
                    \draw[->,darkgreen] (A.east) -- (B.west);
                    \draw[->,darkgreen] (B.east) -- (C.west);

                    \node[above,align=left] at (scope3.north) {Shortcut connections precondition the deep network \\ to identity mappings and train the residual };
                \end{scope}
            \end{tikzpicture}
        \end{center}
        \caption{A visual explenation of residual learninig. Information flows through the shortcut connections as
        if they were identity mappings. The network learns the residual instead of a direct mapping.
        Residual neural networks use this property to achieve very large depth. }
        \label{fig:res_expl}
    \end{figure}

    In order to incorporate this observation into the network, so called \textit{shortcut connections} are introduced to the layers.
    Basically, the input to a block of stacked layers is added back to it's output.
    This way the network instead of learning the direct mapping $\mathcal{H}(x) = \mathcal{F}(x)$, it learns the residual mapping $\mathcal{H}(x) = \mathcal{R}(x)+x$.
    The optimiser can then push the residual mappings to $0$ leading to identity connections.
    Even though identity mappings are unlikely to be optimum, experiments showed that the residual connections have generally small responses, meaning identities are good pre-conditioners for deep architectures.

    Since their conception Residual Networks have revolutionised deep learning allowing for much deeper architectures than previously used.
    Furthermore, the idea of residual learning has found vast appeal in many other architectures.
    Transformers for example, employ residual connections, allowing many such layers to be stacked creating large expressive models~\cite{vaswani2017attention}.


    \section{Neural ODEs}
    There exist examples in the literature of combining neural networks and dynamical systems, even since the 1990s~\cite{rico1992discrete}.
    Interest began to increase after the publication of ResNet~\cite{weinan2018mean}.
    But the field of Neural Differential Equations really became prominent with~\cite{chen2018neural}.

    \subsection{A continuous time network}
    Let's revisit Residual Networks and how they work.
    The input-output relationship of the $k$-th ResNet block is given by:
    \begin{equation}
        \pmb{y}_{k+1} = \pmb{y}_{k} + f(\pmb{y}_{k}; \pmb{\theta}_k) \label{resnet}
    \end{equation}
    where $\pmb{y}_{k+1} \in \mathbb{R}^{N_y}$ is the output of the block, $\pmb{y}^{k} \in \mathbb{R}^{N_y}$ is its input and $f$ is a fully connected layer with parameters $\pmb{\theta}_k \in \mathbb{R}^{N_\theta}$.
    Notice that we refer to ResNet blocks, not layers, since residual connections are generally not between the input and output of a single linear layer but a stack of them; from now on we will use the terms interchangeably.

    The update in the \textit{hidden state} $\pmb{y}$ in~\eqref{resnet} resembles the formula of the forward Euler iteration for solving ordinary differential equations.
    This observation led the authors of~\cite{lu2018beyond} [CITE MORE] to make a connection between some neural network architectures and differential equations.
    Suppose that we could keep increasing the depth of the ResNet in~\eqref{resnet} while the time of the forward pass remained bounded, meaning each layer would take less and less time.
    In the limit we get a differential equation governed by the dynamics defined by a neural network $f$.
    \begin{equation}
        \frac{ \pmb{y}(t)}{dt} = f(\pmb{y}(t); \pmb{\theta}) \label{diffeq}
    \end{equation}
    Instead of a discrete sequence of hidden states, $\pmb{y}(t)$ is a continuous flow $\pmb{y} : [0,T] \to \mathbb{R}^{N_y}$ defined be the vector field parameterised by $\pmb{\theta}$.

    One important issue we haven't addressed is that the weights in~\eqref{resnet} change at each layer but at~\eqref{diffeq} they are the same for the entire trajectory.
    For notationâ€™s sake we could define $f$ as $f(\pmb{y}(t), t, \pmb{\theta})$ meaning at certain ``depths'' different sections of the weights vector $\pmb{\theta}$ are used, maybe in a piecewise constant fashion~\cite{kidger2022neural}.
    Alternatively, the weights could be time dependant which complicates the model but leads to \textit{Galerkin} Neural ODEs, a more suitable continuous time equivalent of ResNet~\cite{massaroli2020dissecting}.
    For our purposes we consider the weights constant for the rest of this paper.

    \subsection{Inference}
    Under this new machine learning framework depth is replaced by time.
    Inference is performed by solving an initial value problem from initial time $t=0$ to terminal time $t=T$.
    \begin{equation}
        \pmb{y}(T) = \pmb{y}(0) + \int_{0}^{T} f(\pmb{y}(\tau); \pmb{\theta}) \, d\tau \label{ivp}
    \end{equation}

    Obviously,~\eqref{ivp} is a continuous time problem while our computers work in discrete time \textit{(for now)}.
    In order to solve it we employ a numerical method for solving \textit{Initial Value Problems}, .
    If we were to use forward Euler, an explicit method, we would get a recurrence relation of the form
    \begin{equation}
        \pmb{y}(t_k + h) = \pmb{y}(t_k) + h \left. \frac{d\pmb{y}(t)}{dt} \right|_{t = t_k}
        \label{forwaredEu}
    \end{equation}
    By considering that $\frac{d \pmb{y}(t)}{dt} = f(\pmb{y}(t);\pmb{\theta})$ eq. \eqref{forwaredEu} can be written as:
    \begin{equation}
        \pmb{y}(t_k + h) = \pmb{y}(t_k) + h f(\pmb{y}(t_k); \pmb{\theta} )
        \label{res2eul}
    \end{equation}
    which coincides with the ResNet formula.
    Many other neural network architectures can be interpreted as solving a neural ODE using different methods~\cite{chen2019ordinary}.
    The field of numerical differential equation solvers is quite vast meaning one could choose one that fits his specifications.
    A notable case is that of adaptive solvers like Runge-Kutta-Fehlberg (RKF45), that use varying step sizes.
    This way to obtain  $\pmb{y}(T)$ the solver may require different number of $f$ evaluations depending on the input and the complexity of the vector field at that input.
    For simple dynamics the solver can ``decide'' to use large step sizes meaning fewer function evaluations.
    Otherwise when the the vector field is more complex, a smaller step size will be used -to reduce error- leading to more function evaluations.
    We could interpret this behaviour as a network of ``variable depth''.

    It's important to clarify here that there are effectively two networks at play.
    Firstly, there is $f$, inherited from a block or layer of the original formulation of the problem.
    It is a neural network in the classical sense comprised of matrix vector multiplications and non-linearities.
    Secondly, there is the outlining model, the neural differential equation, which receives input $\pmb{y}_0$ and produced output $\pmb{y}(T)$; $f$ is to neural ODE what a layer is to ResNet.
    Internally, depending on the solver, $f$ will be evaluated for many values of $\pmb{y}(t)$.
    As disguised above, in contrast to ResNet $f$ contains all the learnable parameters of the model, which in out case are independent of depth.
    In conclusion the Neural ODE consists of: a neural network $f$ -that defines a \textit{learnable} vector field- parameterised by $\pmb{\theta}$,  some input $y_0$ and a numerical solver that applies $f$ until it reaches $\pmb{y}(T)$.

    \begin{figure}[h]
        \begin{center}
            \input{resnet_to_node_plot.pgf}
        \end{center}
        \caption{ResNets can be considered as discretizations of an ODE solution. Under this assumption as Resnet
            defines a continuous time vector field.}
        \label{fig:resnet_to_node}
    \end{figure}

    \subsection{Training}
    Optimising any model through gradient descent, in a supervised learning setting, requires acquiring the gradient of some loss function $L(\pmb{\theta})$ with respect to (wrt.) the model's parameters $\pmb{\theta}$.
    Usually, the loss for each sample in the dataset is calculated using a cost, or distance, function between the output of the network and the desired or ideal output.
    Sometimes, the loss function may be dependent on the state on multiple times.
    But for simplicity's sake we will examine the case where it only depends on the state at time $t=T$, which as is is apparent, depends indirectly on parameters $\theta$.
    \begin{equation}
        L(\pmb{y}(T)) = L \left( \pmb{y}(0) +\int_{0}^{T} f(\pmb{y}(\tau), \tau, \pmb{\theta}) \,d\tau \right) \label{ode_loss}
    \end{equation}
    One could naively solve the initial value problem using any numerical ODE solver and try to backpropagate the gradients through the steps of the solver.
    This approach may be possible with the help of modern automatic differentiation software but its highly memory inefficient [CITATION NEEDED]. In order to overcome this problem we are going to follow a different approach.
    To this end let's first define the gradient of $L$ wrt.
    the hidden state at some time $t$.
    \begin{equation}
        \pmb a(t) = \frac{\partial L}{\partial \pmb{y}(t) }
    \end{equation}
    The above quantity is called the \textit{adjoint state}.
    It can be proven that for its derivative [PONTRYAGIN], the following equation holds:
    \begin{equation}
        \left.
        \frac
        {d \pmb{a}(t)}
        {dt}
        \right|_{t=t_k}
        =
        - \pmb{a}(t)^T
        \left.
        \frac
        {\partial f( \pmb{y}(t), t, \pmb{\theta} )}
        {\partial \pmb{y}(t) }
        \right|_{t=t_k}
        \label{adjoint}
    \end{equation}

    Notice that we can easily compute $\pmb{a}(T)$ as we know the form of the loss function eg.
    mean square error loss.
    Having this initial condition $\pmb{a}(T)$ we can solve the differential equation~\eqref{adjoint} backwards in time to find $\pmb{a}(t)$ for any $t$.
    As we solve the equation, values of $\pmb{y}(t)$ may be required.
    We can get them by solving the initial differential equation~\eqref{ivp} starting from $\pmb{y}(t)$ and again moving from time $T$ to time $0$.

    Our goal is to calculate $\frac{dL}{d\pmb{\theta}}$ which can be done by solving another differential equation or equivalently by evaluating the following integral:
    \begin{equation}
        \frac{dL}{d\pmb{\theta}}
        = \int_T^0
        \pmb{a}(t)^T
        \frac
        {\partial f(\pmb{y}(\tau), \pmb{\theta})}
        {\partial \theta}
        \, d\tau
        \label{dldtheta}
    \end{equation}
    All these results stem from optimal control theory and more specifically consist specialised cases of Pontryagin's Maximum Principle.
    A discussion on how to prove~\eqref{adjoint} and~\eqref{dldtheta} in a simpler way is made on appendix~\ref{adjoint_proof}.

    \subsection{Applications, benefits and limitations}
    There are examples in the literature where researchers have used Neural ODEs to physical problems as the continuous time dynamics fit such problems well.
    Recently the authors of~\cite{chiu2023exploiting} showed Neural CDEs, a variant of neural ODEs, lend themselves well to video modelling tasks, with performance comparable to classical neural nets.
    Other applications include modelling for irregularly sampled time series and continuous normalising flows.
    More generally Neural ODEs could be seen as a drop in replacement for ResNets.

    Despite the scientific interest in the last years continuous time models have mostly remained in the lab.
    One major cause for this is the speed of inference and training.
    Neural ODEs typically require more function evaluations than classical models.
    Furthermore, one subtle limitation is that as the vector fields becomes more and more complex to fit the data the harder and more expensive it gets to solve them numerically.
    For those reasons Neural ODEs tend to become computationally intractable for large datasets.
    Work is been done though to tackle these problems like hypersolvers~\cite{poli2020hypersolvers} and algebraically reversible solvers~\cite{kidger2021efficient}~\cite{zhuang2021mali}.
    Another way to speed up continuous time networks is to speed up the numerical solver through parallelism.


    \section{Parallelism in Time}
    It is apparent that Moore's law is crumbling.
    In the last decades focus has shifted to parallel computation to increase performance rather than increasing single
    threaded frequencies [CITATION?].
    In the field of deep learning, massive parallelisation of training and inference workloads have led to major
    breakthroughs like~\cite{vaswani2017attention}.
    Differential equations solvers have intrinsic serial characteristics.
    Each iteration of a time stepping algorithm depends on the previous steps.
    Nevertheless, after the seminal work presented in~\cite{nievergelt1964parallel}, several methods have been proposed
    for parallel in time, numerical integration.
    Recent advances include the parareal algorithm~\cite{maday2002parareal} and PFASST~\cite{emmett2012toward} algorithm.
    Massaroli et al. \cite{massaroli2021differentiable} used principles of multiple shooting layers,
    a subcategory of parallel in time methods, and root finding algorithms, to accelerate neural ODES.
    In the next subsection a new parallel in time algorithm based on refining a polynomial approximation of the solution
    using a Newton-Raphson iteration is proposed.

    \subsection{Polynomial approximation}
    Let us consider the following initial value problem that we wish to solve in some time interval
    $t \in [0,T]$ with $\pmb{y} (t_0)= \pmb{y}_0$.
    We will examine the scalar case first where $y : \mathbb{R} \to \mathbb{R}$:
    \begin{equation}
        \frac{d y(t)}{dt} = f(y(t)) \label{dydt}
    \end{equation}
    We define a polynomial approximation  $\hat{y}(t)=\sum_{n=0}^{N-1} \phi_n(t) b_n$   where $ \phi_n(t) $ are basis functions
    for a vector space of polynomials and $b_n$ are coefficients we wish to find.
    We can also express $\hat{y}$ as the dot product $\hat{y}(t)= \left[ \pmb{\phi}(t)\right]^T \textbf b$ where
    $\pmb{\phi} : [0,T] \to \mathbb{R}^N$ and $\pmb{b} \in \mathbb{R}^N$.

    By substituting $\hat{y}$ in~\eqref{dydt} we get:
    \begin{align}
        \frac{d}{dt}  \pmb{\phi}^T(t)  \pmb{b} &\approx f( \pmb{\phi}^T(t)  \pmb{b} ) \label{dPhidt}
        \\
        \pmb{\dot{\phi}}^T(t)  \, \pmb{b} &\approx f(  \pmb{\phi}(t)  \pmb{b} ) \label{dphi_fphi}
    \end{align}
    We define an approximation error (in the mean square error sense) that we wish to minimise in the interval of interest.
    \begin{equation}
        e(\pmb{b}) = \int_0^T
        \left(
        \pmb{\dot{\phi}}(\tau)  \, \pmb{b} - f(  \pmb{\phi}^T(\tau)  \pmb{b} )
        \right)^2
        d\tau
    \end{equation}

    Depending on the choice of basis functions, the derivative of $\pmb{\phi}(t)$ with respect to time can be trivial
    to obtain analytically.
    We opt to use Chebyshev polynomials as the basis functions of our approximation which are known for their
    properties in approximation theory [CITATION].
    They form an orthonormal basis for functions in $[-1,1]$ but we can use them in our desired interval through a
    simple change of variables while being mindful of the differential in $\frac{d}{dt} \pmb{\phi}(t) $.
    Chebyshev polynomials of the first kind $T_n(t)$ can be defined by:
    \begin{equation}
        T_n(\cos \theta) = \cos(n \theta)
    \end{equation}
    While Chebyshev polynomials of the second kind can be defined by:
    \begin{equation}
        U_n(\cos \theta) \sin \theta = \sin( (n+1)\theta )
    \end{equation}
    Alternatively, they can be obtained through the recurrence relationships:
    \begin{equation}
        \label{eq:cheb_rec}
        \begin{aligned}[c]
            T_0(t) &= 1 \\
            T_1(t) &= t \\
            T_n(t) &= 2tT_{n-1}(t) - T_{n-1}(t)
        \end{aligned}
        \qquad \quad
        \begin{aligned}[c]
            U_0(t) &= 1 \\
            U_1(t) &= 2t\\
            U_n(t) &= 2tU_{n-1}(t) - U_{n-1}(t)\\
        \end{aligned}
    \end{equation}
    It can also be shown that for the derivative of $T_n$ it holds that:
    \begin{equation}
        \frac{dT_n}{dt} = n U_{n-1}
        \label{Dcheb}
    \end{equation}
    Since we operate in discrete time we define a grid of $M$ points $\pmb{t} = [t_0 \; t_1 \; \dots \; t_{M-1}]^T$
    where we calculate the error.
    The points can be equidistant or another set of points between $0$ and $T$ could be used.
    For our calculations we use the Chebyshev node of the second kind $t_m = \cos\left( \frac{m\pi}{M-1} \right), \ m=0,\dots,M-1$.
    For simplicity's sake let's assume that the time variable has been rescaled to be in $[-1,1]$.
    In this case $\pmb{\phi}^T(\pmb{t}) $ would be written as a $M \times N$ matrix.
    \begin{equation}
        \pmb{\phi}^T(\pmb{t}) =  \Phi^T  =
        \begin{bmatrix}
            T_0(t_0)     & T_1(t_0)     & \dots & T_{N-1}(t_{0})   \\
            T_0(t_1)     & T_1(t_1)     & \dots & T_{N-1}(t_1)     \\
            & & \vdots \\
            T_0(t_{M-1}) & T_1(t_{M-1}) & \dots & T_{N-1}(t_{M-1}) \\
        \end{bmatrix}\label{eq:cheb_mat}
    \end{equation}
    Similarly, the derivative of $\Phi$ at the defined grid of points is found be differentiating each element of~\eqref{eq:cheb_mat} wrt.
    the grid points and by utilising~\eqref{Dcheb}.
    \begin{equation}
        \label{PHI}
        \dot{\Phi}^T  =
        \begin{bmatrix}
            0 & 1 \cdot U_0(t_0)     & \dots & (N-1) \cdot U_{N-2}(t_{0})   \\
            0 & 1 \cdot U_0(t_1)     & \dots & (N-1) \cdot U_{N-2}(t_1)     \\
            & & \vdots \\
            0 & 1 \cdot U_0(t_{M-1}) & \dots & (N-1) \cdot U_{N-2}(t_{M-1}) \\
        \end{bmatrix}
    \end{equation}
    Generalising to a vector valued $\pmb{y} : [0,T] \to \mathbb{R}^L$ the collection of coefficients becomes a matrix $B\in \mathbb{R}^{N \times L}$.
    Another way of thinking about this is stacking $L$ coefficient vectors, one for each dimension next to each other, forming the coefficient matrix $B$.
    The error function is then reformulated as
    \begin{align}
        e(B) &= \sum_t \left(
        \pmb{\dot\phi}^T(t)  B -
        f (  \pmb\phi^T(t) B  )
        \right)^2
        \label{approx_error}
    \end{align}
    We therefore seek to find coefficients $B^*$ that minimise the above error.
    To find the minimum we set the derivative wrt. $B$ equal to zero and solve for it.
    \begin{align}
        \nabla_B e(B) = 0
    \end{align}
    Depending on the form of $f$ it can be hard -or even impossible- to find a closed form solution to this problem.
    We replace $f(  \pmb\phi^T(t) B  )$ with its zero-th order approximation around an initial point $B^{[k]}$.
    \begin{align}
        \nabla_B
        \sum_t \left(
        \pmb{\dot\phi}^T(t))  B -
        f( \pmb\phi^T(t) (B^{[k]} ) +
        r(B^{[k]})
        \right)^2 &=0
        \\
        \sum_t
        \pmb{\dot\phi}(t) +
        \left(
        \pmb{\dot\phi}^T(t)  B -
        f( \pmb\phi^T(t) B^{[k]} )
        \right) &\approx 0
        \\
        \Phi \; \Phi^T B - \Phi f (  \Phi^T B^{[k]} )
        &\approx 0
        \label{eq:approx_error2}
    \end{align}
    Solving for $B$ in the above equation and using the solution for the new zero-th order approximation of $f$ we arrive
    to the iterative solution:
    \begin{equation}
        \label{eq:Bitaration}
        B^{[k]} = \left[  \dot\Phi \; \dot\Phi^T \right]^{-1}  \dot\Phi f (  \pmb\phi^T(t)B^{[k-1]} ), \quad k=0,1,\dots
    \end{equation}

    \begin{figure}[h]
        \begin{center}
            \input{sol_regression.pgf}
        \end{center}
        \caption{ Regression of the approximate solutions to the true solution. As the coefficients go to the
        optimum and the error function approaches the minimum the trajectories are getting closer to the true solution.}
        \label{fig:reg_sol}
    \end{figure}

    It's apparent that the accuracy of this iterative solution depends on the residual $r(B^{[k]})$ of the zero order
    approximation on ~\eqref{eq:approx_error2}.
    Depending on the condition number of $f$ the accuracy of the solution when the method converges may not be
    sufficient.
    To combat that this, we can use the solution matrix $\bar B$ as the starting solution of a higher order method to
    increase accuracy.
    A simple choice is the Newton-Raphson algorithm with Hessian modification, a second order method.
    A version of the algorithm is showcased in Appendix~\ref{newtonalgo}, where line-search is used for the learning
    rate or step size of the update ~\ref{alg:linesearh}.
    Combining polynomial approximation with the Newton algorithm we arrive at algorithm \ref{alg:intapprox}.
    \begin{algorithm}
        \caption{Polynomial approximation numerical integration}
        \begin{algorithmic}
            \State choose $\epsilon_{ls}$, $\epsilon_{n}$, $B^{[0]}$, $K_{ls}, K_{n}$
            \State $\pmb{t} \gets [t_0 \; t_1 \; t_2 \; \dots \; t_{M-1} ]^T$
            \State $k \gets 1$
            \State calculate $\Phi, \Phi'$ at $\pmb{t}$
            \Repeat
                \State $B^{[k]} \gets \left[  \dot\Phi \; \dot\Phi^T \right]^{-1}  \dot\Phi f (  \pmb\phi^T(t)B^{[k-1]} )$
                \If { $\lVert B^{[k]} - B^{[k-1]}\rVert < \epsilon_{ls}$ }
                    \State \textbf{break}
                \EndIf
                \State $k \gets k +1$
            \Until{$k=K_{ls}$}
            \State $B^{[1]} \gets B^{[k]}$
            \State $k \gets 1$
            \Repeat
                \State calculate $e(B^{[k]})$
                \If { $e(B^{[k]})< \epsilon_{n}$ }
                    \State \textbf{return} $B^{[k]}$
                \EndIf
                \State $ \pmb{d}^{[k]} \gets -\left[ \nabla^2 e(B^{[k]}) \right]^{-1} \nabla e(B^{[k]})$
                \State $ \alpha^{[k]} \gets \text{lineseach}(e, B^{[k]}, \pmb{d}^{[k]}) $
                \State $ B^{[k+1]} \gets B^{[k]} + \alpha \pmb{d}^{[k]}$
                \State $k \gets k+1$
            \Until{$k>k_{max2}$}
            \label{alg:intapprox}
        \end{algorithmic}
    \end{algorithm}

    \subsection{Realisation details}

    \subsubsection{Imposing initial conditions}
    Since the value of $y(t)$ and $f(y(t))$ are known at $t=0$ we can calculate a part of $B$ analytically which
    allows to both: impose our initial conditions and to reduce the computational cost.
    Starting with initial state:
    \begin{align}
        \label{eq:phi_y_init}
        \pmb{\phi}(t_0) B &= \pmb{y}(t_0) \\
        \begin{bmatrix}
            \phi_0(t_0) &
            \phi_1(t_0)
        \end{bmatrix}
        \begin{bmatrix}
            B_0 \\
            B_1
        \end{bmatrix}
        +
        \pmb{\tilde\phi}^T(t_0) \tilde{B}
        &= \pmb{y}_0
    \end{align}
    Where $\tilde{B}$ is the remaining $(N-1)-2$ rows of $B$, which the actual learnable part.
    $t$.
    We can write a similar equation for $f(y_0)$.
    \begin{align}
        \label{eq:phi_f_init}
        \pmb{\dot\phi}(t_0) B &= f(\pmb{y}_0) \\
        \begin{bmatrix}
            \dot{\phi}_0(t_0) &
            \dot{\phi}_1(t_0)
        \end{bmatrix}
        \begin{bmatrix}
            B_0 \\
            B_1
        \end{bmatrix}
        +
        \pmb{\tilde{\dot\phi}}^T(t_0)  \tilde{B}
        &= f(\pmb{y}_0)
    \end{align}
    Combining~\eqref{eq:phi_y_init} and~\eqref{eq:phi_f_init} we get:
    \begin{equation}
        \label{eq:init_cond}
        \begin{bmatrix}
            \phi_0(t_0)     & \phi_1(t_0)     \\
            \dot\phi_0(t_0) & \dot\phi_1(t_0)
        \end{bmatrix}
        \begin{bmatrix}
            B_0 \\ B_1
        \end{bmatrix}^T
        +
        \begin{bmatrix}
            \pmb{\tilde\phi}(t_0) &
            \pmb{\tilde{\dot\phi}}(t_0)
        \end{bmatrix}^T
        \tilde{B}
        =
        \begin{bmatrix}
            \pmb{y}_0 \\
            \pmb{f}_0
        \end{bmatrix}
    \end{equation}
    The above equation allows us to express the first two rows of the coefficients matrix $B$ as a linear combination
    of the rest $(N-1)-2$ rows.
    This means we don't need to ``learn'' them though optimisation, we instead minimize the error subject to the
    remaining rows.
    \begin{equation}
        \label{eq:phi_f2}
        \begin{bmatrix}
            B_0 \\ B_1
        \end{bmatrix}^T
        =
        \begin{bmatrix}
            \phi_0(t_0)     & \phi_1(t_0)     \\
            \dot\phi_0(t_0) & \dot\phi_1(t_0)
        \end{bmatrix}^{-1}
        \left(
        \begin{bmatrix}
            \pmb{y}_0 \\
            \pmb{f}_0
        \end{bmatrix}
        -
        \begin{bmatrix}
            \pmb{\tilde\phi}(t_0) &
            \pmb{\tilde{\dot\phi}}(t_0)
        \end{bmatrix}^T
        \tilde{B}
        \right)
    \end{equation}

    The initialisation of $B$ is in the users choice, it could be random matrix.
    Alternatively, one could use a coarse ODE solver, likely a forward Euler iteration with relatively large step size,
    to get a rough estimate $\pmb{y}_c(t)$.
    Then by solving the least squares problem $\pmb{y}_c(t) - \Phi B_0$ get an initialisation that's closer to the
    optimum, $B^*$, for faster convergence.
    One has to figure out a good balance between the number of steps of the coarse solver -which are innately serial-
    and how far the initial $B$ is from the optimum, which will increase the number of iterations.

    \subsubsection{The Hessian}

    The primary concern computationally is the number of function evaluations or \textit{NFE} of $f$,
    the neural networks.
    The number of coefficients in $B$ is orders of magnitude smaller that the number of parameters in the network,
    hence storing the full Hessian shouldn't be a limiting factor.

    Instead of directly inverting the Hessian to obtain the next $B$, we use a modified Cholesky decomposition
    (specifically of $LDU$ form) as described in ~\cite{gill2019practical}.
    We use this technique in order to project the Hessian to a space of positive definite matrices in case it is not,
    which is a necessary condition for the Newton algorithm to converge.
    This way we can increase the definiteness of the Hessian while also solving the linear system
    $\pmb{d}^{[k]} \gets -\left[ \nabla^2 e(B^{[k]}) \right]^{-1} \nabla e(B^{[k]})$ as an LDU system (two triangular
    and one diagonal systems).
    To prevent confusion around the dimensionality of the Hessian $\nabla_{B} e$,
    the columns of $B$ are stacked as a vector so that the hessian is a rectangular matrix instead of a $4D$ tensor.

    \subsection{Comparison with classical and other parellel-in-time methods}
    First of all, the parallelisation potential of such a method comes from the fact that $f$ can be evaluated for all
    input points $\hat y(t)$ simultaneously at a single iteration.
    More specifically, during the fist, zero-th order part of the algorithm, a vector of points is passed into $f$ to
    update the coeffecients as can be seen in  ~\eqref{eq:Bitaration}.
    In reality, $f$ is applied on each element of this vector but since the calculations are independent all elements are
    evaluated in parallel.
    Similarly, during the Newton iteration the Hessian can, up to some degree, be computed in parallel by exploiting
    vectorization [CITE PYTORCH vmap?].
    This is in contrast to classical ODE solvers where the solution, or the trajectory in state space, is calculated from
    initial time to terminal time, step by step; in our case the whole trajectory (a continuous time approximation
    of it rather) is known from the start.
    This is what allows to calculate the values -and the derivatives- along the entire trajectory in a parallel manner.

    Obviously, more than one iteration is needed to obtain a solution to some specified accuracy.
    But if there are sufficiently many computing nodes available, each iteration should be at least comparable to a
    single step of many common ODE solvers.
    The critical point of out method is that it takes less parallel/batch evaluations of $f$ to converge to a solution
    than steps of a time stepping solver.

    For the second part of the algotithm the Jacobian and Hessian matrices of $e$ also have to be calculated.
    Because of how automatic differentiation works the Jacobian is a byproduct of calculating the Hessian.
    Calculating the full Hessian is actually not of trivial cost and we will address this later.
    Additionally, the line search algorithm -for finding step sizes along the path of the Newton direction requires
    some evaluations of $e$ and its Jacobian.
    Decreasing the number of function evaluations is crucial for further speeding up this algorithm.
    To this goal the cheaper recursive least squares solution is found first at significally less number of function
    evaluations and the Newton algorithm is used for refining the solution to highter accuracy.

    It's important to note that traditional ODE numerical solver also take more that 1 (serial) function evaluations at
    each step.
    For example RK45, a widely used algorithm from the family of Runge-Kutta methods, involves six sequential function
    evaluations (NFE) [CITE?] at each step.
    Likewise, Dormand-Prince, the default method in many software packages [CITE?], sometimes referred as DOPRI5 also
    has an NFE of 6 for a single step.

    Comparing popular parallel in time ODE solving methods, like PFASST and parareal, to our approach showcases
    similarities but also differences.
    One major difference is simplicity which allows easier implementation.
    Polynomial Approximation Numerical integration only builds upon simple polynomial approximation and a numerical
    optimiser.
    Other methods, like PFASST ~\cite{emmett2012toward}, rely on more complicated \textit{spectral deferred correction}
    methods, a well established but more involved numerical method.
    Moreover, other parallel in time techniques, like parareal, split the initial value problem into $N$ smaller boundary
    value problems using a combination of coarse and fine solvers ~\cite{maday2002parareal}.

    One advantage of the method presented here is that it evaluates $f$ on multiple inputs simultaneously,
    to minimise some error function.
    This \textit{synchronised} parallelisation scheme resembles how multiples input samples are processed in batches
    during training of a neural network.
    This means that one can exploit GPUs' architecture for mass parallelisation using existing frameworks.

    \subsection{Shortcomings and improvements}
    As mentioned above the critical factor for the performance of neural ODEs is the number of function evaluations that
    have to be performed sequentially.
    While our experiments show that polynomial approximation integration can reduce NFE there are hidden costs involved
    with the calculating of the full Hessian and during line search.

    Firstly, because the parameters $B$ should be significantly less than the parameters of the network; calculating the
    full Hessian doesn't present problems as far as memory is concerned.
    But since Newton optimisation requires the inverse of the Hessian it is not possible to perform a fast
    Hessian-vector operation exploiting automatic differentiation properties.
    We instead calculate the full Hessian first, row by row, and then invert it.

    Secondly, the line search algorithm involves computing the objective function and it's derivative at multiple
    states ($\pmb{y}(t)$).
    While we hope that it shouldn't check more that two or three candidate values for the learning rate we can
    expect at least 4 more function evaluations that will be performed serialy ~\cite{wright2006numerical}.

    These limitations of Newton optimisation with line search call for more numerical methods to be tested like
    Quasi-Newton, Conjugate Gradients and BFGS.

%    \subsection{hyper solver + parallel in time(???)}
%    Shifting out focus to the more specific problem of training neural ODEs.

    \subsection{Experiments}
    We compare the performance of our method to popular time stepping methods in terms of accuracy and absolute error.
    Most implementations of variable step length methods like RK45 allow for absolute (atol) and relative (rtol)
    tolerances to be specified which keep the local error estimates bellow $atol + rtol |y|$.
    For our case, we define tolerances for both steps of the algorithm, least squares and Newton iteration in the
    following ways.
    For the first part, the algorithm stops when there is no sufficient change in the parameters:
    $\lVert B^{[k]} - B^{[k-1]} \rVert < $ tol\_ls.
    While for the Newton part the iteration stops when the gradient is sufficiently close to zero:
    $ \lVert \partial_k e \rVert < $ tol\_newton.
    Adjusting these two \textit{hyperparameters} as well as the number of approximation coefficients and the number of
    points where the error is calculated; we can change the convergence properties of the algorithm.

    \subsubsection{Spiral}
    First, we will examine a toy example of a nonlinear spiral in $\mathbb{R}^{2}$.
    We assume that the true solution is provided by the RK45 method with step size three orders of magnitude smaller than the
    interval.
    We assume this approximation to be good enough for out purpose and compare the norm of the error at terminal time
    with other alongside our approach.

    \begin{equation*}
        \frac{d \pmb{y} }{dt} =
        \tanh(
        \begin{bmatrix}
            a & 1 \\ -1 & -a
        \end{bmatrix}
        \pmb{y}
        )
    \end{equation*}

    We solve the ODE in the interval $[0,10]$ for $\pmb{y}(0) = [2,\ 1]^T$.
    We use the same tolerances for the time stepping methods and for ours (PAN) even thought there is not direct
    equivalence.
    This shouldn't matter as we are ultimately interested only in the comparison between accuracy and number of function
    evaluations (NFE).
    For the other solvers the relative error is also set to a small number so that the absolute error is unaffected.

    \renewcommand{\arraystretch}{1.5}
    \renewcommand{\tabcolsep}{10.25pt}
    \begin{table}[!h]
        \begin{center}
            \caption{Metrics for the solution of an IVP with different methods. For PAN params indicates the number of
            approximation coefficients per dimention, in this case there are 2 dimensions.
            NFE for PAN is show as NFE for least squares plus (+) NFE for Newton.}
            \label{spiral-metrics}
            \begin{tabular}{|c|c|c|c|}
                \hline
                Method(params) & tolerance & error(t) & NFE \\
                \hline
                RK45 & 1.0e-06 & 4.14e-07 & 212
                \\ \hline
                Radau & 1.0e-06 & 6.97e-08 & 599
                \\ \hline
                BDF & 1.0e-06 & 4.56e-06 & 250
                \\ \hline
                LSODA & 1.0e-06 & 4.34e-06 & 171
                \\ \hline
                PAN(50) & 1e-06/1e-06 & 1.86e-10 & 37 + 5
                \\ \hline
                PAN(100) & 1e-06/1e-06 & 4.17e-14 & 36 + 5
                \\ \hline \hline
                RK45 & 1.0e-09 & 6.28e-10 & 716
                \\ \hline
                Radau & 1.0e-09 & 1.05e-10 & 2994
                \\ \hline
                BDF & 1.0e-09 & 1.66e-08 & 656
                \\ \hline
                LSODA & 1.0e-09 & 4.48e-10 & 361
                \\ \hline
                PAN(50) & 1e-09/1e-09 & 1.86e-10 & 42 + 5
                \\ \hline
                PAN(100) & 1e-09/1e-09 & 2.25e-15 & 55 + 5
                \\ \hline
            \end{tabular}
        \end{center}
    \end{table}

    We remind that for PAN, during optimisation $f$ is evaluated on multiple points, but since this calculations can be
    performed in parallel we assume the wall clock time is similar to one evaluation plus parallelisation overhead.
    As we can see in table ~\ref{spiral-metrics} for this simple problem our method achieves better accuracy while
    taking significantly less number of function evaluation.

    \subsubsection{Lorenz Attractor}
    The Lorenz system is a system of ordinary differential equations known for its chaotic behaviour.
    \begin{align*}
        \frac{dx}{dt} &= \sigma( y - x ) \\
        \frac{dy}{dt} &=  x( \rho - z) - y \\
        \frac{dz}{dt} &=  xy - \beta z\\
    \end{align*}

    We again solve the equation using a variety of solvers and compare them with our method.
    Though, this time we use ``step'' for our method, meaning we apply the algorithm in smaller sub-intervals setting
    the endpoint of each sub-solution as the initial conditions for the next interval.
    The problem parameters are set to: $\rho = 28, \ \sigma=10,\ \beta = \frac{8}{3}$, the interval is $[0, 15]$ and
    $\pmb{y}(0) = [2, 1 ,1]^T $.

    \begin{table}[!h]
        \begin{center}
            \caption{Accuracy and NFE for Lorenz system. For PAN the interval has been split in subintervals of length
                $0.3$ and the algorithm has been applied sequencially on each interval.}
            \label{lorenz_metric}
            \renewcommand{\arraystretch}{1.5}
            \renewcommand{\tabcolsep}{10.25pt}
            \begin{tabular}{|c|c|c|c|}
                \hline
                Method(params) & tolerance & error(t) & NFE \\
                \hline
                RK45 & 1.0e-06 & 5.75e-02 & 5174
                \\ \hline
                Radau & 1.0e-06 & 2.17e-02 & 16476
                \\ \hline
                BDF & 1.0e-06 & 2.09e-01 & 4794
                \\ \hline
                LSODA & 1.0e-06 & 4.86e-02 & 2803
                \\ \hline
                PAN(50) & 1e-06/1e-06 & 1.60e-07 & 1180 + 250
                \\ \hline
                PAN(100) & 1e-06/1e-06 & 1.60e-07 & 1231 + 250
                \\ \hline\hline
                RK45 & 1.0e-09 & 6.58e-05 & 19532
                \\ \hline
                Radau & 1.0e-09 & 4.19e-06 & 89999
                \\ \hline
                BDF & 1.0e-09 & 5.65e-04 & 15184
                \\ \hline
                LSODA & 1.0e-09 & 5.45e-06 & 4735
                \\ \hline
                PAN(50) & 1e-09/1e-09 & 1.61e-07 & 1399 + 262
                \\ \hline
                PAN(100) & 1e-09/1e-09 & 1.61e-07 & 1450 + 390
                \\ \hline
            \end{tabular}
        \end{center}
    \end{table}

    In table, \ref{lorenz_metric} we see that our method achieves better accuracy for less amount of function
    evaluation on this system of ODEs.
    Although, we have to note that accuracy seems to become saturated and decreasing tolerances or increasing number
    of coefficients doesn't improve results.
    It becomes apparent that finding the optimal hyperparameters is crucial for the most efficient use of the algorithm.

    \newpage

    \printbibliography

    \appendix

    \section{The backpropagation equations}
    In order to contrast and highlight the differences between classical neural networks, residual neural networks and neural ODEs we showcase how the gradients of the loss wrt.
    the learnable parameters is calculated in each architecture.
    This quantity is necessary in order to minimise the loss of the network using any -gradient descent based- optimisation algorithm.

    \subsection{Vanilla Neural Networks}
    Classical artificial neural networks are comprised of linear transformations followed by non-linear activation functions.
    The equation that describes the input-output relationship of the $n$-th layer is:
    \begin{equation}
        \bm{y}_{n} = \phi( W_{n} \bm{y}_{n-1} )
    \end{equation}

    Where $\phi$ is a non-linear function, $W_n$ is the layer's weights, $\bm{y}_{n-1}$ the output of the previous layer, we omit the biases vector to simplify the equations but the logic is the same.
    Unfolding this equation for the whole network of depth $N$ we get:

    \begin{equation}
        \bm{y}_{out} = \phi ( W_{N} \cdot \phi(W_{N-1} \cdot  \phi(W_{N-2}( \dots ))))
    \end{equation}

    The output of the network is passed in a (scalar) cost function $C(\bm{y}_{out},\bm{y}^*)$.
    In order to apply gradient descent based optimisation algorithms it is necessary to calculate the gradient wrt.
    all the weights:

    \begin{equation}
        \frac{ \partial C}{\partial W_{n} }, \quad \text{for} \ n=1,2,\dots,l
    \end{equation}

    Focusing on a single layer, lets use $\textbf{z}_{n}$ to denote $W_{n}\cdot \textbf{y}_{n-1}$ or whats goes in the non-linear function at each layer.
    By applying the chain rule we get:

    \begin{align}
        \frac{ \partial C}{\partial \textbf{y}_{n-1} }
        =
        \left[ \frac{ \partial \textbf{y}_{n}}{\partial \textbf{y}_{n-1}  } \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        &=
        \left[ \frac{ \partial}{\partial \textbf{y}_{n-1} } \phi(W_n \textbf{y}_{n-1}) \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        \\
        &=
        \left[ W_n^T \phi^{(1)}( \pmb{z}_n ) \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        \\
        &=
        \left[ \phi^{(1)}( \pmb{z}_n ) \right]^T
        W_n \;
        \frac{ \partial C}{\partial \textbf{y}_{n} }
    \end{align}

    The non-linearity $\phi$ is applied on each element of the input vector independently, so its partial derivative wrt.
    the input its a diagonal matrix and its transpose is the same as itself.

    \begin{align}
        \frac{ \partial C}{\partial \textbf{y}_{n-1} }
        =
        \phi^{(1)}( \pmb{z}_n )
        W_n \;
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        \label{dCdy}
    \end{align}

    Equation~\eqref{dCdy} allows to calculate all such derivatives recursively starting from the last layer where $\frac{\partial C}{\partial \textbf{y}_{out}}$ has a closed form solution.
    We can now calculate the desired gradients with respect to the weights.
    Below $W_n^i$ is the $i$-th column of the weights matrix of the $n$-th layer.

    \begin{align*}
        \frac{ \partial C}{\partial W_n^{[i]} }
        =
        \left[ \frac{ \partial \pmb{y}_{n}}{\partial W_n^i } \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        &=
        \left[ \frac{ \partial }{\partial W_n^{[i]} }  \phi( W_{n} \bm{y}_{n-1} )  \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        \\
        &=
        \left[ \frac{ \partial }{\partial W_n^{[i]} }  \phi(  W_{n}^{[1]} \bm{y}_{n-1}^{[1]} + \dots +W_{n}^{[i]} \bm{y}_{n-1}^{[i]} + \dots )  \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        \\
        &=
        \left[ \left[ \pmb{y}_{n-1}^{[i]} \right]^T \phi^{(1)}(\pmb{z}_n )  \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        \\
        &=
        \phi^{(1)}(\pmb{z}_n ) \; \pmb{y}_{n-1}^{[i]}
        \frac{ \partial C}{\partial \pmb{y}_{n} }
    \end{align*}

    Stacking all the columns together we get:

    \begin{equation}
        \label{dCdW}
        \frac{ \partial C}{\partial W_n}
        =
        \phi^{(1)}(\pmb{z}_n )
        \frac{ \partial C}{\partial \pmb{y}_{n} }  \,
        \pmb{y}_{n-1}
    \end{equation}

    Using~\eqref{dCdy} and~\eqref{dCdW} together we can find all the necessary gradients for backpropagation; as expressed in algorithm~\ref{alg:backprop}

    \begin{algorithm}
        \caption{Backpropagation}
        \label{alg:backprop}
        \begin{algorithmic}
            \State Do the forward pass, save in memory $W_n, \textbf{z}_n, \textbf{y}_n$
            \State Calculate $\frac{\partial C}{\partial \textbf{y}_{out}}$ which is trivial
            \For{$n=N-1, \dots, 1$}
                \State Using~\eqref{dCdy} and  $\frac{\partial C}{\partial \textbf{y}_{n+1}} $ find $\frac{\partial C}{\partial \textbf{y}_n}$
                \State  Using~\eqref{dCdW} and $\frac{\partial C}{\partial \textbf{y}_n}$ find $\frac{\partial C}{\partial W_n}$
            \EndFor
        \end{algorithmic}
    \end{algorithm}

    The backpropagation algorithm is quite fast in modern hardware as it consists of mostly matrix vector multiplications.
    Although, it is memory intensive since it requires saving all the weights and activations from the forward pass and accessing them during the backward pass.

    \subsection{Residual Networks}
    Residual neural networks operate quire similarly to classical ones in terms of training.

    \begin{equation}
        \bm{y}_{n} = \phi( W_{n} \bm{y}_{n-1} ) + \bm{y}_{n-1}
    \end{equation}

    \begin{align}
        \label{resnet_dCdy}
        \frac{ \partial C}{\partial \textbf{y}_{n-1} }
        =
        \left[ \frac{ \partial \textbf{y}_{n}}{\partial \textbf{y}_{n-1}  } \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
        &=
        \left\{
        \frac{ \partial }{\partial \textbf{y}_{n-1}}
        \left[
            \phi( W_n \textbf{y}_{n-1} ) + \textbf{y}_{n-1}
            \right]
        \right\}^T
        \frac{ \partial C}{ \partial \textbf{y}_{n} }
        \\
        &=
        \left[
            W_n^T \phi^{(1)}( \pmb{z}_n ) + I
            \right]^T
        \frac{ \partial C}{\partial \textbf{y}_{n} }
    \end{align}

    Equation~\eqref{resnet_dCdy} is again a recursive relation that can be used to find all intermediate ``states '' starting from the output layer.
    The gradients wrt.
    weights becomes:

    \begin{align}
        \frac{ \partial C}{\partial W_n^{[i]} }
        =
        \left[ \frac{ \partial \pmb{y}_{n}}{\partial W_n^i } \right]^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        &=
        \left\{
        \frac{ \partial }{\partial W_n^{[i]} }
        \left[
            \phi( W_{n} \bm{y}_{n-1} )  + \textbf{y}_{n-1}
            \right]
        \right\}^T
        \frac{ \partial C}{\partial \pmb{y}_{n} }
        \label{resnet_dCdW}
    \end{align}

    Since the output of layer $n-1$ doesn't depend on the weights of layer $n$,~\eqref{resnet_dCdW} becomes identical to~\eqref{dCdW} and the same backpropagation algorithm is used to train the network.

    \begin{tikzpicture}
        \node [rectangle, draw, text width=0.8\textwidth, inner sep=10pt] (content) at (0,0) {
            Backpropagation can be seen as a specific of Automatic Differentiation, a very powerful framework to acquire gradients numerically.
            The governing principle is the same, propagating the gradients through functions with closed form derivative, utilising the chain rule.
        };
    \end{tikzpicture}

    \subsection{Neural ODEs}
    Calculating $\nabla_{\theta}L$ in neural ODEs can be achieved by differentiating through the solver steps but as we will demonstrate there is a much more efficient way, using tools from optimal control theory, namely adjoint sensitivities.
    The governing equations for neural ODEs are~\eqref{ivp},~\eqref{adjoint},~\eqref{dldtheta} we repeat them for convenience:

    \begin{equation*}
        \pmb{y}(T) =  \pmb{y}(0) +\int_{0}^{T} f(\pmb{y}(\tau), \tau, \pmb{\theta}) \,d\tau
        , \quad
        \pmb{y}(0) = \pmb{y}_0
    \end{equation*}

    \begin{equation*}
        \frac
        {d \pmb{a}(t)}
        {dt}
        =
        - \pmb{a}(t)^T
        \frac
        {\partial f( \pmb{y}(t), t, \pmb{\theta} )}
        {\partial \pmb{y} }
        , \quad
        \pmb{a}(T) = \frac{\partial L}{\partial \pmb{y}(T)}
    \end{equation*}

    \begin{equation*}
        \nabla_{\pmb{\theta}} L =
        - \int_T^0
        \pmb{a}(t)^T
        \frac
        {\partial f(\pmb{y}(\tau), \pmb{\theta})}
        {\partial \pmb{\theta}}
        \, d\tau
    \end{equation*}

    Calculating the last integral numerically requires the values of $\pmb{a}(t)$ at all the points in time the solver chooses.
    In the case of a fixed step size solver with step $h$ those would be $t_m = T - mh, \; m=0,1,\dots$.
    In the case of an adaptive step size $t_m$ could be any point between $T$ and $0$.
    In both cases we can calculate those quantities by solving the second ODE starting from time $t=T$ -where $a(T)$ has a closed form solution- and moving backwards in time.
    Next we need the value of $\pmb{y}$ at the same $t_m$s. Again we can calculate them starting from $\pmb{y}(T)$ and solving the original IVP again backwards in time.
    Notice that we don't need to save any intermediate state or activations from the forward pass since we can re-calculate them moving backwards.
    In this sense, neural ODEs are reversible (up to numerical tolerances).

    In algorithm~\ref{alg:adjoint} we reformulate the integral equation for the gradient of the loss wrt.
    weights as a differential equation: $ \frac{d}{dt} \pmb{a}_\theta(t) = -\pmb{a}(t)^T \frac{\partial f}{\partial \pmb{\theta}}$.
    We know that $\pmb{a}_\theta(T) = 0$ and we search for $\pmb{a}_\theta(0) = \nabla_\theta L$.

    \begin{algorithm}
        \caption{Adjoint}
        \label{alg:adjoint}
        \begin{algorithmic}
            \State Choose ODE solver hyperparameters ($h$, $\dots$)
            \State Do the forward pass, find $\pmb{y}(T)$
            \State $\pmb{a}(T) \gets \frac{\partial L}{\partial \pmb{y}(T)}$ (closed form)
            \State $\pmb{a}_{\theta}(T) \gets 0$
            \State $t \gets T$
            \While{$t > 0$}
                \State $\pmb{y}(t - \Delta t) \gets \text{IntStep}(\pmb{y}(t), f) $
                \State $\pmb{a}(t - \Delta t) \gets \text{IntStep}(
                \pmb{a}(t), -\pmb{a}(t) \frac{\partial f }{\partial \pmb{y}}
                )$
                \State $\pmb{a}_{\theta}(t - \Delta t) \gets \text{IntStep}(
                \pmb{a}_{\theta}(t), -\pmb{a}(t) \frac{\partial f }{\partial \pmb{\theta}}
                )$
                \State $t \gets t - \Delta t$
            \EndWhile
            \State \textbf{return} $\pmb{a}_\theta(0) = \nabla_\theta L$
        \end{algorithmic}
    \end{algorithm}


    \section{Numerical Solvers}

    \subsection{Runge-Kutta 5}

    solve $\frac{dy}{dt} = f(t,y)$, $y(t_0) = y_0 $

    \begin{align*}
        y_{n+1} &= y_n + \frac{h}{6}(k_1 + 2k_2 +2k_3 + k_4)  \\
        t_{n+1} &= t_n + h
    \end{align*}

    \begin{align*}
        k_{1} &= f(t_{n}, y_{n}), \\
        k_{2} &= f(t_{n} + \frac{h}{2}, y_{n} + h \frac{ k_{1} }{ 2 } ),    \\
        k_{3} &= f(t_{n} + \frac{h}{2}, y_{n} + h \frac{ k_{2} }{ 2 } ),  \\
        k_{4} &= f(t_{n} + h, y_{n} + hk_3), \\
    \end{align*}

    can be proved using taylor series expansion...


    \section{An alternative derivation for $\frac{dL}{d \pmb{\theta}}$}
    \label{adjoint_proof}
    There are already many ways in the literature for finding this quantity without delving into control theory and reverse sensitivities.
    Intuitively in can be thought as a continuous analogous to classical backpropagation.

    \begin{align}
        \text{classical} &\to
        \frac{d L}{ d \pmb{\theta}} =
        \sum_k
        \frac{\partial L}{\partial \pmb{y}_{k}}
        \frac{\partial f(\pmb{y}_k, \pmb\theta)}{\partial \pmb{\theta}}
        \\
        \text{adjoint sensitivities} &\to
        \frac{d L}{ d \pmb{\theta}} =
        \int_T^0
        \frac{\partial L}{\partial \pmb{y}(\tau)}
        \frac{\partial f(\pmb{y}(\tau), \pmb{\theta})}{\partial \pmb{\theta}} \, d\tau
    \end{align}

    In the original neural ODEs paper~\cite{chen2018neural} the authors prove~\eqref{adjoint} and use an augmented state to prove~\eqref{dldtheta} while others like~\cite{kidger2022neural} provide alternative proofs.
    We present another, in our opinion much simpler, derivation for~\eqref{dldtheta}.
    Consider the more general case where the loss is dependant on intermediate state on a point $s \in (0,T)$.
    This is equivalent to the usual case where the loss depends only on the output state with $\mathcal{L}(s) = L(\pmb{y}(s))=0$ for $s\neq T$.
    Moreover, the state $\pmb{y}$ is a also dependant on the weights even though it's not explicitly written.

    \begin{align*}
        \frac{ d L(\bm{y}(s, \bm{\theta})) }{ d \bm{\theta} }
        &= \int_0^s \frac{d}{dt} \left( \frac{ d L(\bm{y}(s, \bm{\theta})}{ d \bm{\theta}} \right) dt \\
        &= \int_0^s \frac{d}{dt} \left( \frac{ d L(\bm{y}(t, \bm{\theta})}{ d \bm{y}(t)} \frac{d \bm{y}(t)}{d\bm{\theta}} \right) dt \\
        &= \int_0^s
        \frac{d}{dt} \frac{ d L(\bm{y}(t, \bm{\theta})}{ d \bm{y}(t)} \cdot \frac{d \bm{y}(t)}{d\bm{\theta}}
        +
        \frac{ d L(\bm{y}(t, \bm{\theta})}{ d \bm{y}(t)} \cdot \frac{d}{dt} \frac{d \bm{y}(t)}{d\bm{\theta}}
        dt \\
        &= \int_0^s
        \dot{\bm{a}}(t) \frac{d \bm{y}(t)}{d\bm{\theta}}
        +
        \bm{a}(t) \cdot \frac{d}{d\bm{\theta}} \frac{d \bm{y}(t)}{dt}
        dt \\
        &= \int_0^s
        \dot{\bm{a}}(t) \frac{d \bm{y}(t)}{d\bm{\theta}}
        +
        \bm{a}(t) \cdot \frac{d\bm{f}}{d\theta}
        dt \\
        &= \int_0^s
        \dot{\bm{a}}(t) \frac{d \bm{y}(t)}{d\bm{\theta}}
        +
        \bm{a}(t) \left( \frac{\partial \bm{f}}{\partial \bm{\theta}}
        +
        \frac{\partial \bm{f}}{\partial \bm{y}(t)} \frac{\partial \bm{y}(t)}{\partial \bm{\theta}}\right)
        dt \\
        &= \int_0^s
        \frac{d \bm{y}(t)}{d\bm{\theta}}
        \left( \dot{\bm{a}}(t) + a\frac{\partial \bm{f}}{\partial \bm{y}(t)} \right)
        +
        \bm{a}(t) \frac{\partial \bm{f}}{\partial \bm{\theta}}
        dt \\
    \end{align*}
    From~\eqref{adjoint} the sum in the parenthesis is 0:
    \begin{align}
        \frac{ d L(\bm{y}(s, \bm{\theta})) }{ d \bm{\theta} }
        = \int_0^s
        \bm{a}(t) \frac{\partial \bm{f}}{\partial \bm{\theta}}
        dt
    \end{align}
    Setting $s=T$ we arrive at the desired formula.


    \section{Vector-Jacobian products}
    Modern software packages utilise automatic differentiation methods to calculate derivatives of some arbitrary function $f$, usually with respect to some parameters or weights $\theta$.
    \begin{equation}
        \frac{\partial f(x; \theta)}{ \partial \theta}
    \end{equation}


    \section{Optimisation}
    In the field of numerical optimisation many methods have been developed for minimising a \textit{scalar} objective function $f$.
    One category of those, \textit{line search} methods, are iterative algorithms that seek to update the current iterate $x_k$ to a new value closer to the minimum.
    They work by choosing a direction $p_k$ -and step size $\alpha$- along which $f$ is decreased.
    Alternatively the problem can be restated as:
    \begin{equation}
        \min_{a>0} f(x_k + \alpha p_k) \label{min_a}
    \end{equation}
    While ideally we would solve~\eqref{min_a} exactly, in practice it is computationally expensive and practically unnecessary.
    Line search implementations usually generate some trial step sizes until they find one that satisfies certain termination conditions we will examine later.

    A \textit{descent direction} is a direction that causes $f$ to decrease along it given sufficiently small $\alpha > 0$: $f(x_k + \alpha p_k) < f(x_k)$.We can show that if $p_k$ is a descent direction then $p_k^T \nabla f_k < 0$.
    From the first order Taylor series expansion we have:
    \begin{equation*}
        f(x_k+ap_k) = f(x_k) + a p_k^T \nabla f_k + O(a^2)< f(x_k) \\
    \end{equation*}
    Ignoring quadratic term since $\alpha$ is small.
    \begin{align*}
        a p_k^T \nabla f_k &< 0 \\
        p_k^T \nabla f_k &< 0
    \end{align*}
    It can be proven though Zoutendijk's theorem that: as long as the search direction is a descent direction and the step size fulfils some conditions the algorithm is globally convergent.

    The defining characteristic of a line search method is the way in which we obtain the search direction $p_k$.
    An obvious choice is to use the direction of \textit{steepest descent}, mathematically obtained as the negative of the gradient at the current iterate.
    \begin{equation}
        x_{k+1} = x_k - \alpha \nabla f_k
    \end{equation}

    In machine learning literature steepest descent, or as they are more commonly refereed, \textit{gradient descent} methods are extremely common.
    Almost all neural network models are trained using some derivative of classic gradient descent.
    These methods are not categorised as line searches since they do not seek to find an appropriate step length $\alpha$ on each iteration.
    They instead define either a fixed or dynamically updated \textit{learning rate}.
    Usually they employ some notions of momentum and stochasticity to move along a direction dictated by an approximation of the local gradient.
    Even so they remain first order methods since they only utilise information about the first derivative.
%    Some notable gradient descent methods as well as how the minimisation problem is formulated in the context of machine learning are discussed later.

    \subsection{The Newton-Raphson algorithm with Hessian modification}
    \label{newtonalgo}
    In more general minimisation problems a prevalent search direction for line search optimisers is the \textit{Newton direction}, methods using this direction are called Newton methods.
    This direction is derived from the second-order Taylor series approximation of the objective function, near the current iterate $x_k$.
    Consider a continuous, twice differentiable function $f: \mathbb{R}^n \to \mathbb{R}^n$, its second order Taylor series approximation is:
    \begin{equation}
        f(x_k + p) = f_k + p^t \nabla f_k + \frac{1}{2} p^t \nabla^2 f_k p + R(p) \label{taylor}
    \end{equation}
    with $R(p)$ being of order $O(\rVert p \rVert^3)$.
    For small values of $p$, the residual term diminishes and we have a pretty good quadratic model of $f$.
    Seeking to minimise this model we set the derivative wrt. $p$ equal to 0:
    \begin{align}
        \nabla f_k + \nabla^2 f_k p_t &= 0  \label{grad_m} \\
        p = -\left[ \nabla^2 f_k \right]^{-1} \nabla f_k \label{newton_dir}
    \end{align}
    Equation~\eqref{newton_dir} gives the definition of the newton direction.
    As mentioned before, in order for the algorithm to be globally convergent the search direction has to be a descent direction.
    By multiplying~\eqref{grad_m} from the left with $p^t$ we get:
    \begin{align}
        p^t \nabla f_k + p_t \nabla^2 f_k p_t =& 0 \\
        p^t \nabla^2 f_k p_t =& -p^t \nabla f_k < 0 \\
        p^t \nabla^2 f_k p_t &> 0 \label{pos_def}
    \end{align}
    From~\eqref{pos_def} it is apparent that the Hessian matrix $H_k=\nabla^2 f_k$ has to be positive definite which is generally true if $x_k$ is near the minimum but not necessarily true away from it.
    If the Hessian is not positive definite there is no guarantee that~\eqref{newton_dir} gives a descent direction or even that the Hessian is non-singular.
    In order to address this issue a positive definite approximation of the Hessian is used.
    The approximation can be obtained in several ways, some involve adding a multiple of the identity or some correction matrix $\Delta H$, others modify the eigenvalues of the true Hessian directly.

    For example in~\cite{cheng1998modified} is shown that, if $H_k$ has spectral decomposition $H_k = Q \Lambda Q^T$ then the correction matrix of minimum Frobenius norm that ensures that the smallest eigenvalue of $H_k + \Delta H$ is larger or equal to $\delta$ is given by:
    \begin{equation*}
        \Delta H = Q \text(diag){\tau_i} Q^T, \quad \text{with} \quad \tau_i =
        \left\{
        \begin{array}{ll}
            0,                  & \lambda_i \geq \delta, \\
            \delta - \lambda_i, & \lambda_i < \delta,
        \end{array}
        \right.
    \end{equation*}

    Another technique, the one used here, is to perform (or try to perform) a Cholesky decomposition, more specifically an LDL decomposition of the Hessian.
    Since the Hessian is not always positive definite the factorisation $H = LDL^T$ may not exist or if even if it does the algorithm used to compute it is numerically unstable.
    The core idea of modified Cholesky decomposition is to modify the values of the diagonal matrix D to be sufficiently positive while the factorisation is computed.

    More specifically,~\cite{wright2006numerical} provide an algorithm for computing the $LDL^T$ factorisation a positive definite approximation of a matrix.
    It accepts two additional parameters $\delta, \beta$ so that the following bounds are satisfied.
    \begin{equation}
        d_j \geq \delta, \quad \lvert m_{ij} \rvert \leq \beta, \quad i = j+1, j_2, \dots, n
    \end{equation}

    The algorithm calculates the elements of the diagonal $D$ and the unitriangular matrix $L$ column by column.
    It is a simpler version of the algorithm presented by~\cite{gill2019practical} that additionally introduces symmetric row-column interchanges resulting in lower $ \lVert H_k - \hat{H}_k \rVert$, $\hat{H}_k$ being the approximation.

    \begin{algorithm}
        \caption{Modified Cholesky}
        \begin{algorithmic}
            \For{ $i = j+1,\dots,n$ }
                \State $c_{jj} \gets a_{jj} - \sum_{s=1}^{j-1} d_s l_{js}$
                \State $\theta_j \gets \max_{j<i\leq n}(\lvert c_{ij} \rvert)$
                \State $d_j \gets  \max
                \left(
                \lvert c_{jj} \rvert,
                \left( \frac{\theta_j}{\beta} \right)^2,
                \delta \right)$
                \For{$i = j+1,\dots, n$}
                    \State $c_{ij} \gets a_{ij} - \sum_{s=1}^{j-1} d_s l_{is} l_{js}$
                    \State $l_{ij} \gets c_{ij} / d_j$
                \EndFor
            \EndFor
        \end{algorithmic}
    \end{algorithm}

    After addressing the definiteness of the the Hessian let's describe how an algorithm would decide on the step size.
    As noted earlier Zoutendijk's theorem requires the search direction to be a descent direction as well as the step size to fulfil a certain set of conditions.
    One can prove Zoutedijk's theorem for multiple sets of conditions, namely the Wolfe, strong Wolfe and Goldstein conditions.
    We will focus on the strong Wolfe conditions.

    Assume we have chose direction $p_k$ at step $k$ of the algorithm, our second objective is to find step size $a$ to proceed to the next step without minimising $\phi(\alpha) = f(x_k + \alpha p_k)$ explicitly.
    The first strong Wolfe condition states that our guess for $a$ should give \textit{sufficient decrease} in the objective function in the following way:
    \begin{equation}
        f(x_k + ap_k) \leq f(x_k) + c_1 a \nabla f_k ^T p_k
    \end{equation}
    for some $c_1 \in (0,1)$ usually chosen to be quite small ($\approx 10^{-4}$). This ensures the reduction in $f$ is proportional to the step size as well as the directional derivative $\nabla f ^T p_k$.
    The sufficient decrease condition is not enough to make reasonable progress since it's satisfied for values of $\alpha$ close to 0.
    To prevent this, the second strong Wolfe condition or \textit{curvature condition} states that $\alpha$ should also satisfy:
    \begin{equation}
        \lvert \nabla f(x_k + \alpha p_k )^T p_k \rvert \leq \lvert c_2 \nabla f_k^T p_k \rvert
    \end{equation}
    for some constant $c_2 \in (c_1, 1)$ usually set around $0.9$ for Newton direction.
    Notice that the left hand side if the derivative wrt. $\alpha$ of $\phi(a)$ at $a_k$ and the right hand size at $0$.
    Practically the curvature condition requires the slope of $\phi$ at the new point to be greater that at the start times $c_2$.
    Remember that $p_k$ is a descent direction so $\phi(0)$ is strictly negative.
    Larger slope would mean we get closer to a stationary point of zero gradient.
    The absolute ensures that the slope doesn't become too positive and we don't overshoot away from the stationary point.

    Utilising those conditions we construct the following line search algorithm parameterised by $c_1, c_2, \alpha_{max}$

    \begin{algorithm}
        \caption{Line search}
        \label{alg:linesearh}
        \begin{algorithmic}
            \State Set $a_0 \gets 0$, choose $a_1 \in (0, a_{max})$
            \State $i \gets 1$
            \Repeat
                \If {$\phi(a_i) > \phi(0) + c_1 a_i \phi'(0)$ or [$\phi(a_i) \geq \phi(a_{i-1})$ and $i>1$]}
                    \State return \textbf{zoom}($a_{i-1}, a_i$)
                \EndIf
                \If {$\lvert \phi'(a_i) \rvert \leq -c_2 \phi'(0)$}
                    \State return $a_i$
                \EndIf
                \If {$\phi'(a_i) \geq 0$}
                    \State return \textbf{zoom}($a_i, a_{i-1}$)
                \EndIf
                \State $a_{i+1} \gets \min({a_max, 2*a_i})$
                \State $i \gets i+1$
            \Until
        \end{algorithmic}
    \end{algorithm}

\end{document}
