\xpginauxfiletrue 
\selectlanguage *{english}
\selectlanguage *{greek}
\selectlanguage *[variant=us,ordinalmonthday=false]{english}
\selectlanguage *{english}
\contentsline {chapter}{\numberline {1}Introduction}{3}{}%
\contentsline {chapter}{\numberline {2}Residual Networks}{4}{}%
\contentsline {subsection}{\numberline {2.0.1}Residual Learning}{4}{}%
\contentsline {chapter}{\numberline {3}Neural ODEs}{7}{}%
\contentsline {subsection}{\numberline {3.0.1}A continuous time network}{7}{}%
\contentsline {subsection}{\numberline {3.0.2}Inference}{8}{}%
\contentsline {subsection}{\numberline {3.0.3}Training}{9}{}%
\contentsline {subsection}{\numberline {3.0.4}Applications, benefits and limitations}{10}{}%
\contentsline {chapter}{\numberline {4}Numerical Solvers}{12}{}%
\contentsline {subsection}{\numberline {4.0.1}Introduction}{12}{}%
\contentsline {subsection}{\numberline {4.0.2}Forward Euler}{13}{}%
\contentsline {subsubsection}{Implicit Euler}{14}{}%
\contentsline {subsection}{\numberline {4.0.3}General explicit one-step method}{15}{}%
\contentsline {subsection}{\numberline {4.0.4}Runge-Kutta methods}{16}{}%
\contentsline {subsubsection}{Explicit Runge-Kutta methods}{16}{}%
\contentsline {chapter}{\numberline {5}Parallelism in Time}{19}{}%
\contentsline {subsection}{\numberline {5.0.1}The Parareal algorithm}{19}{}%
\contentsline {subsection}{\numberline {5.0.2}Polynomial approximation}{20}{}%
\contentsline {subsubsection}{A zero order optimisation algorithm}{22}{}%
\contentsline {subsubsection}{Higher order minimization}{24}{}%
\contentsline {subsection}{\numberline {5.0.3}Realisation details}{25}{}%
\contentsline {subsubsection}{Imposing initial conditions}{25}{}%
\contentsline {subsubsection}{The Hessian}{26}{}%
\contentsline {subsection}{\numberline {5.0.4}Comparison with classical and other parellel-in-time methods}{27}{}%
\contentsline {subsection}{\numberline {5.0.5}Shortcomings and improvements}{28}{}%
\contentsline {subsection}{\numberline {5.0.6}Experiments}{28}{}%
\contentsline {subsubsection}{Spiral}{28}{}%
\contentsline {subsubsection}{Lorenz Attractor}{30}{}%
\contentsline {subsection}{\numberline {5.0.7}A simple Neural Network}{31}{}%
\contentsline {chapter}{\numberline {A}The backpropagation equations}{37}{}%
\contentsline {subsection}{\numberline {A.0.1}Vanilla Neural Networks}{37}{}%
\contentsline {subsection}{\numberline {A.0.2}Residual Networks}{39}{}%
\contentsline {subsection}{\numberline {A.0.3}Neural ODEs}{39}{}%
\contentsline {chapter}{\numberline {B}An alternative derivation for $\frac {dL}{d \pmb {\theta }}$}{41}{}%
\contentsline {chapter}{\numberline {C}Optimisation}{43}{}%
\contentsline {subsection}{\numberline {C.0.1}The Newton-Raphson algorithm with Hessian modification}{44}{}%
\contentsline {subsection}{\numberline {C.0.2}Gradient Descent Methods}{48}{}%
\contentsline {subsubsection}{Stochastic Gradient Descent}{48}{}%
\contentsline {subsubsection}{ Momentum }{49}{}%
\contentsline {subsubsection}{Nesterov Momentum}{49}{}%
\contentsline {subsubsection}{Adagrad}{50}{}%
\contentsline {subsubsection}{RMSProp}{50}{}%
\contentsline {subsubsection}{Adam}{50}{}%
\xpginauxfilefalse 
